{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import expm\n",
    "import os\n",
    "import argparse\n",
    "from scipy.sparse.linalg import eigsh\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#import wandb\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models and Embeddings\n",
    "\n",
    "#EMBEDDINGS\n",
    "class GaussianEmbedding(nn.Module):\n",
    "    def __init__(self, num_terms, num_channels):\n",
    "        super(GaussianEmbedding, self).__init__()\n",
    "        self.num_terms = num_terms\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.h = nn.Parameter(torch.randn(num_terms + 1, num_channels))\n",
    "        nn.init.xavier_uniform_(self.h)\n",
    "\n",
    "    def forward(self, A):\n",
    "        batch_size, num_nodes, _ = A.shape\n",
    "        Y_hat = torch.zeros(batch_size, num_nodes, self.num_channels, device=A.device)\n",
    "        for c in range(self.num_channels):\n",
    "            result = self.h[0, c] * torch.eye(num_nodes, device=A.device).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            for i in range(1, self.num_terms + 1):\n",
    "                A_power_i = torch.matrix_power(A, i)\n",
    "                result += self.h[i, c] * A_power_i\n",
    "            Y_hat[..., c] = torch.diagonal(result, dim1=-2, dim2=-1)\n",
    "        return Y_hat\n",
    "\n",
    "class EigenEmbedding(nn.Module):\n",
    "    def __init__(self, num_eigenvectors = None):\n",
    "        super(EigenEmbedding, self).__init__()\n",
    "        self.num_eigenvectors = num_eigenvectors\n",
    "\n",
    "    def forward(self, A):\n",
    "        eigenvectors = []\n",
    "        for i in range(A.shape[0]):\n",
    "            _, V = torch.linalg.eigh(A[i])\n",
    "            if self.num_eigenvectors is not None:\n",
    "                eigenvectors.append(V[:, -self.num_eigenvectors:])\n",
    "            else:\n",
    "                eigenvectors.append(V)\n",
    "        return torch.stack(eigenvectors, dim=0)\n",
    "\n",
    "#MODELS\n",
    "class GraphConvolutionFilter(nn.Module):\n",
    "    def __init__(self, K, F, G, layer_norm = False):\n",
    "        super(GraphConvolutionFilter, self).__init__()\n",
    "\n",
    "        self.K = K #number of filter taps\n",
    "        self.F = F #number of input channels\n",
    "        self.G = G #number of output channels\n",
    "\n",
    "        self.H = nn.Parameter(torch.randn(K, F, G))\n",
    "        nn.init.xavier_uniform_(self.H)\n",
    "\n",
    "        if layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(G)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "    def forward(self, A, X):\n",
    "\n",
    "        #input X is B x N x F\n",
    "        Z = X @ self.H[0]\n",
    "\n",
    "        A_power_i = A.clone()\n",
    "\n",
    "        for i in range(1, self.K):\n",
    "            Z += torch.bmm(A_power_i, (X @ self.H[i]))\n",
    "            A_power_i = torch.bmm(A_power_i, A)\n",
    "\n",
    "        if self.layer_norm is not None:\n",
    "            Z = self.layer_norm(Z)\n",
    "\n",
    "        return Z\n",
    "    \n",
    "class LinearPE(nn.Module):\n",
    "    def __init__(self, F, D, bias = True, layer_norm = False):\n",
    "        super(LinearPE, self).__init__()\n",
    "        \n",
    "        self.F = F #number of input channels\n",
    "        self.D = D #number of output channels\n",
    "        \n",
    "\n",
    "        self.W = nn.Parameter(torch.randn(F, D))\n",
    "\n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.randn(D))\n",
    "        else:\n",
    "            self.b = torch.zeros(D)\n",
    "        \n",
    "        if layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(D)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        #input X is B x N x F\n",
    "\n",
    "        Z = X @ self.W + self.b\n",
    "\n",
    "        if self.layer_norm is not None:\n",
    "            Z = self.layer_norm(Z)\n",
    "\n",
    "        return Z\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module as described in 'Attention Is All You Need' paper.\n",
    "    \n",
    "    This implementation supports masked attention and different input/output dimensions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_k=None, d_v=None, dropout=0.0, bias=False):\n",
    "        \"\"\"\n",
    "        Initialize the Multi-Head Attention module.\n",
    "        \n",
    "        Parameters:\n",
    "        - d_model: Model dimension (input and output dimension)\n",
    "        - num_heads: Number of attention heads\n",
    "        - d_k: Dimension of keys (default: d_model // num_heads)\n",
    "        - d_v: Dimension of values (default: d_model // num_heads)\n",
    "        - dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # If d_k and d_v are not specified, set them to d_model // num_heads\n",
    "        self.d_k = d_k if d_k is not None else d_model // num_heads\n",
    "        self.d_v = d_v if d_v is not None else d_model // num_heads\n",
    "        \n",
    "        # Linear projections for queries, keys, and values\n",
    "        self.W_q = nn.Linear(d_model, num_heads * self.d_k, bias=bias)\n",
    "        self.W_k = nn.Linear(d_model, num_heads * self.d_k, bias=bias)\n",
    "        self.W_v = nn.Linear(d_model, num_heads * self.d_v, bias=bias)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(num_heads * self.d_v, d_model, bias=bias)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Layer normalization for the output\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Scaling factor for dot product attention\n",
    "        # self.scale = 1 / math.sqrt(self.d_k)\n",
    "        self.scale = 1\n",
    "\n",
    "        # Linear layer to combine attention scores from different heads\n",
    "        self.score_combination = nn.Linear(num_heads, 1, bias=False)\n",
    "    \n",
    "    def forward(self, A, x, mask=None, residual=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the Multi-Head Attention module.\n",
    "        \n",
    "        Parameters:\n",
    "        - Q: Query tensor of shape (batch_size, seq_len_q, d_model)\n",
    "        - K: Key tensor of shape (batch_size, seq_len_k, d_model)\n",
    "        - V: Value tensor of shape (batch_size, seq_len_v, d_model)\n",
    "        - mask: Optional mask tensor of shape (batch_size, seq_len_q, seq_len_k)\n",
    "        - residual: Optional residual connection\n",
    "        \n",
    "        Returns:\n",
    "        - output: Output tensor of shape (batch_size, seq_len_q, d_model)\n",
    "        - attention: Attention weights of shape (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "     \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # If residual connection is not provided, use Q as residual\n",
    "        if residual is None:\n",
    "            residual = x\n",
    "\n",
    "        # Linear projections and reshaping for multi-head attention\n",
    "        # Shape: (batch_size, seq_len, num_heads, d_*)\n",
    "\n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "\n",
    "\n",
    "        q = q.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        k = k.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        v = v.view(batch_size, -1, self.num_heads, self.d_v)\n",
    "\n",
    "\n",
    "\n",
    "        # Transpose to shape: (batch_size, num_heads, seq_len, d_*)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            # Add an extra dimension for the number of heads\n",
    "            if mask.dim() == 3:  # (batch_size, seq_len_q, seq_len_k)\n",
    "                mask = mask.unsqueeze(1)\n",
    "            \n",
    "            # Set masked positions to a large negative value before softmax\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply dropout to attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Calculate weighted sum of values\n",
    "        # (batch_size, num_heads, seq_len_q, d_v)\n",
    "\n",
    "        context = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Transpose and reshape to (batch_size, seq_len_q, num_heads * d_v)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_v)\n",
    "        \n",
    "        # Apply output projection\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        # Apply dropout and residual connection\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        # Combine attention scores from different heads using learned weights\n",
    "        # Transpose scores to have heads dimension last: (batch_size, seq_len_q, seq_len_k, num_heads)\n",
    "        scores = scores.permute(0, 2, 3, 1)\n",
    "        # Apply linear combination: (batch_size, seq_len_q, seq_len_k, 1)\n",
    "        combined_scores = self.score_combination(scores)\n",
    "        # Remove last singleton dimension\n",
    "        combined_scores = combined_scores.squeeze(-1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define sequential model combining embedding and denoising\n",
    "class SequentialDenoisingModel(nn.Module):\n",
    "    def __init__(self, embedding_model, denoising_model, activation = None, asymmetric = False, F = None, D = None):\n",
    "        super(SequentialDenoisingModel, self).__init__()\n",
    "        self.embedding_model = embedding_model\n",
    "        self.denoising_model = denoising_model\n",
    "        self.activation = activation\n",
    "        self.asymmetric = asymmetric\n",
    "\n",
    "        if self.asymmetric:\n",
    "            self.D = D \n",
    "            self.F = F\n",
    "            self.layer_norm = nn.LayerNorm(D)\n",
    "            self.W_x = nn.Parameter(torch.randn(F, D))\n",
    "            self.W_y = nn.Parameter(torch.randn(F, D))\n",
    "        \n",
    "    def forward(self, A):\n",
    "        # First apply embedding model to get node embeddings\n",
    "        E = self.embedding_model(A)\n",
    "\n",
    "        #Then apply denoising model to get denoised embeddings\n",
    "        Z = self.denoising_model(A, E.float())\n",
    "\n",
    "        #Then if we want asymmetry, apply linear transformation to get X and Y\n",
    "        if self.asymmetric:\n",
    "            X = Z @ self.W_x\n",
    "            Y = Z @ self.W_y\n",
    "\n",
    "            X = self.layer_norm(X)\n",
    "            Y = self.layer_norm(Y)\n",
    "        else:\n",
    "            X = Z\n",
    "            Y = Z\n",
    "        \n",
    "        #Then if we want to apply a nonlinearity, apply it otherwise just take outer product of X and Y to get A_recon\n",
    "        if self.activation is None:\n",
    "            A_recon = torch.bmm(X, Y.transpose(1, 2))\n",
    "        else:\n",
    "            A_recon = self.activation(torch.bmm(X, Y.transpose(1, 2)))\n",
    "                                                                                                                                                                                                                                                                                                        \n",
    "        return A_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assorted functions\n",
    "\n",
    "def generate_sbm_adjacency(block_sizes, p, q, rng=None):\n",
    "    \"\"\"\n",
    "    Generate an adjacency matrix for a stochastic block model with variable block sizes.\n",
    "\n",
    "    Parameters:\n",
    "    - block_sizes: List of sizes for each block.\n",
    "    - p: Probability of intra-block edges.\n",
    "    - q: Probability of inter-block edges.\n",
    "    - rng: Random number generator (optional).\n",
    "\n",
    "    Returns:\n",
    "    - Adjacency matrix as a numpy array.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    n_blocks = len(block_sizes)\n",
    "    n = sum(block_sizes)\n",
    "\n",
    "    # Initialize the adjacency matrix with zeros\n",
    "    \n",
    "    adj_matrix = np.zeros((n, n))\n",
    "\n",
    "    # Calculate the starting index of each block\n",
    "    block_starts = [0]\n",
    "    for i in range(n_blocks-1):\n",
    "        block_starts.append(block_starts[-1] + block_sizes[i])\n",
    "\n",
    "    for i in range(n_blocks):\n",
    "        for j in range(i, n_blocks):\n",
    "            density = p if i == j else q\n",
    "            block_start_i = block_starts[i]\n",
    "            block_end_i = block_start_i + block_sizes[i]\n",
    "            block_start_j = block_starts[j]\n",
    "            block_end_j = block_start_j + block_sizes[j]\n",
    "\n",
    "            # Generate random edges within or between blocks\n",
    "            block_i_size = block_sizes[i]\n",
    "            block_j_size = block_sizes[j]\n",
    "            adj_matrix[block_start_i:block_end_i, block_start_j:block_end_j] = (\n",
    "                rng.random((block_i_size, block_j_size)) < density\n",
    "            ).astype(int)\n",
    "\n",
    "            # Make the matrix symmetric (for undirected graphs)\n",
    "            if i != j:\n",
    "                adj_matrix[block_start_j:block_end_j, block_start_i:block_end_i] = (\n",
    "                    adj_matrix[block_start_i:block_end_i, block_start_j:block_end_j].T\n",
    "                )\n",
    "\n",
    "    return adj_matrix\n",
    "\n",
    "def add_digress_noise(A, p, rng=None, symmetric=False):\n",
    "    \"\"\"\n",
    "    Add noise to an adjacency matrix by flipping edges with probability p.\n",
    "\n",
    "    Parameters:\n",
    "    - A: A 2D numpy array or tensor representing an adjacency matrix (0s and 1s)\n",
    "    - p: Probability of flipping each element (0 to 1, 1 becomes 0 and 0 becomes 1)\n",
    "    - rng: Random number generator (optional)\n",
    "    - symmetric: If True, noise pattern will be symmetric (upper-tri noise mirrored to lower-tri)\n",
    "\n",
    "    Returns:\n",
    "    - Noisy adjacency matrix with some edges flipped\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    A_tensor = torch.tensor(A)\n",
    "    device = A_tensor.device if hasattr(A_tensor, \"device\") else \"cpu\"\n",
    "\n",
    "    # Generate random values for each matrix element\n",
    "    if symmetric:\n",
    "        # Only generate random mask for upper triangle (including diagonal)\n",
    "        n = A_tensor.shape[0]\n",
    "        random_values_upper = torch.rand((n, n), device=device)\n",
    "        # Keep only the upper triangle\n",
    "        upper_tri_mask = torch.triu(torch.ones((n, n), device=device, dtype=bool))\n",
    "        random_values_upper[~upper_tri_mask] = 1.1  # Set below-tri elements > p\n",
    "        flip_mask_upper = random_values_upper < p\n",
    "        \n",
    "        # Build full flip mask by mirroring upper to lower, keeping symmetry\n",
    "        flip_mask = flip_mask_upper | flip_mask_upper.t()\n",
    "        # Ensure diagonal follows upper (optional: could also do flip_mask.diagonal().fill_(value) if needed)\n",
    "    else:\n",
    "        random_values = torch.rand_like(A_tensor)\n",
    "        flip_mask = random_values < p\n",
    "\n",
    "    # Flip elements using mask\n",
    "    A_noisy = torch.where(flip_mask, 1 - A_tensor, A_tensor)\n",
    "\n",
    "    l, V = torch.linalg.eigh(A_noisy.float())\n",
    "\n",
    "    return torch.tensor(A_noisy, dtype=torch.float32), torch.tensor(V, dtype=torch.float32), torch.tensor(l, dtype=torch.float32)\n",
    "\n",
    "def generate_block_sizes(n, min_blocks=2, max_blocks=4, min_size=2, max_size=15):\n",
    "    # Example usage:\n",
    "    # n is the number of nodes\n",
    "    # n = 20\n",
    "    # partitions = generate_block_sizes(n)\n",
    "    # print(f\"Valid block size partitions for n={n}:\")\n",
    "    # for p in partitions:\n",
    "    #     print(p)\n",
    "    valid_partitions = []\n",
    "    \n",
    "    # Try different numbers of blocks\n",
    "    for num_blocks in range(min_blocks, max_blocks + 1):\n",
    "        def generate_partitions(remaining, blocks_left, current_partition):\n",
    "            # Base cases\n",
    "            if blocks_left == 0:\n",
    "                if remaining == 0:\n",
    "                    valid_partitions.append(current_partition[:])\n",
    "                return\n",
    "            \n",
    "            # Try different sizes for current block\n",
    "            start = max(min_size, remaining - (blocks_left-1)*max_size)\n",
    "            end = min(max_size, remaining - (blocks_left-1)*min_size) + 1\n",
    "            \n",
    "            for size in range(start, end):\n",
    "                if size <= remaining:\n",
    "                    current_partition.append(size)\n",
    "                    generate_partitions(remaining - size, blocks_left - 1, current_partition)\n",
    "                    current_partition.pop()\n",
    "        \n",
    "        generate_partitions(n, num_blocks, [])\n",
    "    \n",
    "    return valid_partitions\n",
    "\n",
    "class PermutedAdjacencyDataset(Dataset):\n",
    "    def __init__(self, adjacency_matrices, num_samples):\n",
    "        self.adjacency_matrices = adjacency_matrices\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Randomly choose between A_1 and A_2\n",
    "        matrix_idx = torch.randint(0, len(self.adjacency_matrices), (1,)).item()\n",
    "        adjacency_matrix = self.adjacency_matrices[matrix_idx]\n",
    "        \n",
    "        # Generate random permutation\n",
    "        permuted_indices = torch.randperm(adjacency_matrix.size(0))\n",
    "        A_permuted = adjacency_matrix[permuted_indices, :][:, permuted_indices]\n",
    "        return A_permuted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "random.seed(42)  # For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#hyperparameters\n",
    "#######################################\n",
    "use_wandb = False\n",
    "\n",
    "#number of nodes\n",
    "n = 20\n",
    "\n",
    "#GNN parameters\n",
    "gnn_num_layers = 2\n",
    "gnn_num_terms = 2\n",
    "gnn_feature_dim_in = 50\n",
    "gnn_feature_dim_out = 10 #this is the low dimensional embedding dimension\n",
    "\n",
    "#transformer parameters\n",
    "transformer_num_layers = 1\n",
    "transformer_num_heads = 1\n",
    "transformer_d_k = 10\n",
    "transformer_d_v = 10\n",
    "\n",
    "loss_type = \"MSE\" #loss criteria: either \"MSE\" or \"BCE\"\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "noise_levels = [0.2, 0.2] \n",
    "num_samples = 128  # Define the number of samples you want\n",
    "num_epochs = 500\n",
    "test_epochs = 10\n",
    "train_batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "embedding_type = \"gaussian\"\n",
    "dropout = 0.0\n",
    "num_partitions = 1\n",
    "\n",
    "p_intra = 1.0\n",
    "q_inter = 0.0\n",
    "########################################\n",
    "\n",
    "#gives all possible partitions of n into 2-4 blocks\n",
    "partitions = generate_block_sizes(n)\n",
    "\n",
    "# Sample num_partitions many partitions for training and num_partitions many partitions for test\n",
    "train_partitions = random.sample(partitions, num_partitions)\n",
    "test_partitions = random.sample([p for p in partitions if p not in train_partitions], num_partitions)\n",
    "\n",
    "As_train = []\n",
    "As_test = []\n",
    "\n",
    "for p in train_partitions:\n",
    "    A = generate_sbm_adjacency(p, p_intra, q_inter)\n",
    "    A = torch.tensor(A, dtype=torch.float)\n",
    "    As_train.append(A)\n",
    "\n",
    "for p in test_partitions:\n",
    "    A = generate_sbm_adjacency(p, p_intra, q_inter)\n",
    "    A = torch.tensor(A, dtype=torch.float)\n",
    "    As_test.append(A)\n",
    "\n",
    "print(\"\\nTraining partitions:\")\n",
    "for p in train_partitions:\n",
    "    print(p)\n",
    "    \n",
    "print(\"\\nTest partitions:\") \n",
    "for p in test_partitions:\n",
    "    print(p)\n",
    "\n",
    "train_dataset = PermutedAdjacencyDataset(As_train, num_samples)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = PermutedAdjacencyDataset(As_test, num_samples)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "level = 0.1\n",
    "noise_levels = [level, level]\n",
    "num_eigenvectors = 10\n",
    "H = 2 #number of heads in the transformer\n",
    "\n",
    "\n",
    "\n",
    "for layer_norm in [True]:\n",
    "\n",
    "    for final_nonlinearity in [True]:\n",
    "\n",
    "        for asymmetric in [True]:\n",
    "\n",
    "            for embedding_type in [\"eigen\"]:\n",
    "\n",
    "                for model_type in ['GraphConvolutionFilter']:\n",
    "\n",
    "                    print('layer_norm: ', layer_norm)\n",
    "                    print('final_nonlinearity: ', final_nonlinearity)\n",
    "                    print('asymmetric: ', asymmetric)\n",
    "                    print('embedding_type: ', embedding_type)\n",
    "                    print('model_type: ', model_type)\n",
    "\n",
    "            \n",
    "                    if use_wandb:\n",
    "                        wandb.init(project=\"graph-denoising\")\n",
    "\n",
    "\n",
    "                    #New Cleaned Up Models\n",
    "\n",
    "                    K = 3 #number of filter taps in the filter bank\n",
    "                    F = 50\n",
    "                    G = 10\n",
    "\n",
    "\n",
    "                    #case 1: simple linear graph filter bank\n",
    "                    if model_type == 'GraphConvolutionFilter':\n",
    "                        if embedding_type == \"gaussian\":\n",
    "                            model_denoiser = GraphConvolutionFilter(K=K, F=F, G=G, layer_norm = layer_norm)\n",
    "                        elif embedding_type == \"eigen\":\n",
    "                            model_denoiser = GraphConvolutionFilter(K=K, F=num_eigenvectors, G=G, layer_norm = layer_norm)\n",
    "                    #case 2: linear PE\n",
    "                    elif model_type == 'LinearPE':\n",
    "                        if embedding_type == \"gaussian\":\n",
    "                            model_denoiser = LinearPE(F=F, D=G, layer_norm = layer_norm)\n",
    "                        elif embedding_type == \"eigen\":\n",
    "                            model_denoiser = LinearPE(F=num_eigenvectors, D=G, layer_norm = layer_norm)\n",
    "                    \n",
    "                    #case 3: self-attention\n",
    "                    elif model_type == 'SelfAttention':\n",
    "                        if embedding_type == \"gaussian\":\n",
    "                            model_denoiser = MultiHeadAttention(d_model=F, num_heads = H, d_k=G, d_v=G)\n",
    "                        elif embedding_type == \"eigen\":\n",
    "                            model_denoiser = MultiHeadAttention(d_model=num_eigenvectors, num_heads = H, d_k=G, d_v=G)\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    if embedding_type == \"gaussian\":\n",
    "                        model_embedding = GaussianEmbedding(num_terms=K, num_channels=F)\n",
    "                    elif embedding_type == \"eigen\":\n",
    "                        model_embedding = EigenEmbedding(num_eigenvectors = num_eigenvectors) \n",
    "\n",
    "\n",
    "                    model_denoiser.float().to(device)\n",
    "                    model_embedding.float().to(device)\n",
    "                    if model_type == 'SelfAttention' and embedding_type == \"gaussian\":\n",
    "                        model = SequentialDenoisingModel(model_embedding, model_denoiser, asymmetric = asymmetric, F = F, D = G)\n",
    "                    else:\n",
    "                        if final_nonlinearity:\n",
    "                            model = SequentialDenoisingModel(model_embedding, model_denoiser, activation = nn.Sigmoid(), asymmetric = asymmetric, F = G, D = G)\n",
    "                        else:\n",
    "                            model = SequentialDenoisingModel(model_embedding, model_denoiser, asymmetric = asymmetric, F = G, D = G)\n",
    "                    model = model.to(device)\n",
    "\n",
    "\n",
    "                    # Log model hyperparameters if using wandb\n",
    "                    if use_wandb:\n",
    "                        wandb.config.update({\n",
    "                            \"loss_type\": loss_type,\n",
    "                            \"train_batch_size\": train_batch_size,\n",
    "                            \"test_batch_size\": test_batch_size,\n",
    "                            \"learning_rate\": learning_rate,\n",
    "                            \"embedding_type\": embedding_type,\n",
    "                            \"num_partitions\": num_partitions,\n",
    "                            \"num_samples\": num_samples,\n",
    "                            \"num_epochs\": num_epochs,\n",
    "                            \"dropout\": dropout,\n",
    "                            \"layer_norm\": layer_norm,\n",
    "                            \"final_nonlinearity\": final_nonlinearity,\n",
    "                            \"asymmetric\": asymmetric,\n",
    "                            \"num_eigenvectors\": num_eigenvectors,\n",
    "                            \"K\": K,\n",
    "                            \"F\": F,\n",
    "                            \"G\": G,\n",
    "                            \"model_type\": model_type,\n",
    "                        })\n",
    "\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1000, T_mult=2)\n",
    "                    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "                    if loss_type == \"MSE\":  \n",
    "                        criterion = nn.MSELoss()\n",
    "                    elif loss_type == \"BCE\":\n",
    "                        criterion = nn.BCELoss()\n",
    "\n",
    "                    #Training loop\n",
    "                    for epoch in tqdm(range(num_epochs), desc=\"Training epochs\"):\n",
    "                        model.train()\n",
    "                        epoch_loss = 0.0\n",
    "                        num_batches = 0\n",
    "                        \n",
    "                        for batch in train_dataloader:\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                            # Sample random noise level for this batch\n",
    "                            eps = np.random.choice(noise_levels)\n",
    "                            batch_noisy = (add_digress_noise(batch, eps))[0]\n",
    "\n",
    "                            batch_noisy = batch_noisy.float().to(device)\n",
    "                            batch = batch.float().to(device)\n",
    "\n",
    "                 \n",
    "\n",
    "                            output = model(batch_noisy)\n",
    "                            \n",
    "                            # Compute loss\n",
    "                            loss = criterion(output, batch)\n",
    "                            epoch_loss += loss.item()\n",
    "                            num_batches += 1\n",
    "                            \n",
    "                            # Backward pass and optimization\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                        \n",
    "                        # Step the scheduler\n",
    "                        scheduler.step()\n",
    "                        \n",
    "                        # Calculate average epoch loss\n",
    "                        avg_epoch_loss = epoch_loss / num_batches\n",
    "                        \n",
    "                        #evaluate test error\n",
    "                        test_loss = 0.0\n",
    "                        num_batches = 0\n",
    "                        model.eval()\n",
    "                        with torch.no_grad():\n",
    "                            for batch in test_dataloader:\n",
    "                                # Sample random noise level for this batch\n",
    "                                eps = np.random.choice(noise_levels)\n",
    "                                batch_noisy = (add_digress_noise(batch, eps))[0]\n",
    "\n",
    "                                batch_noisy = batch_noisy.float().to(device)\n",
    "                                batch = batch.float().to(device)\n",
    "\n",
    "                                output = model(batch_noisy)\n",
    "                                \n",
    "                                # Compute loss\n",
    "                                loss = criterion(output, batch)\n",
    "                                test_loss += loss.item()\n",
    "                                num_batches += 1\n",
    "                        avg_test_loss = test_loss / num_batches\n",
    "                        \n",
    "                        # Log metrics to wandb if enabled\n",
    "                        if use_wandb:\n",
    "                            wandb.log({\n",
    "                                \"train_loss\": avg_epoch_loss,\n",
    "                                \"test_loss\": avg_test_loss,\n",
    "                                \"noise_level\": eps,\n",
    "                                \"learning_rate\": scheduler.get_last_lr()[0]\n",
    "                            }, step=epoch)\n",
    "                        \n",
    "                        if (epoch) % test_epochs == 0:\n",
    "\n",
    "                            if use_wandb:\n",
    "                                #evaluate train and test error per noise level\n",
    "                                model.eval()\n",
    "                                with torch.no_grad():\n",
    "                                    train_losses = []\n",
    "                                    test_losses = []\n",
    "                                    for eps in noise_levels:\n",
    "\n",
    "                                        train_loss = 0.0\n",
    "                                        test_loss = 0.0\n",
    "                                        num_batches = 0\n",
    "\n",
    "                                        for batch in train_dataloader:\n",
    "                                            batch_noisy = (add_digress_noise(batch, eps))[0]\n",
    "                                            batch_noisy = batch_noisy.float().to(device)\n",
    "                                            batch = batch.float().to(device)\n",
    "                                            output = model(batch_noisy)\n",
    "                                            loss = criterion(output, batch)\n",
    "                                            train_loss += loss.item()\n",
    "                                            num_batches += 1\n",
    "\n",
    "                                        train_losses.append(train_loss / num_batches)\n",
    "                                        \n",
    "                                        for batch in test_dataloader:\n",
    "                                            batch_noisy = (add_digress_noise(batch, eps))[0]\n",
    "                                            batch_noisy = batch_noisy.float().to(device)\n",
    "                                            batch = batch.float().to(device)\n",
    "                                            output = model(batch_noisy)\n",
    "                                            loss = criterion(output, batch)\n",
    "                                            test_loss += loss.item()\n",
    "                                            num_batches += 1\n",
    "\n",
    "                                        test_losses.append(test_loss / num_batches)\n",
    "\n",
    "                                        wandb.log({\n",
    "                                            \"eps_\" + str(eps) + \"_train_loss\": train_losses[-1],\n",
    "                                            \"eps_\" + str(eps) + \"_test_loss\": test_losses[-1],\n",
    "                                        }, step=epoch)\n",
    "\n",
    "                                    #visualize the results\n",
    "                                    eps_values = noise_levels\n",
    "                                    # Get reconstructions from the model for different noise levels\n",
    "                                    test_idx = np.random.randint(len(As_test))\n",
    "                                    train_idx = np.random.randint(len(As_train))\n",
    "                                    A_test = As_test[test_idx]\n",
    "                                    A_train = As_train[train_idx]\n",
    "\n",
    "                                    for j, A_orig in enumerate([A_test, A_train]):\n",
    "                                        A_orig = A_orig.unsqueeze(0)\n",
    "\n",
    "                                        l_orig, V_orig = torch.linalg.eigh(A_orig)\n",
    "\n",
    "                                        fig_size = 4\n",
    "\n",
    "                                        # Create a figure with subplots - 2 rows for noisy/recon pairs, 5 columns for noise levels\n",
    "                                        fig, axes = plt.subplots(2, len(eps_values), figsize=(fig_size*len(eps_values), fig_size*2))\n",
    "\n",
    "                                        # Plot pairs of noisy and reconstructed matrices for each noise level\n",
    "                                        for i, eps in enumerate(eps_values):\n",
    "                                            A_noisy = add_digress_noise(A_orig, eps)[0]\n",
    "                                            with torch.no_grad():\n",
    "                                                A_recon = model(A_noisy.float().to(device))[0]\n",
    "\n",
    "                                                l_noisy, V_noisy = torch.linalg.eigh(A_noisy)\n",
    "                                                l_recon, V_recon = torch.linalg.eigh(A_recon)\n",
    "                                            plt.figure()\n",
    "\n",
    "                                            V_orig = V_orig.squeeze(0)\n",
    "                                            V_noisy = V_noisy.squeeze(0)\n",
    "\n",
    "                                            V_orig = (V_orig[:, -3:]).to(device)\n",
    "                                            V_noisy = (V_noisy[:, -3:]).to(device)\n",
    "                                            V_recon = (V_recon[:, -3:]).to(device)\n",
    "\n",
    "                                            d_subspace_noisy_orig = torch.linalg.norm(V_orig@V_orig.T - V_noisy@V_noisy.T)\n",
    "                                            d_subspace_recon_orig = torch.linalg.norm(V_orig@V_orig.T - V_recon@V_recon.T)\n",
    "\n",
    "                                            # plt.stem(l_orig.squeeze(0).detach().cpu().numpy())\n",
    "                                            # plt.show()\n",
    "                                            # plt.stem(l_noisy.squeeze(0).detach().cpu().numpy())\n",
    "                                            # plt.show()\n",
    "                                            # plt.stem(l_recon.detach().cpu().numpy())\n",
    "                                            # plt.show()\n",
    "                                            # assert False\n",
    "\n",
    "                                            # Plot noisy matrix on top row\n",
    "                                            im1 = axes[0,i].imshow(A_noisy.squeeze(0).detach().cpu().numpy(), cmap='viridis')\n",
    "                                            axes[0,i].set_title(f'Noisy (Îµ={eps})')\n",
    "                                            \n",
    "                                            # Plot reconstructed matrix below\n",
    "                                            im2 = axes[1,i].imshow(A_recon.squeeze(0).detach().cpu().numpy(), cmap='viridis')\n",
    "                                            axes[1,i].set_title(f'Reconstructed')\n",
    "\n",
    "                                        if j == 0:\n",
    "                                            wandb.log({\"d_subspace_noisy_orig (test)\": d_subspace_noisy_orig, \"d_subspace_recon_orig (test)\": d_subspace_recon_orig}, step=epoch)\n",
    "                                        else:\n",
    "                                            wandb.log({\"d_subspace_noisy_orig (train)\": d_subspace_noisy_orig, \"d_subspace_recon_orig (train)\": d_subspace_recon_orig}, step=epoch)\n",
    "\n",
    "                                        # Log visualization to wandb if enabled\n",
    "                                        if j == 0:\n",
    "                                            wandb.log({\"Test graph reconstruction\": wandb.Image(fig)}, step=epoch)\n",
    "                                        else:\n",
    "                                            wandb.log({\"Train graph reconstruction\": wandb.Image(fig)}, step=epoch)\n",
    "                                        plt.close(fig)\n",
    "\n",
    "                    # Save the trained model\n",
    "                    torch.save(model.state_dict(), 'models/denoiser_model_1.pt')\n",
    "                    print(\"Model saved successfully to denoiser_model_1.pt\")\n",
    "\n",
    "                    if use_wandb:\n",
    "                        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
