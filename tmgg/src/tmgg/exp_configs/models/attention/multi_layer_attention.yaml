# Multi-layer attention model configuration

_target_: tmgg.experiments.attention_denoising.lightning_module.AttentionDenoisingLightningModule

# Model-specific architecture parameters
d_model: 20
num_heads: 8
num_layers: 8
d_k: null  # Will default to d_model // num_heads
d_v: null  # Will default to d_model // num_heads

# Common parameters from base (explicitly included)
dropout: 0.0
bias: true
learning_rate: 0.001
loss_type: "MSE"
scheduler_config:
  type: "cosine_warmup"
  warmup_fraction: 0.02
  decay_fraction: 0.8
rotation_k: 20

# These will be overridden by data config
noise_type: "digress"
noise_levels: [0.005, 0.02, 0.05, 0.1, 0.25, 0.4, 0.5]
seed: 42