# Small DiGress SBM model with GNN projections for Q/K/V
#
# Architecture: Scaled-down version (4 layers vs 8 in official, proportionally reduced dims)
#               with polynomial GNN convolutions replacing linear Q/K/V projections
# Optimizer: Official DiGress settings (AdamW + amsgrad, LR=0.0002, weight_decay=1e-12)

_target_: tmgg.experiments.digress_denoising.lightning_module.DigressDenoisingLightningModule

# Experiment naming
digress_arch: "small_gnn"
digress_mode: "eigenvec"
model_type: digress_small_gnn_eigenvec

# Official DiGress optimizer settings
learning_rate: 0.0002
weight_decay: 1e-12
optimizer_type: adamw
amsgrad: true

# No scheduler (matches official DiGress)
scheduler_config:
  type: none

# Loss
loss_type: "BCEWithLogits"

# Noise configuration (from data config)
noise_type: ${data.noise_type}
noise_levels: ${data.noise_levels}
seed: 42

# Small model (4 layers, scaled-down dims)
use_eigenvectors: true
k: 50  # Number of eigenvectors (padded if graph < k)
n_layers: 4

hidden_dims:
  dx: 128
  de: 32
  dy: 32
  n_head: 4
  use_gnn_q: true    # GNN for query projection
  use_gnn_k: true    # GNN for key projection
  use_gnn_v: true    # GNN for value projection
  gnn_num_terms: 2   # Polynomial degree for GNN convolutions

# MLP hidden dims (scaled down proportionally)
hidden_mlp_dims:
  X: 64
  E: 32
  y: 64

output_dims:
  X: 0
  E: 1
  y: 0
