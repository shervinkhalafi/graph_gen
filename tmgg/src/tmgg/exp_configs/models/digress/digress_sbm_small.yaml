# Small DiGress model for stage 1 POC with official optimizer settings
#
# Architecture: Scaled-down version (4 layers vs 8 in official SBM config)
# Optimizer: Official DiGress settings (AdamW + amsgrad, LR=0.0002, weight_decay=1e-12)

_target_: tmgg.experiments.digress_denoising.lightning_module.DigressDenoisingLightningModule

# Experiment naming (used by base_config_digress.yaml)
digress_arch: "small"
digress_mode: "eigenvec"
model_type: digress_small_eigenvec

# Official DiGress optimizer settings
learning_rate: 0.0002
weight_decay: 1e-12
optimizer_type: adamw
amsgrad: true

# No scheduler (matches official DiGress)
scheduler_config:
  type: none

# Loss
loss_type: "BCEWithLogits"

# Noise configuration (will be overridden by stage config)
noise_type: "digress"
noise_levels: [0.1]
seed: 42

# Small model for stage 1 (4 layers instead of official 8)
use_eigenvectors: true
k: 50  # Number of eigenvectors (padded if graph < k)
n_layers: 4

hidden_dims:
  dx: 128
  de: 32
  dy: 128
  n_head: 4

# MLP hidden dims (scaled down proportionally from official 128/64/128)
hidden_mlp_dims:
  X: 64
  E: 32
  y: 64

output_dims:
  X: 0
  E: 1
  y: 0
