# Stage 2: Core Validation
# Budget: ~70 GPU-hours (reduced from original 166.5)
# Validates best Stage 1 settings across datasets and compares with DiGress
# Stage 1 findings: linear_pe + lr1e-3 dominates, k=8/16 both good

name: stage2_validation
base_config: base_config_spectral

architectures:
  - models/spectral/linear_pe        # Winner from Stage 1
  - models/spectral/self_attention   # Runner-up for comparison
  - models/digress/digress_transformer  # Baseline comparison

datasets:
  - sbm_default
  # - sbm_n100  # Uncomment when config exists

hyperparameters:
  # Narrowed based on Stage 1 results: lr1e-3 won clearly
  learning_rate: [5e-4, 1e-3]
  weight_decay: [1e-2, 1e-3]  # Both performed well in Stage 1
  # + prefix needed for Hydra struct mode
  "+model.k": [8, 16, 32]  # Stage 1 showed k=8,16 good, test k=32 too

seeds: [1, 2, 3]

run_id_template: "stage2_{arch}_{data}_{lr}_{wd}_{k}_s{seed}"
