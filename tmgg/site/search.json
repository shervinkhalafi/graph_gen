{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"","level":1,"title":"TMGG Documentation","text":"<p>TMGG is a research framework for graph denoising with attention, GNN, and hybrid architectures. It uses Hydra for reproducible experiments, supports multiple noise models, and can run locally or on the cloud.</p>","path":["TMGG Documentation"],"tags":[]},{"location":"#quick-start","level":2,"title":"Quick start","text":"<pre><code>uv sync\nuv run tmgg-attention\n</code></pre> <p>Need a different model or a custom configuration? Start with the Get started guide and the Configuration reference.</p>","path":["TMGG Documentation"],"tags":[]},{"location":"#whats-in-this-documentation","level":2,"title":"What’s in this documentation","text":"<ul> <li>Get started — install, run a first experiment, and learn the basics</li> <li>Configuration — Hydra config hierarchy and override patterns</li> <li>Experiments — stage-based workflows, outputs, and logging</li> <li>Data — datasets, noise types, and data modules</li> <li>Models — available architectures and parameters</li> <li>Cloud execution — local vs Modal runners and storage backends</li> <li>Architecture — system design and module layout</li> <li>Extending — add new models, datasets, or backends</li> </ul>","path":["TMGG Documentation"],"tags":[]},{"location":"#when-to-use-which-entry-point","level":2,"title":"When to use which entry point","text":"<ul> <li><code>tmgg-attention</code> — attention-based denoising</li> <li><code>tmgg-gnn</code> — GNN-based denoising</li> <li><code>tmgg-hybrid</code> — hybrid GNN + transformer model</li> <li><code>tmgg-digress</code> — DiGress transformer</li> <li><code>tmgg-spectral</code> — spectral positional encoding models</li> </ul> <p>If you’re unsure, start with <code>tmgg-attention</code> and explore overrides in the Configuration guide.</p>","path":["TMGG Documentation"],"tags":[]},{"location":"architecture/","level":1,"title":"Architecture","text":"<p>This document describes the system design, module organization, and how components interact.</p>","path":["Guides","Architecture"],"tags":[]},{"location":"architecture/#directory-structure","level":2,"title":"Directory Structure","text":"<pre><code>src/tmgg/\n├── models/                    # Neural network architectures\n│   ├── base.py                # BaseModel, DenoisingModel base classes\n│   ├── attention/             # Transformer attention models\n│   ├── gnn/                   # Graph neural networks\n│   ├── hybrid/                # GNN + Transformer combinations\n│   ├── layers/                # Shared layers (GCN, MHA, EigenEmbedding)\n│   └── spectral_denoisers/    # Spectral positional encoding models\n├── experiments/               # Experiment runners\n│   ├── attention_denoising/   # Each has lightning_module.py + runner.py\n│   ├── gnn_denoising/\n│   ├── hybrid_denoising/\n│   ├── digress_denoising/\n│   ├── spectral_denoising/\n│   └── stages/                # Multi-stage experiment framework\n├── experiment_utils/          # Shared infrastructure\n│   ├── data/                  # Data loading, generation, noise\n│   ├── cloud/                 # Cloud execution backends\n│   ├── base_lightningmodule.py\n│   ├── run_experiment.py\n│   ├── metrics.py\n│   └── plotting.py\n└── exp_configs/               # Hydra configuration files\n    ├── base_config_*.yaml     # Top-level experiment configs\n    ├── models/                # Model configurations\n    ├── data/                  # Data configurations\n    ├── base/                  # Trainer, logger configs\n    └── stages/                # Stage definitions\n</code></pre>","path":["Guides","Architecture"],"tags":[]},{"location":"architecture/#core-abstractions","level":2,"title":"Core Abstractions","text":"","path":["Guides","Architecture"],"tags":[]},{"location":"architecture/#denoisingmodel","level":3,"title":"DenoisingModel","text":"<p>The base class for all denoising models, defined in <code>src/tmgg/models/base.py</code>. It provides:</p> <ul> <li>Domain transformations (<code>standard</code> or <code>inv-sigmoid</code>) for numerical stability</li> <li>Parameter counting via <code>parameter_count()</code></li> <li>Configuration export via <code>get_config()</code></li> </ul> <pre><code>from tmgg.models.base import DenoisingModel\n\nclass MyModel(DenoisingModel):\n    def __init__(self, ..., domain: str = \"standard\"):\n        super().__init__(domain=domain)\n        # Model setup\n\n    def forward(self, A: torch.Tensor) -&gt; torch.Tensor:\n        A_transformed = self._apply_domain_transform(A)\n        # Process\n        return self._apply_output_transform(output)\n</code></pre>","path":["Guides","Architecture"],"tags":[]},{"location":"architecture/#denoisinglightningmodule","level":3,"title":"DenoisingLightningModule","text":"<p>The PyTorch Lightning base class for experiments, defined in <code>src/tmgg/experiment_utils/base_lightningmodule.py</code>. It handles:</p> <ul> <li>Optimizer and scheduler configuration</li> <li>Noise generation (Gaussian, Rotation, Digress)</li> <li>Training, validation, and test steps</li> <li>Visualization logging</li> </ul> <p>Experiment-specific modules inherit from this and implement <code>_make_model()</code>:</p> <pre><code>class AttentionDenoisingLightningModule(DenoisingLightningModule):\n    def _make_model(self, d_model, num_heads, num_layers, ...):\n        return MultiLayerAttention(d_model, num_heads, num_layers, ...)\n</code></pre>","path":["Guides","Architecture"],"tags":[]},{"location":"architecture/#graphdatamodule","level":3,"title":"GraphDataModule","text":"<p>The data loading abstraction in <code>src/tmgg/experiment_utils/data/data_module.py</code>. Supports:</p> <ul> <li>Multiple dataset types (SBM, NetworkX, PyG, synthetic)</li> <li>Train/val/test splitting</li> <li>Batch loading with noise injection</li> </ul>","path":["Guides","Architecture"],"tags":[]},{"location":"architecture/#cloudrunner-and-cloudrunnerfactory","level":3,"title":"CloudRunner and CloudRunnerFactory","text":"<p>The cloud execution abstraction in <code>src/tmgg/experiment_utils/cloud/</code>. The factory pattern allows registering multiple backends:</p> <pre><code>from tmgg.experiment_utils.cloud import CloudRunnerFactory\n\nrunner = CloudRunnerFactory.create(\"local\")  # or \"modal\"\nresult = runner.run_experiment(config)\n</code></pre>","path":["Guides","Architecture"],"tags":[]},{"location":"architecture/#execution-flow","level":2,"title":"Execution Flow","text":"<p>When you run an experiment:</p> <pre><code>CLI Entry Point (e.g., tmgg-attention)\n    │\n    ▼\n@hydra.main decorator loads configuration\n    │\n    ▼\nrun_experiment(config)\n    ├── set_seed(config.seed)\n    ├── hydra.utils.instantiate(config.data) → GraphDataModule\n    ├── hydra.utils.instantiate(config.model) → LightningModule\n    ├── create_callbacks(config) → [ModelCheckpoint, EarlyStopping, ...]\n    ├── create_loggers(config) → [TensorBoard, WandB, ...]\n    ├── hydra.utils.instantiate(config.trainer) → Trainer\n    ├── maybe_run_sanity_check()\n    ├── trainer.fit(model, datamodule)\n    ├── trainer.test(model, datamodule)\n    └── final_eval() → evaluation at multiple noise levels\n</code></pre> <p>Each step is driven by the Hydra configuration. The <code>_target_</code> fields in YAML configs specify which classes to instantiate.</p>","path":["Guides","Architecture"],"tags":[]},{"location":"architecture/#design-patterns","level":2,"title":"Design Patterns","text":"","path":["Guides","Architecture"],"tags":[]},{"location":"architecture/#factory-pattern","level":3,"title":"Factory Pattern","text":"<p><code>CloudRunnerFactory</code> registers and creates execution backends:</p> <pre><code>CloudRunnerFactory.register(\"modal\", ModalRunner)\nrunner = CloudRunnerFactory.create(\"modal\", **kwargs)\n</code></pre>","path":["Guides","Architecture"],"tags":[]},{"location":"architecture/#strategy-pattern","level":3,"title":"Strategy Pattern","text":"<p>Noise generators, loggers, and optimizers are interchangeable via configuration. The same training loop works with different strategies.</p>","path":["Guides","Architecture"],"tags":[]},{"location":"architecture/#template-method","level":3,"title":"Template Method","text":"<p><code>DenoisingLightningModule</code> defines the training algorithm structure. Subclasses override <code>_make_model()</code> to specify the model architecture.</p>","path":["Guides","Architecture"],"tags":[]},{"location":"architecture/#composition","level":3,"title":"Composition","text":"<p><code>SequentialDenoisingModel</code> composes a GNN embedding model with a transformer denoiser:</p> <pre><code>model = SequentialDenoisingModel(\n    embedding_model=GNN(...),\n    denoising_model=MultiLayerAttention(...)\n)\n</code></pre>","path":["Guides","Architecture"],"tags":[]},{"location":"architecture/#key-files-by-purpose","level":2,"title":"Key Files by Purpose","text":"<p>To understand training flow: - <code>experiment_utils/run_experiment.py</code> - Main orchestration - <code>experiment_utils/base_lightningmodule.py</code> - Training loop</p> <p>To understand models: - <code>models/base.py</code> - Base classes - <code>models/gnn/gnn.py</code> - GNN implementation - <code>models/attention/attention.py</code> - Attention implementation</p> <p>To understand data: - <code>experiment_utils/data/data_module.py</code> - Data loading - <code>experiment_utils/data/noise_generators.py</code> - Noise models</p> <p>To understand configuration: - <code>exp_configs/base_config_*.yaml</code> - Top-level configs - Individual model/data YAML files</p>","path":["Guides","Architecture"],"tags":[]},{"location":"cloud/","level":1,"title":"Cloud Execution","text":"<p>This document covers cloud execution using the CloudRunner abstraction and Modal integration.</p>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#architecture","level":2,"title":"Architecture","text":"<pre><code>ExperimentCoordinator\n    │\n    ├── CloudRunnerFactory (creates runners)\n    │   ├── LocalRunner (built-in, subprocess)\n    │   └── ModalRunner (optional, cloud GPUs)\n    │\n    └── CloudStorage (result persistence)\n        ├── LocalStorage\n        └── S3Storage\n</code></pre>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#cloudrunnerfactory","level":2,"title":"CloudRunnerFactory","text":"<p>The factory pattern allows registering multiple execution backends.</p>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#basic-usage","level":3,"title":"Basic Usage","text":"<pre><code>from tmgg.experiment_utils.cloud import CloudRunnerFactory\n\n# List available backends\nprint(CloudRunnerFactory.available_backends())  # ['local', 'modal']\n\n# Create a runner\nrunner = CloudRunnerFactory.create(\"local\")\n\n# Run an experiment\nresult = runner.run_experiment(config, gpu_type=\"standard\", timeout_seconds=3600)\n</code></pre>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#available-backends","level":3,"title":"Available Backends","text":"Backend Description Installation <code>local</code> Subprocess execution Built-in <code>modal</code> Cloud GPUs via Modal <code>pip install tmgg-modal</code>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#localrunner","level":2,"title":"LocalRunner","text":"<p>The default runner executes experiments in a subprocess. No additional setup required.</p> <pre><code>from tmgg.experiment_utils.cloud import CloudRunnerFactory\n\nrunner = CloudRunnerFactory.create(\"local\", output_dir=\"./results\")\nresult = runner.run_experiment(config)\n</code></pre>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#parameters","level":3,"title":"Parameters","text":"Parameter Type Default Description <code>output_dir</code> Path <code>./outputs</code> Output directory for results","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#modalrunner","level":2,"title":"ModalRunner","text":"<p>Executes experiments on Modal cloud infrastructure with GPU support.</p>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#setup","level":3,"title":"Setup","text":"<ol> <li> <p>Install the Modal package:    <pre><code>pip install tmgg-modal\n</code></pre></p> </li> <li> <p>Authenticate with Modal:    <pre><code>modal token new\n</code></pre></p> </li> <li> <p>The runner auto-registers when <code>tmgg_modal</code> is available.</p> </li> </ol>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#usage","level":3,"title":"Usage","text":"<pre><code>from tmgg.experiment_utils.cloud import CloudRunnerFactory\n\nrunner = CloudRunnerFactory.create(\"modal\")\nresult = runner.run_experiment(\n    config,\n    gpu_type=\"A10G\",\n    timeout_seconds=3600\n)\n</code></pre>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#gpu-types","level":3,"title":"GPU Types","text":"GPU Type Description <code>standard</code> Default GPU (T4 or similar) <code>A10G</code> NVIDIA A10G <code>A100</code> NVIDIA A100","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#experimentcoordinator","level":2,"title":"ExperimentCoordinator","text":"<p>Manages multi-experiment runs with configuration generation and result aggregation.</p>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#basic-usage_1","level":3,"title":"Basic Usage","text":"<pre><code>from tmgg.experiment_utils.cloud import ExperimentCoordinator\n\ncoordinator = ExperimentCoordinator(\n    backend=\"local\",  # or \"modal\"\n    base_config_path=Path(\"exp_configs/\"),\n)\n\n# Run a stage\nresult = coordinator.run_stage(\n    stage_config,\n    base_config,\n    parallelism=4,\n    resume=True,\n)\n</code></pre>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#parameters_1","level":3,"title":"Parameters","text":"Parameter Type Default Description <code>backend</code> str \"local\" Execution backend <code>storage</code> CloudStorage LocalStorage Result storage <code>base_config_path</code> Path None Path to config directory <code>cache_dir</code> Path <code>./cache</code> Local cache directory","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#running-sweeps","level":3,"title":"Running Sweeps","text":"<p>The coordinator generates all combinations of architectures, datasets, hyperparameters, and seeds:</p> <pre><code>from tmgg.experiment_utils.cloud import ExperimentCoordinator, StageConfig\n\nstage = StageConfig(\n    name=\"my_sweep\",\n    architectures=[\"models/gnn/standard\", \"models/attention/multi_layer\"],\n    datasets=[\"data/sbm_default\"],\n    hyperparameter_space={\n        \"learning_rate\": [1e-4, 1e-3],\n        \"model.num_layers\": [4, 8],\n    },\n    seeds=[1, 2, 3],\n)\n\ncoordinator = ExperimentCoordinator(backend=\"modal\")\nresult = coordinator.run_stage(stage, base_config, parallelism=8)\n</code></pre>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#stageconfig","level":2,"title":"StageConfig","text":"<p>Configuration for experimental stages.</p>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#fields","level":3,"title":"Fields","text":"Field Type Description <code>name</code> str Stage identifier <code>architectures</code> list[str] Model config paths <code>datasets</code> list[str] Data config paths <code>hyperparameter_space</code> dict Parameters to sweep <code>num_trials</code> int Bayesian optimization trials <code>seeds</code> list[int] Random seeds <code>gpu_type</code> str GPU tier <code>timeout_seconds</code> int Max runtime per experiment","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#loading-from-yaml","level":3,"title":"Loading from YAML","text":"<pre><code>from tmgg.experiment_utils.cloud import StageConfig\nfrom pathlib import Path\n\nstage = StageConfig.from_yaml(Path(\"exp_configs/stages/stage1_poc.yaml\"))\n</code></pre>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#stageresult","level":2,"title":"StageResult","text":"<p>Aggregated results from a stage run.</p>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#fields_1","level":3,"title":"Fields","text":"Field Type Description <code>stage_name</code> str Stage identifier <code>experiments</code> list[ExperimentResult] Individual results <code>best_config</code> dict Configuration with best val loss <code>best_metrics</code> dict Metrics from best run <code>summary</code> dict Aggregated statistics <code>started_at</code> str Start timestamp <code>completed_at</code> str End timestamp","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#summary-statistics","level":3,"title":"Summary Statistics","text":"<pre><code>result.summary = {\n    \"total_experiments\": 24,\n    \"completed\": 22,\n    \"failed\": 2,\n    \"success_rate\": 0.917,\n    \"mean_duration_seconds\": 342.5,\n    \"total_duration_seconds\": 7535.0,\n}\n</code></pre>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#experimentresult","level":2,"title":"ExperimentResult","text":"<p>Result from a single experiment run.</p> <pre><code>@dataclass\nclass ExperimentResult:\n    run_id: str\n    config: dict\n    metrics: dict          # {\"val_loss\": 0.123, \"test_loss\": 0.145}\n    checkpoint_path: str | None\n    status: str            # \"completed\", \"failed\", \"timeout\"\n    error_message: str | None\n    duration_seconds: float\n</code></pre>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#resuming-experiments","level":2,"title":"Resuming Experiments","text":"<p>The coordinator supports resuming interrupted sweeps:</p> <pre><code>result = coordinator.run_stage(\n    stage_config,\n    base_config,\n    resume=True,  # Skip completed experiments\n)\n</code></pre> <p>Completed experiments are tracked via storage and skipped on resume.</p>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"cloud/#custom-backends","level":2,"title":"Custom Backends","text":"<p>To add a custom execution backend:</p> <pre><code>from tmgg.experiment_utils.cloud import CloudRunner, CloudRunnerFactory\n\nclass MyCloudRunner(CloudRunner):\n    def run_experiment(self, config, gpu_type=\"standard\", timeout_seconds=3600):\n        # Implementation\n        return ExperimentResult(...)\n\n    def run_sweep(self, configs, gpu_type, parallelism, timeout_seconds):\n        # Implementation\n        return [ExperimentResult(...), ...]\n\n    def get_status(self, run_id):\n        return \"running\"  # or \"completed\", \"failed\"\n\n    def cancel(self, run_id):\n        return True\n\n# Register\nCloudRunnerFactory.register(\"mycloud\", MyCloudRunner)\n</code></pre>","path":["Guides","Cloud Execution"],"tags":[]},{"location":"configuration/","level":1,"title":"Configuration","text":"<p>The framework uses Hydra for configuration management. This document covers the config hierarchy, common overrides, and how to create custom configurations.</p>","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#config-hierarchy","level":2,"title":"Config Hierarchy","text":"<pre><code>exp_configs/\n├── base_config_attention.yaml     # Top-level for attention experiments\n├── base_config_gnn.yaml           # Top-level for GNN experiments\n├── base_config_digress.yaml       # Top-level for DiGress experiments\n├── base_config_hybrid.yaml        # Top-level for hybrid experiments\n├── base_config_spectral.yaml      # Top-level for spectral experiments\n├── base/\n│   ├── trainer/default.yaml       # PyTorch Lightning trainer settings\n│   └── logger/                    # tensorboard.yaml, wandb.yaml, csv.yaml\n├── models/\n│   ├── attention/                 # multi_layer_attention.yaml\n│   ├── gnn/                       # standard_gnn.yaml, nodevar_gnn.yaml, symmetric_gnn.yaml\n│   ├── digress/                   # digress_transformer.yaml\n│   ├── hybrid/                    # hybrid_with_transformer.yaml\n│   └── spectral/                  # filter_bank.yaml, linear_pe.yaml, self_attention.yaml\n├── data/\n│   ├── sbm_default.yaml           # SBM with n=20\n│   ├── legacy_match.yaml          # Legacy denoising replication\n│   └── ...\n└── stages/\n    ├── stage1_poc.yaml            # Proof of concept\n    └── stage2_validation.yaml     # Validation\n</code></pre> <p>Each base config composes defaults from subdirectories:</p> <pre><code># base_config_attention.yaml\ndefaults:\n  - models/attention/multi_layer_attention@model\n  - data: sbm_default\n  - base/trainer/default@trainer\n  - base/logger/tensorboard@logger\n  - _self_\n\nexperiment_name: \"attention_denoising\"\nseed: 42\n</code></pre>","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#hydra-override-syntax","level":2,"title":"Hydra Override Syntax","text":"<p>Override parameters from the command line:</p> <pre><code># Simple override\nuv run tmgg-attention model.num_layers=16\n\n# Nested override\nuv run tmgg-gnn model.scheduler_config.T_0=10\n\n# List override (use quotes)\nuv run tmgg-attention 'data.noise_levels=[0.1,0.2,0.3]'\n\n# Switch config group\nuv run tmgg-gnn data=legacy_match\n\n# Switch model variant\nuv run tmgg-gnn model=gnn/nodevar_gnn\n</code></pre>","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#common-overrides","level":2,"title":"Common Overrides","text":"What Override Example Training steps <code>trainer.max_steps=N</code> <code>trainer.max_steps=50000</code> Validation frequency <code>trainer.val_check_interval=N</code> <code>trainer.val_check_interval=500</code> Learning rate <code>model.learning_rate=X</code> <code>model.learning_rate=0.001</code> Batch size <code>data.batch_size=N</code> <code>data.batch_size=64</code> Model layers <code>model.num_layers=N</code> <code>model.num_layers=8</code> Attention heads <code>model.num_heads=N</code> <code>model.num_heads=8</code> Eigenvectors (spectral) <code>model.k=N</code> <code>model.k=50</code> Noise levels <code>'data.noise_levels=[...]'</code> <code>'data.noise_levels=[0.1,0.2]'</code> Noise type <code>data.noise_type=X</code> <code>data.noise_type=gaussian</code> Scheduler type <code>scheduler_config.type=X</code> <code>scheduler_config.type=none</code> Random seed <code>seed=N</code> <code>seed=123</code> Output directory <code>hydra.run.dir=PATH</code> <code>hydra.run.dir=./my_output</code> Logger backend <code>logger=X</code> <code>logger=wandb</code>","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#viewing-configuration","level":2,"title":"Viewing Configuration","text":"<p>View the resolved configuration without running:</p> <pre><code># Print full config\nuv run tmgg-attention --cfg job\n\n# Print specific group\nuv run tmgg-attention --cfg job --package model\n</code></pre>","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#multirun-hyperparameter-sweeps","level":2,"title":"Multirun (Hyperparameter Sweeps)","text":"<p>Run multiple configurations:</p> <pre><code># Sweep over values\nuv run tmgg-attention --multirun model.num_layers=4,8,16\n\n# Multiple parameters\nuv run tmgg-attention --multirun model.num_layers=4,8 model.learning_rate=0.001,0.01\n\n# Grid search (all combinations)\nuv run tmgg-attention --multirun \\\n  model.num_layers=4,8,16 \\\n  model.num_heads=4,8 \\\n  seed=1,2,3\n</code></pre>","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#model-configuration-reference","level":2,"title":"Model Configuration Reference","text":"","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#attention-model","level":3,"title":"Attention Model","text":"<pre><code># models/attention/multi_layer_attention.yaml\n_target_: tmgg.experiments.attention_denoising.lightning_module.AttentionDenoisingLightningModule\n\nd_model: 20           # Model dimension (typically matches num_nodes)\nnum_heads: 8          # Number of attention heads\nnum_layers: 8         # Number of transformer layers\nd_k: null             # Key dimension (defaults to d_model // num_heads)\nd_v: null             # Value dimension (defaults to d_model // num_heads)\ndropout: 0.0          # Dropout rate\nbias: true            # Use bias in linear layers\nlearning_rate: 0.001\nloss_type: \"MSE\"      # MSE or BCE\n</code></pre>","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#gnn-model","level":3,"title":"GNN Model","text":"<pre><code># models/gnn/standard_gnn.yaml\n_target_: tmgg.experiments.gnn_denoising.lightning_module.GNNDenoisingLightningModule\n\nnum_layers: 2         # Number of GCN layers\nnum_terms: 3          # Polynomial filter terms\nfeature_dim_in: 20    # Input feature dimension\nfeature_dim_out: 10   # Output feature dimension\neigenvalue_reg: 0.0   # Eigenvalue regularization (0.001 helps gradient stability)\ndomain: \"standard\"    # standard or inv-sigmoid\n</code></pre>","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#data-configuration","level":3,"title":"Data Configuration","text":"<pre><code># data/sbm_default.yaml\ndataset_name: sbm\ndataset_config:\n  num_nodes: 20\n  p_intra: 1.0        # Intra-block edge probability\n  q_inter: 0.0        # Inter-block edge probability\n  min_blocks: 2\n  max_blocks: 4\n\nnum_samples_per_graph: 1000\nbatch_size: 100\nnoise_type: \"digress\"\nnoise_levels: [0.005, 0.02, 0.05, 0.1, 0.25, 0.4, 0.5]\n</code></pre>","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#trainer-configuration-step-based","level":3,"title":"Trainer Configuration (Step-Based)","text":"<p>Training is configured in steps, not epochs, to decouple from batch size and dataset size.</p> <pre><code># base/trainer/default.yaml\n_target_: pytorch_lightning.Trainer\n\n# Step-based training (no epochs)\nmax_steps: 10000              # Total training steps\nmax_epochs: -1                # Disable epoch-based termination\n\n# Validation in steps\nval_check_interval: 1000      # Validate every 1000 steps\ncheck_val_every_n_epoch: null # Disable epoch-based validation\n\n# Hardware\naccelerator: \"auto\"\ndevices: \"auto\"\nprecision: 32\n\n# Gradient clipping\ngradient_clip_val: 1.0\ngradient_clip_algorithm: \"norm\"\n\nlog_every_n_steps: 1\n</code></pre>","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#learning-rate-scheduler","level":3,"title":"Learning Rate Scheduler","text":"<p>Configured via <code>scheduler_config</code> in base Lightning module:</p> <pre><code># base/lightning_base.yaml\nscheduler_config:\n  type: \"cosine_warmup\"       # \"cosine_warmup\", \"cosine\", \"step\", or null\n  warmup_fraction: 0.02       # 2% of training for linear warmup\n  decay_fraction: 0.8         # LR reaches 0 at 80% of training\n</code></pre> <p>The <code>cosine_warmup</code> scheduler automatically adapts to total training steps. Stages typically disable scheduling (<code>type: none</code>) for simpler optimization.</p>","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#callbacks-configuration-step-based","level":3,"title":"Callbacks Configuration (Step-Based)","text":"<pre><code># base/callbacks/default.yaml\ncallbacks:\n  early_stopping:\n    monitor: val/loss\n    mode: min\n    patience: 10              # Validation checks (× val_check_interval = steps until stop)\n    min_delta: 1e-4\n  checkpoint:\n    monitor: val/loss\n    mode: min\n    save_top_k: 3\n    save_last: true\n    filename: \"model-step={step:06d}-val_loss={val/loss:.4f}\"\n</code></pre>","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#progress-bar-configuration","level":3,"title":"Progress Bar Configuration","text":"<pre><code># base/progress_bar/default.yaml\nprogress_bar:\n  metrics_to_show:\n    - train_loss\n    - val/loss\n  show_epoch: true\n  metrics_format: \".4f\"\n</code></pre>","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#creating-custom-configs","level":2,"title":"Creating Custom Configs","text":"","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#new-model-configuration","level":3,"title":"New Model Configuration","text":"<p>Create a new file in <code>exp_configs/models/</code>:</p> <pre><code># models/attention/large_attention.yaml\n_target_: tmgg.experiments.attention_denoising.lightning_module.AttentionDenoisingLightningModule\n\nd_model: 40\nnum_heads: 16\nnum_layers: 16\ndropout: 0.1\nlearning_rate: 0.0005\n</code></pre> <p>Use it:</p> <pre><code>uv run tmgg-attention model=attention/large_attention\n</code></pre>","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#new-data-configuration","level":3,"title":"New Data Configuration","text":"<p>Create a file in <code>exp_configs/data/</code>:</p> <pre><code># data/sbm_large.yaml\ndataset_name: sbm\ndataset_config:\n  num_nodes: 100\n  p_intra: 0.9\n  q_inter: 0.1\n  min_blocks: 3\n  max_blocks: 6\n\nnum_samples_per_graph: 500\nbatch_size: 32\nnoise_type: \"gaussian\"\nnoise_levels: [0.1, 0.2, 0.3]\n</code></pre> <p>Use it:</p> <pre><code>uv run tmgg-gnn data=sbm_large\n</code></pre>","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#experiment-variation","level":3,"title":"Experiment Variation","text":"<p>Create a file in <code>exp_configs/experiments/</code>:</p> <pre><code># experiments/attention_long_training.yaml\n# @package _global_\ndefaults:\n  - /base_config_attention\n  - _self_\n\nmodel:\n  num_layers: 12\n  dropout: 0.1\n\ntrainer:\n  max_epochs: 1000\n  gradient_clip_val: 0.5\n</code></pre> <p>Use it:</p> <pre><code>uv run tmgg-attention +experiments=attention_long_training\n</code></pre>","path":["Guides","Configuration"],"tags":[]},{"location":"configuration/#environment-variables","level":2,"title":"Environment Variables","text":"<p>Hydra respects standard environment variables:</p> <pre><code># Override output directory\nHYDRA_FULL_ERROR=1 uv run tmgg-attention  # Show full stack traces\n</code></pre>","path":["Guides","Configuration"],"tags":[]},{"location":"data/","level":1,"title":"Data","text":"<p>This document describes the data pipeline, supported datasets, and noise types.</p>","path":["Guides","Data"],"tags":[]},{"location":"data/#data-modules","level":2,"title":"Data Modules","text":"<p>Two data module classes are available, depending on the experimental protocol.</p>","path":["Guides","Data"],"tags":[]},{"location":"data/#singlegraphdatamodule","level":3,"title":"SingleGraphDataModule","text":"<p>For single-graph training protocols (Stages 1, 1.5). All splits use the same graph structure; only noise varies across samples.</p> <p>Location: <code>src/tmgg/experiment_utils/data/single_graph_data_module.py</code></p> <p>Parameters:</p> Parameter Type Default Description <code>graph_type</code> str \"sbm\" Graph type (sbm, erdos_renyi, regular, tree, lfr, ring_of_cliques, pyg_*) <code>n</code> int 50 Number of nodes <code>num_train_samples</code> int 1000 Training samples per epoch (noise realizations) <code>num_val_samples</code> int 100 Validation samples <code>num_test_samples</code> int 100 Test samples <code>batch_size</code> int 16 Batch size <code>same_graph_all_splits</code> bool False If True, val/test use identical graph as training <code>train_seed</code> int 42 Seed for training graph generation <code>val_test_seed</code> int 123 Seed for val/test graphs (when different) <p>Stage protocols: - <code>same_graph_all_splits=True</code> (Stage 1): All splits use identical graph G, only noise varies - <code>same_graph_all_splits=False</code> (Stage 2+): Validation/test use different graphs G', G''</p> <p>Usage:</p> <pre><code>from tmgg.experiment_utils.data import SingleGraphDataModule\n\ndm = SingleGraphDataModule(\n    graph_type=\"sbm\",\n    n=50,\n    num_train_samples=1000,\n    batch_size=16,\n    same_graph_all_splits=True,  # Stage 1 protocol\n    p_intra=0.7,\n    p_inter=0.05,\n    num_blocks=3,\n)\ndm.setup()\n\nfor batch in dm.train_dataloader():\n    clean = batch  # (batch_size, n, n) - noise applied in LightningModule\n</code></pre>","path":["Guides","Data"],"tags":[]},{"location":"data/#graphdatamodule","level":3,"title":"GraphDataModule","text":"<p>For multi-graph training protocols (Stages 2+). Multiple graphs with train/val/test splits.</p> <p>Location: <code>src/tmgg/experiment_utils/data/data_module.py</code></p> <p>Parameters:</p> Parameter Type Default Description <code>dataset_name</code> str required Dataset type (sbm, nx, pyg, synthetic) <code>dataset_config</code> dict required Dataset-specific configuration <code>num_samples_per_graph</code> int 1000 Permutations per base graph <code>batch_size</code> int 100 Batch size <code>val_split</code> float 0.2 Validation set fraction <code>test_split</code> float 0.2 Test set fraction <code>noise_type</code> str \"digress\" Noise model (gaussian, rotation, digress) <code>noise_levels</code> list [0.1] Noise levels to sample from <p>Usage:</p> <pre><code>from tmgg.experiment_utils.data import GraphDataModule\n\ndata_module = GraphDataModule(\n    dataset_name=\"sbm\",\n    dataset_config={\"num_nodes\": 20, \"p_intra\": 1.0, \"q_inter\": 0.0},\n    noise_type=\"digress\",\n    noise_levels=[0.05, 0.1, 0.2],\n)\ndata_module.setup()\n\nfor batch in data_module.train_dataloader():\n    clean, noisy = batch  # Both (batch_size, n, n)\n</code></pre>","path":["Guides","Data"],"tags":[]},{"location":"data/#supported-datasets","level":2,"title":"Supported Datasets","text":"","path":["Guides","Data"],"tags":[]},{"location":"data/#stochastic-block-model-sbm","level":3,"title":"Stochastic Block Model (SBM)","text":"<p>Random graphs with community structure. Nodes are partitioned into blocks, with different edge probabilities within and between blocks.</p> <p>Config: <code>exp_configs/data/sbm_default.yaml</code></p> <pre><code>dataset_name: sbm\ndataset_config:\n  num_nodes: 20\n  p_intra: 1.0       # Edge probability within blocks\n  q_inter: 0.0       # Edge probability between blocks\n  min_blocks: 2\n  max_blocks: 4\n  min_block_size: 2\n  max_block_size: 15\n  num_train_partitions: 10\n  num_test_partitions: 10\n</code></pre>","path":["Guides","Data"],"tags":[]},{"location":"data/#networkx-graphs","level":3,"title":"NetworkX Graphs","text":"<p>Classical graph structures from NetworkX.</p> <p>Configs: <code>exp_configs/data/nx_square.yaml</code>, <code>exp_configs/data/nx_star.yaml</code></p> <pre><code>dataset_name: nx\ndataset_config:\n  graph_type: \"grid_2d\"  # or \"star\", \"cycle\", etc.\n  n: 20\n</code></pre>","path":["Guides","Data"],"tags":[]},{"location":"data/#pyg-benchmark-datasets","level":3,"title":"PyG Benchmark Datasets","text":"<p>Datasets from PyTorch Geometric.</p> <p>Config: <code>exp_configs/data/pyg_enzymes.yaml</code></p> <pre><code>dataset_name: pyg\ndataset_config:\n  name: \"ENZYMES\"  # or \"PROTEINS\", \"QM9\"\n</code></pre>","path":["Guides","Data"],"tags":[]},{"location":"data/#lfr-benchmark-graphs","level":3,"title":"LFR Benchmark Graphs","text":"<p>Lancichinetti-Fortunato-Radicchi benchmark with planted community structure.</p> <p>Config: <code>exp_configs/data/lfr_single_graph.yaml</code></p> <pre><code>graph_type: lfr\nn: 50\ntau1: 3.0          # Exponent for degree distribution\ntau2: 1.5          # Exponent for community size distribution\nmu: 0.1            # Mixing parameter (fraction of inter-community edges)\naverage_degree: 5\nmin_community: 10\n</code></pre>","path":["Guides","Data"],"tags":[]},{"location":"data/#ring-of-cliques","level":3,"title":"Ring of Cliques","text":"<p>Graph constructed by connecting multiple cliques in a ring topology.</p> <p>Config: <code>exp_configs/data/roc_single_graph.yaml</code></p> <pre><code>graph_type: ring_of_cliques\nnum_cliques: 4\nclique_size: 5\n</code></pre>","path":["Guides","Data"],"tags":[]},{"location":"data/#synthetic-graphs","level":3,"title":"Synthetic Graphs","text":"<p>Various random graph models.</p> <pre><code>dataset_name: synthetic\ndataset_config:\n  graph_type: \"erdos_renyi\"  # or \"regular\", \"tree\", \"watts_strogatz\", etc.\n  n: 20\n  num_graphs: 100\n</code></pre>","path":["Guides","Data"],"tags":[]},{"location":"data/#synthetic-graph-generators","level":2,"title":"Synthetic Graph Generators","text":"<p>Available in <code>src/tmgg/experiment_utils/data/synthetic_graphs.py</code>:</p>","path":["Guides","Data"],"tags":[]},{"location":"data/#regular-graphs","level":3,"title":"Regular Graphs","text":"<p>D-regular graphs where every node has exactly d neighbors.</p> <pre><code>from tmgg.experiment_utils.data import generate_regular_graphs\n\ngraphs = generate_regular_graphs(n=20, d=3, num_graphs=100)\n</code></pre>","path":["Guides","Data"],"tags":[]},{"location":"data/#erdos-renyi-graphs","level":3,"title":"Erdős-Rényi Graphs","text":"<p>Random graphs with independent edge probability p.</p> <pre><code>from tmgg.experiment_utils.data import generate_erdos_renyi_graphs\n\ngraphs = generate_erdos_renyi_graphs(n=20, p=0.3, num_graphs=100)\n</code></pre>","path":["Guides","Data"],"tags":[]},{"location":"data/#tree-graphs","level":3,"title":"Tree Graphs","text":"<p>Random trees with n-1 edges.</p> <pre><code>from tmgg.experiment_utils.data import generate_tree_graphs\n\ngraphs = generate_tree_graphs(n=20, num_graphs=100)\n</code></pre>","path":["Guides","Data"],"tags":[]},{"location":"data/#watts-strogatz-graphs","level":3,"title":"Watts-Strogatz Graphs","text":"<p>Small-world graphs with tunable clustering and path length.</p> <pre><code>from tmgg.experiment_utils.data import generate_watts_strogatz_graphs\n\ngraphs = generate_watts_strogatz_graphs(\n    n=20,\n    num_graphs=100,\n    k=4,    # Each node connected to k nearest neighbors\n    p=0.3   # Rewiring probability\n)\n</code></pre>","path":["Guides","Data"],"tags":[]},{"location":"data/#random-geometric-graphs","level":3,"title":"Random Geometric Graphs","text":"<p>Nodes placed uniformly in a unit square, edges between nearby nodes.</p> <pre><code>from tmgg.experiment_utils.data import generate_random_geometric_graphs\n\ngraphs = generate_random_geometric_graphs(\n    n=20,\n    num_graphs=100,\n    radius=0.3  # Edge threshold distance\n)\n</code></pre>","path":["Guides","Data"],"tags":[]},{"location":"data/#configuration-model-graphs","level":3,"title":"Configuration Model Graphs","text":"<p>Random graphs with a specified degree sequence.</p> <pre><code>from tmgg.experiment_utils.data import generate_configuration_model_graphs\n\ngraphs = generate_configuration_model_graphs(\n    n=20,\n    num_graphs=100,\n    degree_sequence=[3] * 20  # All nodes degree 3\n)\n</code></pre>","path":["Guides","Data"],"tags":[]},{"location":"data/#syntheticgraphdataset-class","level":3,"title":"SyntheticGraphDataset Class","text":"<p>Unified interface for all synthetic graphs:</p> <pre><code>from tmgg.experiment_utils.data import SyntheticGraphDataset\n\n# Aliases available: er, ws, rg, cm\ndataset = SyntheticGraphDataset(\"ws\", n=20, num_graphs=100, k=4, p=0.2)\ntrain, val, test = dataset.train_val_test_split()\ntensor = dataset.to_torch()  # Returns torch.Tensor\n</code></pre>","path":["Guides","Data"],"tags":[]},{"location":"data/#noise-types","level":2,"title":"Noise Types","text":"<p>Three noise models are available for training and evaluation.</p>","path":["Guides","Data"],"tags":[]},{"location":"data/#gaussian-noise","level":3,"title":"Gaussian Noise","text":"<p>Additive Gaussian noise to the adjacency matrix.</p> <pre><code>from tmgg.experiment_utils.data import add_gaussian_noise\n\nnoisy = add_gaussian_noise(adjacency, eps=0.1)\n</code></pre> <p>The noise level <code>eps</code> controls the standard deviation.</p>","path":["Guides","Data"],"tags":[]},{"location":"data/#rotation-noise","level":3,"title":"Rotation Noise","text":"<p>Rotates the adjacency matrix in eigenspace using a skew-symmetric matrix. Preserves spectral properties while perturbing structure.</p> <pre><code>from tmgg.experiment_utils.data import add_rotation_noise, random_skew_symmetric_matrix\n\nskew = random_skew_symmetric_matrix(k=20)  # k = num eigenvectors\nnoisy = add_rotation_noise(adjacency, eps=0.1, skew=skew)\n</code></pre>","path":["Guides","Data"],"tags":[]},{"location":"data/#digress-noise","level":3,"title":"Digress Noise","text":"<p>Flips edges with probability proportional to the noise level. Discrete noise model suited for binary adjacency matrices.</p> <pre><code>from tmgg.experiment_utils.data import add_digress_noise\n\nnoisy = add_digress_noise(adjacency, p=0.1)  # 10% flip probability\n</code></pre>","path":["Guides","Data"],"tags":[]},{"location":"data/#configuring-noise","level":2,"title":"Configuring Noise","text":"<p>In YAML configs:</p> <pre><code>noise_type: \"digress\"  # gaussian, rotation, or digress\nnoise_levels: [0.005, 0.02, 0.05, 0.1, 0.25, 0.4, 0.5]\n</code></pre> <p>During training, a noise level is sampled uniformly from <code>noise_levels</code> for each batch.</p>","path":["Guides","Data"],"tags":[]},{"location":"data/#data-flow","level":2,"title":"Data Flow","text":"<ol> <li>Setup: <code>GraphDataModule.setup()</code> loads or generates base graphs</li> <li>Permutation: Each base graph is permuted <code>num_samples_per_graph</code> times</li> <li>Splitting: Data split into train/val/test sets</li> <li>Batching: DataLoader creates batches</li> <li>Noise: Noise applied to each batch during training (clean data preserved for loss computation)</li> </ol>","path":["Guides","Data"],"tags":[]},{"location":"experiments/","level":1,"title":"Experiments","text":"<p>This document covers running experiments, interpreting results, and using the stage-based experiment system.</p>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#running-experiments","level":2,"title":"Running Experiments","text":"","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#basic-usage","level":3,"title":"Basic Usage","text":"<pre><code># Run with default configuration\nuv run tmgg-attention\n\n# Override parameters\nuv run tmgg-gnn trainer.max_epochs=100 model.num_layers=4\n\n# Different data configuration\nuv run tmgg-attention data=legacy_match\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#available-commands","level":3,"title":"Available Commands","text":"Command Model Type Base Config <code>tmgg-attention</code> Multi-layer attention <code>base_config_attention.yaml</code> <code>tmgg-gnn</code> Standard GNN <code>base_config_gnn.yaml</code> <code>tmgg-hybrid</code> GNN + Transformer <code>base_config_hybrid.yaml</code> <code>tmgg-digress</code> DiGress transformer <code>base_config_digress.yaml</code> <code>tmgg-spectral</code> Spectral PE models <code>base_config_spectral.yaml</code>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#sanity-checks","level":3,"title":"Sanity Checks","text":"<p>Validate setup without full training:</p> <pre><code># Run sanity check (tests data loading, forward pass, loss computation)\nuv run tmgg-attention sanity_check=true\n\n# Fast dev run (one batch only)\nuv run tmgg-gnn trainer.fast_dev_run=true\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#output-structure","level":2,"title":"Output Structure","text":"<p>Each run creates a timestamped directory:</p> <pre><code>outputs/YYYY-MM-DD/HH-MM-SS/\n├── config.yaml              # Resolved configuration\n├── checkpoints/\n│   ├── model-epoch=05-val_loss=0.1234.ckpt\n│   ├── model-epoch=10-val_loss=0.0987.ckpt\n│   └── last.ckpt\n├── tensorboard/             # TensorBoard logs (if enabled)\n│   └── events.out.tfevents.*\n└── .hydra/\n    ├── config.yaml          # Original config\n    ├── hydra.yaml           # Hydra settings\n    └── overrides.yaml       # Command-line overrides\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#checkpoints","level":3,"title":"Checkpoints","text":"<p>The framework saves: - Top 3 models by validation loss - Last checkpoint</p> <p>Checkpoint naming: <code>model-epoch={N}-val_loss={X}.ckpt</code></p>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#loading-checkpoints","level":3,"title":"Loading Checkpoints","text":"<pre><code>from tmgg.experiments.attention_denoising.lightning_module import AttentionDenoisingLightningModule\n\nmodel = AttentionDenoisingLightningModule.load_from_checkpoint(\n    \"outputs/.../checkpoints/model-epoch=10-val_loss=0.0987.ckpt\"\n)\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#logging-options","level":2,"title":"Logging Options","text":"","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#tensorboard-default","level":3,"title":"TensorBoard (Default)","text":"<pre><code>uv run tmgg-attention logger=tensorboard\n\n# View logs\ntensorboard --logdir outputs/\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#weights-biases","level":3,"title":"Weights &amp; Biases","text":"<pre><code>uv run tmgg-attention logger=wandb\n\n# Or with project name\nuv run tmgg-attention logger=wandb wandb.project=\"my-project\"\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#csv-logger","level":3,"title":"CSV Logger","text":"<pre><code>uv run tmgg-attention logger=csv\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#multiple-loggers","level":3,"title":"Multiple Loggers","text":"<pre><code>uv run tmgg-attention logger=multi  # TensorBoard + CSV\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#metrics","level":2,"title":"Metrics","text":"<p>The framework tracks these metrics:</p> Metric Description <code>train/loss</code> Training loss per step <code>val/loss</code> Validation loss per epoch <code>test/loss</code> Test loss (final evaluation) <code>train/loss_epoch</code> Average training loss per epoch <p>Additional metrics computed in final evaluation: - Eigenvalue error - Subspace distance - Reconstruction MAE/MSE</p>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#hyperparameter-sweeps","level":2,"title":"Hyperparameter Sweeps","text":"","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#multirun","level":3,"title":"Multirun","text":"<p>Run multiple configurations sequentially:</p> <pre><code># Single parameter sweep\nuv run tmgg-attention --multirun model.num_layers=4,8,16\n\n# Multiple parameters (grid search)\nuv run tmgg-attention --multirun \\\n  model.num_layers=4,8 \\\n  model.learning_rate=0.001,0.01 \\\n  seed=1,2,3\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#grid-search-command","level":3,"title":"Grid Search Command","text":"<pre><code>uv run tmgg-grid-search  # Uses grid search configuration\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#stage-based-experiments","level":2,"title":"Stage-Based Experiments","text":"<p>The stage system runs systematic experiments across architectures and datasets with a fixed compute budget. This allows methodical validation of spectral PE approaches before scaling.</p>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#stage-1-proof-of-concept","level":3,"title":"Stage 1: Proof of Concept","text":"<p>Budget: 4.4 GPU-hours</p> <p>Purpose: Validate that spectral PE architectures can denoise graphs on a single-graph SBM protocol (n=50).</p> <p>Architectures: - Linear PE - Filter Bank - Self-Attention - DiGress (official LR=0.0002) - DiGress (high LR=1e-2, matching spectral)</p> <p>Dataset: SBM n=50, single-graph protocol (same graph for train/val/test, only noise varies)</p> <p>Optimizer: Adam, no weight decay, no LR scheduling</p> <p>Hyperparameter sweep: - learning_rate: [1e-3, 1e-2] - noise_levels: [0.01, 0.05, 0.1, 0.2, 0.3] - model.k: 50 (full spectrum) - 4 trials per configuration, 3 seeds</p> <p>Success criteria: ≥15% improvement over Linear PE baseline on val_loss.</p> <pre><code>uv run tmgg-stage1              # Single run\nuv run tmgg-stage1 sweep=true   # Full sweep\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#stage-1-sanity-constant-noise-memorization","level":3,"title":"Stage 1 Sanity: Constant Noise Memorization","text":"<p>Budget: &lt;20 GPU-minutes</p> <p>Purpose: Validate model functionality by testing memorization of a fixed noisy→clean mapping. This debugging stage verifies gradient flow, model capacity, and optimizer behavior.</p> <p>Architectures: Linear PE, Filter Bank, Self-Attention</p> <p>Key setting: <code>fixed_noise_seed: 42</code> produces identical noise at every training step.</p> <p>Hyperparameters (fixed, no sweep): - learning_rate: 1e-2 - weight_decay: 0.0 - model.k: 50</p> <p>Success criteria: <code>val_accuracy &gt;= 0.99</code></p> <p>If this stage fails, something fundamental is broken in the architecture or optimizer.</p> <pre><code>uv run tmgg-stage1-sanity\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#stage-15-cross-dataset-validation","level":3,"title":"Stage 1.5: Cross-Dataset Validation","text":"<p>Budget: ~19 GPU-hours</p> <p>Purpose: Validate single-graph denoising across diverse graph families to ensure generalization beyond SBM.</p> <p>Architectures: - Linear PE, Filter Bank, Self-Attention (high LR=1e-2, AMSGrad, weight_decay=1e-12) - DiGress variants (official and high LR)</p> <p>Datasets (9 graph types, single-graph protocol): - Synthetic: Erdős-Rényi, d-regular, tree, ring of cliques, LFR, SBM - PyG benchmarks: QM9, ENZYMES, PROTEINS</p> <p>Hyperparameter sweep: - noise_levels: [0.01, 0.1, 0.2] - 4 trials per configuration, 3 seeds</p> <pre><code>uv run tmgg-stage1-5              # Single run\nuv run tmgg-stage1-5 sweep=true   # Full sweep\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#stage-2-core-validation","level":3,"title":"Stage 2: Core Validation","text":"<p>Budget: 166.5 GPU-hours</p> <p>Purpose: Validate generalization across SBM configurations and compare with DiGress baseline.</p> <p>Architectures: Filter Bank, Self-Attention, DiGress (both variants)</p> <p>Datasets: - SBM n=50 (p_in=0.7, p_out=0.05, k=2) - SBM n=100 (p_in=0.7, p_out=0.05, k=3)</p> <p>Hyperparameter sweep: - learning_rate: [1e-3, 1e-2] - model.k: [50, 100] (matching dataset sizes) - noise_levels: [0.01, 0.05, 0.1, 0.2, 0.3] - 12 trials per configuration, 3 seeds - 45-minute timeout per run</p> <p>Testing protocols: - Single-graph: 1000 noise samples, separate val/test seeds - Multi-graph: 100 graphs, 70/10/20 train/val/test split</p> <p>Success criteria: - reconstruction_error &lt; 0.05 - generalization_gap &lt; 0.15 - Match or exceed DiGress baseline</p> <pre><code>uv run tmgg-stage2 sweep=true parallelism=8\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#stage-3-dataset-diversity","level":3,"title":"Stage 3: Dataset Diversity","text":"<p>Budget: 400 GPU-hours (future work)</p> <p>Purpose: Validate across all graph families.</p> <p>Trigger condition: Run only if Stage 2 achieves &lt;3% error on multi-graph and matches DiGress.</p> <p>Architectures: Filter Bank, Self-Attention, DiGress variants</p> <p>Datasets: - SBM: n=50, n=100, n=200 - Erdős-Rényi, d-regular, trees - LFR benchmark (planted community) - Ring of cliques</p> <p>Hyperparameter sweep: - learning_rate: [1e-3, 1e-2] - model.k: [50, 100, 200] - noise_levels: [0.01, 0.05, 0.1, 0.2, 0.3] - 8 trials, 5 seeds</p> <pre><code>uv run tmgg-stage3 sweep=true\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#stage-4-real-world-benchmarks","level":3,"title":"Stage 4: Real-World Benchmarks","text":"<p>Budget: 300 GPU-hours (future work)</p> <p>Purpose: Validate on PyTorch Geometric benchmark datasets.</p> <p>Trigger condition: Run if Stage 3 validates cross-family generalization.</p> <p>Datasets: PyG QM9, ENZYMES, PROTEINS</p> <p>Hyperparameter sweep: - model.k: [16, 32, 64] (variable graph sizes) - 6 trials, 5 seeds - 60-minute timeout, fast GPU tier</p> <pre><code>uv run tmgg-stage4 sweep=true\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#stage-5-full-validation","level":3,"title":"Stage 5: Full Validation","text":"<p>Budget: 1500 GPU-hours (future work, for publication)</p> <p>Purpose: Comprehensive ablations and robustness analysis.</p> <p>Architectures: All (Linear PE, Filter Bank, Self-Attention, DiGress variants)</p> <p>Datasets: All 11 datasets from previous stages</p> <p>Ablation studies: - Spectral polynomial depth: [3, 5, 8] - Eigenvector count: [2, 4, 8, 16, 32, 64] - Attention key dimension: [32, 64, 128]</p> <p>Statistical analysis: - Wilcoxon signed-rank test - Holm-Bonferroni correction - Significance level: 0.05</p> <pre><code>uv run tmgg-stage5 sweep=true\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#stage-configuration","level":3,"title":"Stage Configuration","text":"<p>Stage configs are defined in <code>exp_configs/stage/</code>. Example structure:</p> <pre><code># stage1_poc.yaml\n# @package _global_\ndefaults:\n  - override /data: sbm_single_graph\n\nstage: stage1_poc\n\n# Optimizer settings\nlearning_rate: 1e-2\nweight_decay: 0.0\noptimizer_type: adam\nscheduler_config:\n  type: none\n\nmodel:\n  k: 50\n\nnoise_levels: [0.1]\n\n# Sweep metadata (used by coordinator when sweep=true)\n_sweep_config:\n  architectures:\n    - models/spectral/linear_pe\n    - models/spectral/filter_bank\n    - models/spectral/self_attention\n  hyperparameter_space:\n    learning_rate: [1e-3, 1e-2]\n    noise_levels:\n      - [0.01]\n      - [0.1]\n  num_trials: 4\n  seeds: [1, 2, 3]\n  timeout_seconds: 600\n  success_criteria:\n    metric: val_loss\n    improvement_threshold: 0.15\n    baseline: linear_pe\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#debugging","level":2,"title":"Debugging","text":"","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#verbose-logging","level":3,"title":"Verbose Logging","text":"<pre><code># Show full stack traces\nHYDRA_FULL_ERROR=1 uv run tmgg-attention\n\n# Debug mode\nuv run tmgg-attention trainer.fast_dev_run=true\n</code></pre>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#common-issues","level":3,"title":"Common Issues","text":"<p>CUDA out of memory: Reduce batch size <pre><code>uv run tmgg-attention data.batch_size=32\n</code></pre></p> <p>NaN gradients: Enable eigenvalue regularization <pre><code>uv run tmgg-gnn model.eigenvalue_reg=0.001\n</code></pre></p> <p>Slow training: Check GPU availability <pre><code>uv run tmgg-attention trainer.accelerator=gpu\n</code></pre></p>","path":["Guides","Experiments"],"tags":[]},{"location":"experiments/#reproducibility","level":2,"title":"Reproducibility","text":"<p>For reproducible experiments:</p> <pre><code># Set seed\nuv run tmgg-attention seed=42\n\n# Deterministic mode (slower but reproducible)\nuv run tmgg-attention trainer.deterministic=true\n</code></pre> <p>The full configuration is saved in <code>outputs/.../config.yaml</code> for reproduction.</p>","path":["Guides","Experiments"],"tags":[]},{"location":"extending/","level":1,"title":"Extending the Framework","text":"<p>This document covers how to add new models, datasets, noise types, and execution backends.</p>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#adding-a-new-model","level":2,"title":"Adding a New Model","text":"","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#step-1-create-the-model-class","level":3,"title":"Step 1: Create the Model Class","text":"<p>Create a new file in <code>src/tmgg/models/</code> inheriting from <code>DenoisingModel</code>:</p> <pre><code># src/tmgg/models/mymodels/my_model.py\nimport torch\nimport torch.nn as nn\nfrom tmgg.models.base import DenoisingModel\n\nclass MyModel(DenoisingModel):\n    \"\"\"My custom denoising model.\"\"\"\n\n    def __init__(\n        self,\n        hidden_dim: int,\n        num_layers: int,\n        domain: str = \"standard\",\n    ):\n        super().__init__(domain=domain)\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n\n        # Define layers\n        self.layers = nn.ModuleList([\n            nn.Linear(hidden_dim, hidden_dim)\n            for _ in range(num_layers)\n        ])\n        self.output = nn.Linear(hidden_dim, hidden_dim)\n\n    def forward(self, A: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass.\n\n        Args:\n            A: Adjacency matrix (batch, n, n)\n\n        Returns:\n            Reconstructed adjacency (batch, n, n)\n        \"\"\"\n        # Apply input domain transformation\n        x = self._apply_domain_transform(A)\n\n        # Process\n        for layer in self.layers:\n            x = torch.relu(layer(x))\n        x = self.output(x)\n\n        # Apply output domain transformation\n        return self._apply_output_transform(x)\n\n    def get_config(self) -&gt; dict:\n        \"\"\"Return model configuration.\"\"\"\n        return {\n            \"hidden_dim\": self.hidden_dim,\n            \"num_layers\": self.num_layers,\n            \"domain\": self.domain,\n        }\n</code></pre>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#step-2-create-the-lightning-module","level":3,"title":"Step 2: Create the Lightning Module","text":"<p>Create a Lightning module in <code>src/tmgg/experiments/</code>:</p> <pre><code># src/tmgg/experiments/my_experiment/lightning_module.py\nfrom tmgg.experiment_utils.base_lightningmodule import DenoisingLightningModule\nfrom tmgg.models.mymodels.my_model import MyModel\n\nclass MyLightningModule(DenoisingLightningModule):\n    \"\"\"Lightning module for MyModel.\"\"\"\n\n    def _make_model(\n        self,\n        hidden_dim: int,\n        num_layers: int,\n        domain: str = \"standard\",\n        **kwargs,\n    ) -&gt; MyModel:\n        return MyModel(\n            hidden_dim=hidden_dim,\n            num_layers=num_layers,\n            domain=domain,\n        )\n</code></pre>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#step-3-create-the-runner","level":3,"title":"Step 3: Create the Runner","text":"<pre><code># src/tmgg/experiments/my_experiment/runner.py\nimport hydra\nfrom omegaconf import DictConfig\nfrom tmgg.experiment_utils.run_experiment import run_experiment\n\nCONFIG_PATH = \"../../exp_configs\"\n\n@hydra.main(version_base=\"1.3\", config_path=CONFIG_PATH, config_name=\"base_config_my\")\ndef main(config: DictConfig):\n    return run_experiment(config)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#step-4-create-configuration-files","level":3,"title":"Step 4: Create Configuration Files","text":"<p>Base config (<code>exp_configs/base_config_my.yaml</code>):</p> <pre><code>defaults:\n  - models/my/my_model@model\n  - data: sbm_default\n  - base/trainer/default@trainer\n  - base/logger/tensorboard@logger\n  - _self_\n\nexperiment_name: \"my_experiment\"\nseed: 42\n</code></pre> <p>Model config (<code>exp_configs/models/my/my_model.yaml</code>):</p> <pre><code>_target_: tmgg.experiments.my_experiment.lightning_module.MyLightningModule\n\nhidden_dim: 64\nnum_layers: 4\ndomain: \"standard\"\n\nlearning_rate: 0.001\nloss_type: \"MSE\"\nnoise_type: ${data.noise_type}\nnoise_levels: ${data.noise_levels}\nseed: ${seed}\n</code></pre>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#step-5-add-cli-entry-point","level":3,"title":"Step 5: Add CLI Entry Point","text":"<p>In <code>pyproject.toml</code>:</p> <pre><code>[project.scripts]\ntmgg-my = \"tmgg.experiments.my_experiment.runner:main\"\n</code></pre>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#step-6-export-the-model","level":3,"title":"Step 6: Export the Model","text":"<p>In <code>src/tmgg/models/__init__.py</code>:</p> <pre><code>from tmgg.models.mymodels.my_model import MyModel\n</code></pre>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#adding-a-new-dataset","level":2,"title":"Adding a New Dataset","text":"","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#step-1-create-a-wrapper","level":3,"title":"Step 1: Create a Wrapper","text":"<p>If using external data, create a wrapper in <code>src/tmgg/experiment_utils/data/</code>:</p> <pre><code># src/tmgg/experiment_utils/data/my_dataset.py\nimport torch\nfrom torch.utils.data import Dataset\n\nclass MyDatasetWrapper(Dataset):\n    \"\"\"Wrapper for my custom dataset.\"\"\"\n\n    def __init__(self, data_path: str, num_samples: int = 1000):\n        self.data_path = data_path\n        self.num_samples = num_samples\n        self._load_data()\n\n    def _load_data(self):\n        # Load your data\n        self.graphs = [...]  # List of adjacency matrices\n\n    def __len__(self):\n        return len(self.graphs) * self.num_samples\n\n    def __getitem__(self, idx):\n        graph_idx = idx // self.num_samples\n        return torch.tensor(self.graphs[graph_idx], dtype=torch.float32)\n</code></pre>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#step-2-register-in-graphdatamodule","level":3,"title":"Step 2: Register in GraphDataModule","text":"<p>In <code>src/tmgg/experiment_utils/data/data_module.py</code>, add handling:</p> <pre><code>def _create_dataset(self):\n    if self.dataset_name == \"my_dataset\":\n        from .my_dataset import MyDatasetWrapper\n        return MyDatasetWrapper(**self.dataset_config)\n    # ... existing code\n</code></pre>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#step-3-create-config","level":3,"title":"Step 3: Create Config","text":"<p>Create <code>exp_configs/data/my_data.yaml</code>:</p> <pre><code>dataset_name: my_dataset\ndataset_config:\n  data_path: \"/path/to/data\"\n  num_samples: 1000\n\nbatch_size: 64\nnoise_type: \"gaussian\"\nnoise_levels: [0.1, 0.2, 0.3]\n</code></pre>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#adding-a-new-synthetic-graph-generator","level":2,"title":"Adding a New Synthetic Graph Generator","text":"","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#step-1-implement-the-generator","level":3,"title":"Step 1: Implement the Generator","text":"<p>In <code>src/tmgg/experiment_utils/data/synthetic_graphs.py</code>:</p> <pre><code>def generate_my_graphs(\n    n: int,\n    num_graphs: int,\n    my_param: float = 0.5,\n    seed: int | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Generate my custom graph type.\n\n    Parameters\n    ----------\n    n\n        Number of nodes per graph.\n    num_graphs\n        Number of graphs to generate.\n    my_param\n        My custom parameter.\n    seed\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    np.ndarray\n        Array of shape (num_graphs, n, n) containing adjacency matrices.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    graphs = []\n    for _ in range(num_graphs):\n        # Generate your graph\n        A = np.zeros((n, n))\n        # ... your generation logic\n        graphs.append(A)\n\n    return np.array(graphs)\n</code></pre>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#step-2-register-in-syntheticgraphdataset","level":3,"title":"Step 2: Register in SyntheticGraphDataset","text":"<p>Update the <code>VALID_TYPES</code> and dispatch in <code>SyntheticGraphDataset</code>:</p> <pre><code>VALID_TYPES = {..., \"my_type\", \"mt\"}  # Add type and alias\nTYPE_ALIASES = {..., \"mt\": \"my_type\"}\n\ndef _generate(self):\n    if self.graph_type == \"my_type\":\n        return generate_my_graphs(\n            self.n, self.num_graphs, my_param=self.kwargs.get(\"my_param\", 0.5)\n        )\n</code></pre>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#adding-a-new-noise-type","level":2,"title":"Adding a New Noise Type","text":"","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#step-1-implement-the-noise-function","level":3,"title":"Step 1: Implement the Noise Function","text":"<p>In <code>src/tmgg/experiment_utils/data/noise_generators.py</code>:</p> <pre><code>def add_my_noise(\n    A: torch.Tensor,\n    eps: float,\n    **kwargs,\n) -&gt; torch.Tensor:\n    \"\"\"Apply my custom noise.\n\n    Parameters\n    ----------\n    A\n        Clean adjacency matrix (batch, n, n).\n    eps\n        Noise level.\n\n    Returns\n    -------\n    torch.Tensor\n        Noisy adjacency matrix.\n    \"\"\"\n    # Your noise implementation\n    noise = torch.randn_like(A) * eps\n    return A + noise\n</code></pre>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#step-2-register-in-lightning-module","level":3,"title":"Step 2: Register in Lightning Module","text":"<p>In <code>src/tmgg/experiment_utils/base_lightningmodule.py</code>, add to noise dispatch:</p> <pre><code>def _apply_noise(self, batch, noise_level):\n    if self.noise_type == \"my_noise\":\n        return add_my_noise(batch, noise_level, **self.noise_kwargs)\n    # ... existing code\n</code></pre>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#step-3-export","level":3,"title":"Step 3: Export","text":"<p>In <code>src/tmgg/experiment_utils/data/__init__.py</code>:</p> <pre><code>from .noise_generators import add_my_noise\n</code></pre>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#adding-a-new-cloud-backend","level":2,"title":"Adding a New Cloud Backend","text":"","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#step-1-implement-cloudrunner","level":3,"title":"Step 1: Implement CloudRunner","text":"<pre><code># src/tmgg/experiment_utils/cloud/my_runner.py\nfrom tmgg.experiment_utils.cloud.base import CloudRunner, ExperimentResult\n\nclass MyCloudRunner(CloudRunner):\n    \"\"\"My custom cloud runner.\"\"\"\n\n    def __init__(self, api_key: str, **kwargs):\n        self.api_key = api_key\n\n    def run_experiment(\n        self,\n        config,\n        gpu_type: str = \"standard\",\n        timeout_seconds: int = 3600,\n    ) -&gt; ExperimentResult:\n        # Submit to your cloud\n        job_id = self._submit(config, gpu_type)\n\n        # Wait for completion\n        result = self._wait(job_id, timeout_seconds)\n\n        return ExperimentResult(\n            run_id=job_id,\n            config=config,\n            metrics=result[\"metrics\"],\n            checkpoint_path=result.get(\"checkpoint\"),\n            status=\"completed\",\n            error_message=None,\n            duration_seconds=result[\"duration\"],\n        )\n\n    def run_sweep(\n        self,\n        configs,\n        gpu_type: str = \"standard\",\n        parallelism: int = 4,\n        timeout_seconds: int = 3600,\n    ) -&gt; list[ExperimentResult]:\n        # Run configs in parallel\n        results = []\n        for config in configs:\n            results.append(self.run_experiment(config, gpu_type, timeout_seconds))\n        return results\n\n    def get_status(self, run_id: str) -&gt; str:\n        # Query your cloud\n        return \"completed\"\n\n    def cancel(self, run_id: str) -&gt; bool:\n        # Cancel job\n        return True\n</code></pre>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#step-2-register-the-runner","level":3,"title":"Step 2: Register the Runner","text":"<p>In <code>src/tmgg/experiment_utils/cloud/factory.py</code>:</p> <pre><code>def _try_register_my_cloud() -&gt; None:\n    try:\n        from .my_runner import MyCloudRunner\n        CloudRunnerFactory.register(\"mycloud\", MyCloudRunner)\n    except ImportError:\n        pass\n\n# Call during module initialization\n_try_register_my_cloud()\n</code></pre> <p>Or register at runtime:</p> <pre><code>from tmgg.experiment_utils.cloud import CloudRunnerFactory\nfrom my_package import MyCloudRunner\n\nCloudRunnerFactory.register(\"mycloud\", MyCloudRunner)\n</code></pre>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"extending/#testing-your-extensions","level":2,"title":"Testing Your Extensions","text":"<p>Create tests in <code>tests/</code>:</p> <pre><code># tests/test_my_model.py\nimport pytest\nimport torch\nfrom tmgg.models.mymodels.my_model import MyModel\n\nclass TestMyModel:\n    def test_forward_shape(self):\n        model = MyModel(hidden_dim=20, num_layers=2)\n        x = torch.randn(4, 20, 20)\n        output = model(x)\n        assert output.shape == (4, 20, 20)\n\n    def test_output_range(self):\n        model = MyModel(hidden_dim=20, num_layers=2)\n        model.eval()\n        x = torch.randn(4, 20, 20)\n        output = model(x)\n        # In eval mode with standard domain, output is sigmoid\n        assert torch.all(output &gt;= 0) and torch.all(output &lt;= 1)\n</code></pre> <p>Run tests:</p> <pre><code>uv run pytest tests/test_my_model.py -v\n</code></pre>","path":["Guides","Extending the Framework"],"tags":[]},{"location":"get-started/","level":1,"title":"Get started","text":"<p>This guide walks you through installation and running your first experiment.</p>","path":["Get started"],"tags":[]},{"location":"get-started/#prerequisites","level":2,"title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>uv installed</li> </ul>","path":["Get started"],"tags":[]},{"location":"get-started/#install","level":2,"title":"Install","text":"<p>From the repository root:</p> <pre><code>uv sync\n</code></pre> <p>For development and tests:</p> <pre><code>uv sync --all-extras\n</code></pre>","path":["Get started"],"tags":[]},{"location":"get-started/#run-your-first-experiment","level":2,"title":"Run your first experiment","text":"<pre><code># Attention-based denoising\nuv run tmgg-attention\n</code></pre> <p>Other entry points:</p> <pre><code># GNN-based denoising\nuv run tmgg-gnn\n\n# Hybrid GNN + Transformer\nuv run tmgg-hybrid\n\n# DiGress transformer\nuv run tmgg-digress\n\n# Spectral denoising\nuv run tmgg-spectral\n</code></pre>","path":["Get started"],"tags":[]},{"location":"get-started/#common-overrides","level":2,"title":"Common overrides","text":"<p>TMGG uses Hydra overrides. A few examples:</p> <pre><code># Increase training steps\nuv run tmgg-attention trainer.max_steps=50000\n\n# Change model depth\nuv run tmgg-gnn model.num_layers=8\n\n# Switch dataset configuration\nuv run tmgg-attention data=legacy_match\n</code></pre> <p>Note: training is configured in steps, not epochs.</p>","path":["Get started"],"tags":[]},{"location":"get-started/#next-steps","level":2,"title":"Next steps","text":"<ul> <li>Read Configuration to understand the Hydra hierarchy.</li> <li>See Experiments for stage-based workflows and outputs.</li> <li>Check Data and Models for dataset and architecture details.</li> </ul>","path":["Get started"],"tags":[]},{"location":"models/","level":1,"title":"Models","text":"<p>This document describes the model architectures available in the framework.</p>","path":["Guides","Models"],"tags":[]},{"location":"models/#model-hierarchy","level":2,"title":"Model Hierarchy","text":"<pre><code>BaseModel (src/tmgg/models/base.py)\n└── DenoisingModel\n    ├── MultiLayerAttention (attention/attention.py)\n    ├── GNN (gnn/gnn.py)\n    ├── GNNSymmetric (gnn/gnn_sym.py)\n    ├── NodeVarGNN (gnn/nvgnn.py)\n    ├── SequentialDenoisingModel (hybrid/hybrid.py)\n    └── Spectral denoisers (spectral_denoisers/)\n</code></pre> <p>All models inherit from <code>DenoisingModel</code>, which provides domain transformations and configuration utilities.</p>","path":["Guides","Models"],"tags":[]},{"location":"models/#attention-models","level":2,"title":"Attention Models","text":"","path":["Guides","Models"],"tags":[]},{"location":"models/#multilayerattention","level":3,"title":"MultiLayerAttention","text":"<p>Multi-layer transformer attention for denoising adjacency matrices. Processes the full adjacency matrix through stacked self-attention layers.</p> <p>Location: <code>src/tmgg/models/attention/attention.py</code></p> <p>Parameters:</p> Parameter Type Default Description <code>d_model</code> int required Model dimension (typically num_nodes) <code>num_heads</code> int required Number of attention heads <code>num_layers</code> int required Number of transformer layers <code>d_k</code> int None Key dimension (defaults to d_model // num_heads) <code>d_v</code> int None Value dimension (defaults to d_model // num_heads) <code>dropout</code> float 0.0 Dropout rate <code>bias</code> bool True Use bias in linear layers <code>domain</code> str \"standard\" Domain transformation <p>Config: <code>exp_configs/models/attention/multi_layer_attention.yaml</code></p> <p>Usage:</p> <pre><code>from tmgg.models.attention import MultiLayerAttention\n\nmodel = MultiLayerAttention(\n    d_model=20,\n    num_heads=4,\n    num_layers=4,\n)\noutput = model(adjacency_matrix)  # (batch, n, n) -&gt; (batch, n, n)\n</code></pre>","path":["Guides","Models"],"tags":[]},{"location":"models/#gnn-models","level":2,"title":"GNN Models","text":"<p>All GNN models use spectral embeddings via eigendecomposition of the adjacency matrix.</p>","path":["Guides","Models"],"tags":[]},{"location":"models/#gnn","level":3,"title":"GNN","text":"<p>Standard graph neural network with polynomial graph convolution filters.</p> <p>Location: <code>src/tmgg/models/gnn/gnn.py</code></p> <p>Parameters:</p> Parameter Type Default Description <code>num_layers</code> int required Number of GCN layers <code>num_terms</code> int 3 Polynomial filter terms <code>feature_dim_in</code> int 10 Input feature dimension <code>feature_dim_out</code> int 10 Output feature dimension <code>eigenvalue_reg</code> float 0.0 Eigenvalue regularization <code>domain</code> str \"standard\" Domain transformation <p>Config: <code>exp_configs/models/gnn/standard_gnn.yaml</code></p> <p>Returns: Tuple of (X, Y) embeddings for reconstruction via X @ Y.T</p>","path":["Guides","Models"],"tags":[]},{"location":"models/#gnnsymmetric","level":3,"title":"GNNSymmetric","text":"<p>Symmetric GNN with shared X and Y embeddings. Uses a single embedding pathway, making the reconstruction inherently symmetric.</p> <p>Location: <code>src/tmgg/models/gnn/gnn_sym.py</code></p> <p>Config: <code>exp_configs/models/gnn/symmetric_gnn.yaml</code></p> <p>Returns: Tuple of (reconstructed_adjacency, X_embeddings)</p>","path":["Guides","Models"],"tags":[]},{"location":"models/#nodevargnn","level":3,"title":"NodeVarGNN","text":"<p>Node-variant GNN with per-node filter coefficients. Allows heterogeneous processing across nodes.</p> <p>Location: <code>src/tmgg/models/gnn/nvgnn.py</code></p> <p>Config: <code>exp_configs/models/gnn/nodevar_gnn.yaml</code></p>","path":["Guides","Models"],"tags":[]},{"location":"models/#hybrid-models","level":2,"title":"Hybrid Models","text":"","path":["Guides","Models"],"tags":[]},{"location":"models/#sequentialdenoisingmodel","level":3,"title":"SequentialDenoisingModel","text":"<p>Combines a GNN embedding model with a transformer denoising model. The GNN extracts node embeddings, which are then processed by attention layers.</p> <p>Location: <code>src/tmgg/models/hybrid/hybrid.py</code></p> <p>Usage:</p> <pre><code>from tmgg.models.hybrid import create_sequential_model\n\nmodel = create_sequential_model(\n    gnn_config={\"num_layers\": 2, \"feature_dim_out\": 10},\n    transformer_config={\"num_heads\": 4, \"num_layers\": 2}\n)\n</code></pre> <p>Config: <code>exp_configs/models/hybrid/hybrid_with_transformer.yaml</code></p>","path":["Guides","Models"],"tags":[]},{"location":"models/#spectral-denoisers","level":2,"title":"Spectral Denoisers","text":"<p>Models operating in the spectral domain, using the eigendecomposition of the noisy adjacency matrix. All output raw logits; use <code>model.predict(logits)</code> for [0,1] probabilities.</p> <p>Location: <code>src/tmgg/models/spectral_denoisers/</code></p>","path":["Guides","Models"],"tags":[]},{"location":"models/#linearpe","level":3,"title":"LinearPE","text":"<p>Linear transformation in eigenspace with optional bias correction.</p> <p>Formula: <pre><code>Â = V W V^T + 1 b^T + b 1^T\n</code></pre></p> <p>where V ∈ R^{n×k} are top-k eigenvectors, W ∈ R^{k×k} is learnable, and b ∈ R^{max_n} is a bias vector capturing node-specific degree corrections.</p> <p>Parameters:</p> Parameter Type Default Description <code>k</code> int required Number of eigenvectors <code>max_nodes</code> int 200 Maximum graph size (for bias) <code>use_bias</code> bool True Enable node-specific bias term <p>Config: <code>exp_configs/models/spectral/linear_pe.yaml</code></p>","path":["Guides","Models"],"tags":[]},{"location":"models/#graphfilterbank","level":3,"title":"GraphFilterBank","text":"<p>Spectral polynomial filter with learnable coefficient matrices.</p> <p>Formula: <pre><code>W = Σ_{ℓ=0}^{K-1} Λ^ℓ ⊙ H^{(ℓ)}\nÂ = V W V^T\n</code></pre></p> <p>where H^{(ℓ)} ∈ R^{k×k} are learnable coefficient matrices. The polynomial filter allows learning frequency-dependent transformations: different eigenvalue magnitudes can be amplified or attenuated differently.</p> <p>Parameters:</p> Parameter Type Default Description <code>k</code> int required Number of eigenvectors <code>polynomial_degree</code> int 5 Polynomial degree K <p>Config: <code>exp_configs/models/spectral/filter_bank.yaml</code></p>","path":["Guides","Models"],"tags":[]},{"location":"models/#selfattentiondenoiser","level":3,"title":"SelfAttentionDenoiser","text":"<p>Scaled dot-product attention on eigenvector embeddings.</p> <p>Formula: <pre><code>Q = V W_Q,  K = V W_K\nÂ = Q K^T / √d_k\n</code></pre></p> <p>where W_Q, W_K ∈ R^{k×d_k} are learnable projections. The 1/√d_k scaling stabilizes gradients following transformer practice.</p> <p>Parameters:</p> Parameter Type Default Description <code>k</code> int required Number of eigenvectors (input dim) <code>d_k</code> int 64 Key/query dimension <p>Config: <code>exp_configs/models/spectral/self_attention.yaml</code></p>","path":["Guides","Models"],"tags":[]},{"location":"models/#digress-models","level":2,"title":"DiGress Models","text":"<p>Diffusion-based transformer for graph denoising. Used as baseline for comparing spectral architectures.</p> <p>Location: <code>src/tmgg/models/digress/</code></p>","path":["Guides","Models"],"tags":[]},{"location":"models/#digress-transformer","level":3,"title":"DiGress Transformer","text":"<p>Score-based model operating on graph structure. Two variants are used in experiments:</p> <p>Official settings (<code>digress_sbm_small.yaml</code>): - LR=0.0002, AdamW + amsgrad, weight_decay=1e-12 - No LR scheduling</p> <p>High LR variant (<code>digress_sbm_small_highlr.yaml</code>): - LR=1e-2 (matching spectral models) - For fair comparison with spectral architectures</p> <p>Parameters:</p> Parameter Type Default Description <code>n_layers</code> int 4 Number of transformer layers <code>node_feature_dim</code> int 50 Node feature dimension <code>use_eigenvectors</code> bool True Use eigenvector features <code>hidden_dims.dx</code> int 128 Node hidden dimension <code>hidden_dims.de</code> int 32 Edge hidden dimension <code>hidden_dims.n_head</code> int 4 Number of attention heads <p>Config: <code>exp_configs/models/digress/digress_sbm_small.yaml</code></p>","path":["Guides","Models"],"tags":[]},{"location":"models/#baseline-models","level":2,"title":"Baseline Models","text":"<p>Simple baselines for comparison.</p> <p>Location: <code>src/tmgg/experiments/baselines/</code></p>","path":["Guides","Models"],"tags":[]},{"location":"models/#linear-baseline","level":3,"title":"Linear Baseline","text":"<p>Linear transformation initialized at identity.</p> <p>Formula: <pre><code>A_pred = W @ A @ W^T + b\n</code></pre></p> <p>Config: <code>exp_configs/models/baselines/linear.yaml</code></p>","path":["Guides","Models"],"tags":[]},{"location":"models/#mlp-baseline","level":3,"title":"MLP Baseline","text":"<p>Multi-layer perceptron applied to flattened adjacency.</p> <p>Config: <code>exp_configs/models/baselines/mlp.yaml</code></p>","path":["Guides","Models"],"tags":[]},{"location":"models/#layers","level":2,"title":"Layers","text":"<p>Shared layers used by the models:</p>","path":["Guides","Models"],"tags":[]},{"location":"models/#eigenembedding","level":3,"title":"EigenEmbedding","text":"<p>Computes eigenvector embeddings from adjacency matrices.</p> <p>Location: <code>src/tmgg/models/layers/eigen_embedding.py</code></p> <p>Parameters:</p> Parameter Type Default Description <code>eigenvalue_reg</code> float 0.0 Diagonal regularization for gradient stability <p>Eigendecomposition gradients can be unstable when eigenvalues are close together. Setting <code>eigenvalue_reg</code> to a small value (e.g., 1e-3) adds a diagonal perturbation that spreads eigenvalues apart.</p>","path":["Guides","Models"],"tags":[]},{"location":"models/#graphconvolutionlayer","level":3,"title":"GraphConvolutionLayer","text":"<p>Polynomial graph convolution layer.</p> <p>Location: <code>src/tmgg/models/layers/gcn.py</code></p>","path":["Guides","Models"],"tags":[]},{"location":"models/#multiheadattention","level":3,"title":"MultiHeadAttention","text":"<p>Standard multi-head attention layer.</p> <p>Location: <code>src/tmgg/models/layers/mha_layer.py</code></p>","path":["Guides","Models"],"tags":[]},{"location":"models/#domain-transformations","level":2,"title":"Domain Transformations","text":"<p>Models support two domain transformations, configured via the <code>domain</code> parameter:</p>","path":["Guides","Models"],"tags":[]},{"location":"models/#standard-domain","level":3,"title":"Standard Domain","text":"<ul> <li>Input: Adjacency matrix used directly</li> <li>Output: Sigmoid applied to produce probabilities in [0, 1]</li> </ul>","path":["Guides","Models"],"tags":[]},{"location":"models/#inv-sigmoid-domain","level":3,"title":"Inv-Sigmoid Domain","text":"<ul> <li>Input: Logit transform applied (numerically stabilized)</li> <li>Output: In training mode, raw logits are returned for BCEWithLogitsLoss; in eval mode, sigmoid is applied</li> </ul> <p>The inv-sigmoid domain can improve numerical stability for sparse graphs.</p> <pre><code># Using inv-sigmoid domain\nmodel = GNN(num_layers=2, domain=\"inv-sigmoid\")\n</code></pre>","path":["Guides","Models"],"tags":[]},{"location":"models/#eigenvalue-regularization","level":2,"title":"Eigenvalue Regularization","text":"<p>GNN models can experience gradient instability due to eigendecomposition. When eigenvalues are close together, gradients involve terms like 1/(λ_i - λ_j) that can explode.</p> <p>To mitigate this, set <code>eigenvalue_reg</code> to a small positive value:</p> <pre><code>model = GNN(num_layers=2, eigenvalue_reg=1e-3)\n</code></pre> <p>This adds <code>eigenvalue_reg * I</code> to the adjacency matrix before eigendecomposition, spreading eigenvalues apart. Use values between 1e-4 and 1e-2 if you observe NaN gradients or unstable training.</p>","path":["Guides","Models"],"tags":[]},{"location":"reference/","level":1,"title":"Reference","text":"<p>This section is generated from docstrings using <code>mkdocstrings</code>. It renders API docs directly from the source in <code>src/tmgg</code>.</p> <p>Start with the top-level package, then drill into models, experiments, data, or cloud utilities.</p>","path":["Reference"],"tags":[]},{"location":"reference/tmgg/","level":1,"title":"tmgg","text":"<p>TMGG: Graph denoising experiments with Hydra and PyTorch Lightning.</p>","path":["Reference","tmgg"],"tags":[]},{"location":"reference/experiment_utils/","level":1,"title":"Experiment utils","text":"<p>Shared utilities for graph denoising experiments.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.DebugCallback","level":2,"title":"<code>DebugCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback for logging detailed training diagnostics.</p> <p>Logs statistics about logits, gradients, prediction errors, and weights to help diagnose training issues like models stuck at 0.5 output.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.DebugCallback--parameters","level":4,"title":"Parameters","text":"<p>log_interval : int     Log statistics every N training steps. Default 50. log_gradients : bool     Whether to log gradient norms. Default True. log_weights : bool     Whether to log weight statistics. Default True. gradient_names : list of str, optional     Specific parameter names to log gradients for. If None, logs all.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.DebugCallback--examples","level":4,"title":"Examples","text":"<p>callback = DebugCallback(log_interval=100) trainer = pl.Trainer(callbacks=[callback])</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.DebugCallback.on_after_backward","level":3,"title":"<code>on_after_backward(trainer, pl_module)</code>","text":"<p>Log gradient statistics after backward pass.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.DebugCallback.on_train_batch_end","level":3,"title":"<code>on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx)</code>","text":"<p>Log statistics after each training batch.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.DebugCallback.on_train_epoch_start","level":3,"title":"<code>on_train_epoch_start(trainer, pl_module)</code>","text":"<p>Log weight statistics at epoch start.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.DigressNoiseGenerator","level":2,"title":"<code>DigressNoiseGenerator</code>","text":"<p>               Bases: <code>NoiseGenerator</code></p> <p>Digress (edge flipping) noise generator (stateless).</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.DigressNoiseGenerator.add_noise","level":3,"title":"<code>add_noise(A, eps)</code>","text":"<p>Add digress noise by flipping edges with probability eps.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.GaussianNoiseGenerator","level":2,"title":"<code>GaussianNoiseGenerator</code>","text":"<p>               Bases: <code>NoiseGenerator</code></p> <p>Gaussian noise generator (stateless).</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.GaussianNoiseGenerator.add_noise","level":3,"title":"<code>add_noise(A, eps)</code>","text":"<p>Add Gaussian noise to adjacency matrix.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.GraphDataset","level":2,"title":"<code>GraphDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Unified dataset for graph adjacency matrices with permutation support.</p> <p>This dataset can handle both single and multiple adjacency matrices, and optionally applies random permutations to increase data diversity.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.GraphDataset.__getitem__","level":3,"title":"<code>__getitem__(idx)</code>","text":"<p>Get a sample from the dataset.</p> <p>Returns:</p> Type Description <code>Union[Tensor, Tuple[Tensor, int]]</code> <p>If return_original_idx is False: permuted adjacency matrix</p> <code>Union[Tensor, Tuple[Tensor, int]]</code> <p>If return_original_idx is True: (permuted adjacency matrix, original matrix index)</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.GraphDataset.__init__","level":3,"title":"<code>__init__(adjacency_matrices, num_samples, apply_permutation=True, return_original_idx=False)</code>","text":"<p>Initialize graph dataset.</p> <p>Parameters:</p> Name Type Description Default <code>adjacency_matrices</code> <code>Union[ndarray, Tensor, List[Union[ndarray, Tensor]]]</code> <p>Single adjacency matrix or list of matrices</p> required <code>num_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>apply_permutation</code> <code>bool</code> <p>Whether to apply random permutations</p> <code>True</code> <code>return_original_idx</code> <code>bool</code> <p>Whether to return the index of the original matrix</p> <code>False</code>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.NoiseGenerator","level":2,"title":"<code>NoiseGenerator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for noise generators.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.NoiseGenerator.requires_state","level":3,"title":"<code>requires_state</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Whether this noise generator maintains internal state.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.NoiseGenerator.add_noise","level":3,"title":"<code>add_noise(A, eps)</code>  <code>abstractmethod</code>","text":"<p>Add noise to an adjacency matrix.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>Input adjacency matrix (can be batched)</p> required <code>eps</code> <code>float</code> <p>Noise level/intensity</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Noisy adjacency matrix</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.RotationNoiseGenerator","level":2,"title":"<code>RotationNoiseGenerator</code>","text":"<p>               Bases: <code>NoiseGenerator</code></p> <p>Rotation noise generator with skew matrix management.</p> <p>This generator maintains a skew-symmetric matrix that is used to rotate eigenvectors, providing a consistent rotation throughout the experiment.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.RotationNoiseGenerator.__init__","level":3,"title":"<code>__init__(k, seed=None)</code>","text":"<p>Initialize rotation noise generator.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Dimension of the skew matrix (number of eigenvectors)</p> required <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducible skew matrix generation</p> <code>None</code>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.RotationNoiseGenerator.add_noise","level":3,"title":"<code>add_noise(A, eps)</code>","text":"<p>Add rotation noise by rotating eigenvectors.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.RotationNoiseGenerator.get_config","level":3,"title":"<code>get_config()</code>","text":"<p>Get configuration for this generator.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.SanityCheckResult","level":2,"title":"<code>SanityCheckResult</code>","text":"<p>Container for sanity check results.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.SanityCheckResult.__str__","level":3,"title":"<code>__str__()</code>","text":"<p>String representation of results.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.SanityCheckResult.add_check","level":3,"title":"<code>add_check(name, passed, message)</code>","text":"<p>Add a check result.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.SanityCheckResult.add_metric","level":3,"title":"<code>add_metric(name, value)</code>","text":"<p>Add a metric value.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.SanityCheckResult.add_warning","level":3,"title":"<code>add_warning(message)</code>","text":"<p>Add a warning message.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.add_digress_noise","level":2,"title":"<code>add_digress_noise(A, p, rng=None)</code>","text":"<p>Add noise to an adjacency matrix by flipping edges with probability p.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Union[Tensor, ndarray]</code> <p>Input adjacency matrix (0s and 1s)</p> required <code>p</code> <code>float</code> <p>Probability of flipping each element</p> required <code>rng</code> <code>Optional[Generator]</code> <p>Random number generator (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Noisy adjacency matrix</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.add_gaussian_noise","level":2,"title":"<code>add_gaussian_noise(A, eps)</code>","text":"<p>Add Gaussian noise to adjacency matrix.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Union[Tensor, ndarray]</code> <p>Input adjacency matrix</p> required <code>eps</code> <code>float</code> <p>Noise level</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Noisy adjacency matrix</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.add_rotation_noise","level":2,"title":"<code>add_rotation_noise(A, eps, skew)</code>","text":"<p>Add rotation noise to adjacency matrix by rotating eigenvectors.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Union[Tensor, ndarray]</code> <p>Input adjacency matrix</p> required <code>eps</code> <code>float</code> <p>Noise level</p> required <code>skew</code> <code>ndarray</code> <p>Skew-symmetric matrix for rotation</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Noisy adjacency matrix</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.check_data_loader","level":2,"title":"<code>check_data_loader(data_loader, expected_batch_size, expected_shape, num_batches_to_check=3)</code>","text":"<p>Verify data loader is producing correct batches.</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>DataLoader</code> <p>DataLoader to test</p> required <code>expected_batch_size</code> <code>int</code> <p>Expected batch size</p> required <code>expected_shape</code> <code>Tuple[int, int]</code> <p>Expected shape of each sample (without batch dim)</p> required <code>num_batches_to_check</code> <code>int</code> <p>Number of batches to check</p> <code>3</code> <p>Returns:</p> Type Description <code>SanityCheckResult</code> <p>SanityCheckResult with test outcomes</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.check_loss_computation","level":2,"title":"<code>check_loss_computation(model, criterion, sample_input, sample_target, device=torch.device('cpu'))</code>","text":"<p>Verify loss computation works correctly.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to test</p> required <code>criterion</code> <code>Module</code> <p>Loss function</p> required <code>sample_input</code> <code>Tensor</code> <p>Sample input</p> required <code>sample_target</code> <code>Tensor</code> <p>Sample target</p> required <code>device</code> <code>device</code> <p>Device to run on</p> <code>device('cpu')</code> <p>Returns:</p> Type Description <code>SanityCheckResult</code> <p>SanityCheckResult with test outcomes</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.check_model_forward_pass","level":2,"title":"<code>check_model_forward_pass(model, input_shape, device=torch.device('cpu'))</code>","text":"<p>Verify model forward pass works correctly.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to test</p> required <code>input_shape</code> <code>Tuple[int, ...]</code> <p>Expected input shape</p> required <code>device</code> <code>device</code> <p>Device to run on</p> <code>device('cpu')</code> <p>Returns:</p> Type Description <code>SanityCheckResult</code> <p>SanityCheckResult with test outcomes</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.check_noise_generator","level":2,"title":"<code>check_noise_generator(noise_generator, sample_size=10, noise_levels=[0.1, 0.3, 0.5])</code>","text":"<p>Verify noise generator is working correctly.</p> <p>Parameters:</p> Name Type Description Default <code>noise_generator</code> <code>NoiseGenerator</code> <p>Noise generator to test</p> required <code>sample_size</code> <code>int</code> <p>Size of test adjacency matrix</p> <code>10</code> <code>noise_levels</code> <code>List[float]</code> <p>Noise levels to test</p> <code>[0.1, 0.3, 0.5]</code> <p>Returns:</p> Type Description <code>SanityCheckResult</code> <p>SanityCheckResult with test outcomes</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.compute_accuracy","level":2,"title":"<code>compute_accuracy(A_true, A_pred, threshold=0.5)</code>","text":"<p>Compute edge prediction accuracy.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.compute_accuracy--parameters","level":4,"title":"Parameters","text":"<p>A_true     Ground truth binary adjacency matrix. A_pred     Predicted probabilities in [0,1]. threshold     Classification threshold for predictions.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.compute_accuracy--returns","level":4,"title":"Returns","text":"<p>float     Fraction of correctly predicted edges.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.compute_batch_metrics","level":2,"title":"<code>compute_batch_metrics(A_true_batch, A_pred_batch)</code>","text":"<p>Compute metrics averaged over a batch.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.compute_batch_metrics--parameters","level":4,"title":"Parameters","text":"<p>A_true_batch     Batch of true adjacency matrices. A_pred_batch     Batch of predicted adjacency matrices (probabilities).</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.compute_batch_metrics--returns","level":4,"title":"Returns","text":"<p>dict     Averaged metrics over the batch, including accuracy.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.compute_eigendecomposition","level":2,"title":"<code>compute_eigendecomposition(A)</code>","text":"<p>Compute eigenvalues and eigenvectors of a symmetric matrix.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Union[Tensor, ndarray]</code> <p>Input matrix (symmetric)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple of (eigenvalues, eigenvectors) where eigenvalues are sorted</p> <code>Tensor</code> <p>in ascending order</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.compute_eigenvalue_error","level":2,"title":"<code>compute_eigenvalue_error(A_true, A_pred, k=4)</code>","text":"<p>Compute normalized eigenvalue error between adjacency matrices.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.compute_eigenvalue_error--parameters","level":4,"title":"Parameters","text":"<p>A_true     True adjacency matrix. A_pred     Predicted adjacency matrix. k     Number of top eigenvalues to compare.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.compute_eigenvalue_error--returns","level":4,"title":"Returns","text":"<p>float     Normalized eigenvalue error: ||λ_pred - λ_true|| / ||λ_true||.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.compute_reconstruction_metrics","level":2,"title":"<code>compute_reconstruction_metrics(A_true, A_pred)</code>","text":"<p>Compute reconstruction metrics for graph denoising.</p> <p>Expects predictions (post-sigmoid, in [0,1] range).</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.compute_reconstruction_metrics--parameters","level":4,"title":"Parameters","text":"<p>A_true     True adjacency matrix (binary). A_pred     Predicted adjacency matrix (probabilities in [0,1]).</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.compute_reconstruction_metrics--returns","level":4,"title":"Returns","text":"<p>dict     Metrics: mse, frobenius_error, eigenvalue_error, subspace_distance, accuracy.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.compute_subspace_distance","level":2,"title":"<code>compute_subspace_distance(A_true, A_pred, k=4)</code>","text":"<p>Compute subspace distance via projection matrix difference.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.compute_subspace_distance--parameters","level":4,"title":"Parameters","text":"<p>A_true     True adjacency matrix. A_pred     Predicted adjacency matrix. k     Number of top eigenvectors to compare.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.compute_subspace_distance--returns","level":4,"title":"Returns","text":"<p>float     Frobenius norm of ||V_true V_true^T - V_pred V_pred^T||.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.create_graph_denoising_figure","level":2,"title":"<code>create_graph_denoising_figure(A_clean, noise_fn, denoise_fn, noise_level, noise_type='Unknown', title_prefix='', cmap='viridis')</code>","text":"<p>Create a matplotlib figure for graph denoising visualization.</p> <p>This is a logger-agnostic version that returns a matplotlib figure.</p> <p>Parameters:</p> Name Type Description Default <code>A_clean</code> <code>Union[ndarray, Tensor]</code> <p>Clean adjacency matrix</p> required <code>noise_fn</code> <code>Callable[[Union[ndarray, Tensor], float], Tuple[Union[ndarray, Tensor], Any, Any]]</code> <p>Function that adds noise</p> required <code>denoise_fn</code> <code>Callable[[Union[ndarray, Tensor]], Union[ndarray, Tensor]]</code> <p>Function that denoises</p> required <code>noise_level</code> <code>float</code> <p>Noise level parameter</p> required <code>noise_type</code> <code>str</code> <p>Name of the noise type</p> <code>'Unknown'</code> <code>title_prefix</code> <code>str</code> <p>Optional prefix for the plot title</p> <code>''</code> <code>cmap</code> <code>str</code> <p>Colormap to use</p> <code>'viridis'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure object</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.create_graph_denoising_wandb_image","level":2,"title":"<code>create_graph_denoising_wandb_image(A_clean, noise_fn, denoise_fn, noise_level, noise_type='Unknown', title_prefix='', cmap='viridis')</code>","text":"<p>Create a wandb Image for graph denoising visualization.</p> <p>This is a wandb-compatible wrapper around plot_graph_denoising_comparison.</p> <p>Parameters:</p> Name Type Description Default <code>A_clean</code> <code>Union[ndarray, Tensor]</code> <p>Clean adjacency matrix</p> required <code>noise_fn</code> <code>Callable[[Union[ndarray, Tensor], float], Tuple[Union[ndarray, Tensor], Any, Any]]</code> <p>Function that adds noise</p> required <code>denoise_fn</code> <code>Callable[[Union[ndarray, Tensor]], Union[ndarray, Tensor]]</code> <p>Function that denoises</p> required <code>noise_level</code> <code>float</code> <p>Noise level parameter</p> required <code>noise_type</code> <code>str</code> <p>Name of the noise type</p> <code>'Unknown'</code> <code>title_prefix</code> <code>str</code> <p>Optional prefix for the plot title</p> <code>''</code> <code>cmap</code> <code>str</code> <p>Colormap to use</p> <code>'viridis'</code> <p>Returns:</p> Type Description <code>Image</code> <p>wandb.Image object</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.create_noise_generator","level":2,"title":"<code>create_noise_generator(noise_type, rotation_k=None, seed=None, **kwargs)</code>","text":"<p>Factory function to create noise generators.</p> <p>Parameters:</p> Name Type Description Default <code>noise_type</code> <code>str</code> <p>Type of noise (\"gaussian\", \"digress\", or \"rotation\")</p> required <code>rotation_k</code> <code>Optional[int]</code> <p>Dimension for rotation noise skew matrix</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducible noise generation</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments (for future extensibility)</p> <code>{}</code> <p>Returns:</p> Type Description <code>NoiseGenerator</code> <p>NoiseGenerator instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If noise_type is unknown or required parameters are missing</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.create_wandb_visualization","level":2,"title":"<code>create_wandb_visualization(A_original, model, noise_function, noise_levels, dataset_type='test', device='cpu')</code>","text":"<p>Create visualizations for Weights &amp; Biases logging.</p> <p>Parameters:</p> Name Type Description Default <code>A_original</code> <code>Tensor</code> <p>Original clean adjacency matrix</p> required <code>model</code> <code>Module</code> <p>Trained denoising model</p> required <code>noise_function</code> <p>Function to add noise</p> required <code>noise_levels</code> <code>List[float]</code> <p>List of noise levels</p> required <code>dataset_type</code> <code>str</code> <p>Type of dataset (\"train\" or \"test\")</p> <code>'test'</code> <code>device</code> <code>str</code> <p>Device for computation</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>Dict[str, Image]</code> <p>Dictionary of wandb Images</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.generate_block_sizes","level":2,"title":"<code>generate_block_sizes(n, min_blocks=2, max_blocks=4, min_size=2, max_size=15)</code>","text":"<p>Generate all valid block size partitions for n nodes.</p> <p>This function finds all ways to partition n nodes into blocks where: - The number of blocks is between min_blocks and max_blocks (inclusive) - Each block has size between min_size and max_size (inclusive) - The sum of all block sizes equals n</p> <p>The algorithm uses recursive backtracking to explore all valid partitions. For each position, it calculates the valid range of sizes based on: - Minimum size: Must be at least min_size, but also large enough that    remaining blocks can fit within max_size constraint - Maximum size: Must be at most max_size, but also small enough that   remaining blocks can satisfy min_size constraint</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Total number of nodes to partition</p> required <code>min_blocks</code> <code>int</code> <p>Minimum number of blocks in a partition</p> <code>2</code> <code>max_blocks</code> <code>int</code> <p>Maximum number of blocks in a partition</p> <code>4</code> <code>min_size</code> <code>int</code> <p>Minimum size for any block</p> <code>2</code> <code>max_size</code> <code>int</code> <p>Maximum size for any block</p> <code>15</code> <p>Returns:</p> Type Description <code>List[List[int]]</code> <p>List of valid block size partitions, where each partition is a list</p> <code>List[List[int]]</code> <p>of integers summing to n</p> Example <p>generate_block_sizes(6, min_blocks=2, max_blocks=3, min_size=2, max_size=4) [[2, 4], [3, 3], [4, 2], [2, 2, 2]]</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.generate_sbm_adjacency","level":2,"title":"<code>generate_sbm_adjacency(block_sizes, p, q, rng=None)</code>","text":"<p>Generate an adjacency matrix for a stochastic block model with variable block sizes.</p> <p>Parameters:</p> Name Type Description Default <code>block_sizes</code> <code>List[int]</code> <p>List of sizes for each block</p> required <code>p</code> <code>float</code> <p>Probability of intra-block edges</p> required <code>q</code> <code>float</code> <p>Probability of inter-block edges</p> required <code>rng</code> <code>Optional[Generator]</code> <p>Random number generator (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Adjacency matrix as a numpy array</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.load_checkpoint_with_fallback","level":2,"title":"<code>load_checkpoint_with_fallback(module_class, checkpoint_path, strict=True, map_location=None, **override_kwargs)</code>","text":"<p>Load a LightningModule from checkpoint with fallback for mismatched hyperparameters.</p> <p>When checkpoint contains hyperparameters that module's init no longer accepts, filters them out and retries loading. This handles backwards compatibility when module signatures change between code versions.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.load_checkpoint_with_fallback--parameters","level":4,"title":"Parameters","text":"<p>module_class     The LightningModule subclass to instantiate. checkpoint_path     Path to the checkpoint file. strict     Whether to strictly enforce state_dict key matching. map_location     Device to map tensors to during loading. **override_kwargs     Additional kwargs to pass to init, overriding checkpoint values.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.load_checkpoint_with_fallback--returns","level":4,"title":"Returns","text":"<p>T     The loaded module instance.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.load_checkpoint_with_fallback--raises","level":4,"title":"Raises","text":"<p>TypeError     If loading fails for reasons other than unexpected keyword arguments.</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.plot_denoising_results","level":2,"title":"<code>plot_denoising_results(A_original, A_noisy, A_denoised, noise_type='Unknown', eps=0.0, save_path=None)</code>","text":"<p>Plot original, noisy, denoised, and delta adjacency matrices side by side.</p> <p>Parameters:</p> Name Type Description Default <code>A_original</code> <code>ndarray</code> <p>Original clean adjacency matrix</p> required <code>A_noisy</code> <code>ndarray</code> <p>Noisy adjacency matrix</p> required <code>A_denoised</code> <code>ndarray</code> <p>Denoised adjacency matrix</p> required <code>noise_type</code> <code>str</code> <p>Type of noise applied</p> <code>'Unknown'</code> <code>eps</code> <code>float</code> <p>Noise level</p> <code>0.0</code> <code>save_path</code> <code>Optional[str]</code> <p>Optional path to save the plot</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Matplotlib figure</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.plot_eigenvalue_comparison","level":2,"title":"<code>plot_eigenvalue_comparison(eigenvals_true, eigenvals_noisy, eigenvals_denoised, save_path=None)</code>","text":"<p>Plot comparison of eigenvalues before and after denoising.</p> <p>Parameters:</p> Name Type Description Default <code>eigenvals_true</code> <code>ndarray</code> <p>True eigenvalues</p> required <code>eigenvals_noisy</code> <code>ndarray</code> <p>Noisy eigenvalues</p> required <code>eigenvals_denoised</code> <code>ndarray</code> <p>Denoised eigenvalues</p> required <code>save_path</code> <code>Optional[str]</code> <p>Optional path to save the plot</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Matplotlib figure</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.plot_eigenvalue_denoising","level":2,"title":"<code>plot_eigenvalue_denoising(A_original, A_noisy, eigenvals_denoised, eigenvecs_noisy, save_path=None)</code>","text":"<p>Plot eigenvalue-only denoising visualization.</p> <p>This shows the adjacency matrix reconstructed using denoised eigenvalues but keeping the noisy eigenvectors, which isolates the effect of eigenvalue denoising.</p> <p>Parameters:</p> Name Type Description Default <code>A_original</code> <code>ndarray</code> <p>Original clean adjacency matrix</p> required <code>A_noisy</code> <code>ndarray</code> <p>Noisy adjacency matrix</p> required <code>eigenvals_denoised</code> <code>ndarray</code> <p>Denoised eigenvalues</p> required <code>eigenvecs_noisy</code> <code>ndarray</code> <p>Noisy eigenvectors</p> required <code>save_path</code> <code>Optional[str]</code> <p>Optional path to save the plot</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Matplotlib figure</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.plot_graph_denoising_comparison","level":2,"title":"<code>plot_graph_denoising_comparison(A_clean, noise_fn, denoise_fn, noise_level, noise_type='Unknown', title_prefix='', cmap='viridis', figsize=(30, 5), save_path=None, vmin=None, vmax=None)</code>","text":"<p>General-purpose visualization for graph denoising experiments.</p> <p>This function creates a 6-panel visualization showing: 1. Clean graph (binary ground truth) 2. Noisy graph (after noise applied) 3. Predicted graph (thresholded at 0.5) 4. Prediction error (binary difference) 5. Logits (raw model output) 6. Logit error (logits - clean)</p> <p>Parameters:</p> Name Type Description Default <code>A_clean</code> <code>Union[ndarray, Tensor]</code> <p>Clean adjacency matrix (numpy array or torch tensor)</p> required <code>noise_fn</code> <code>Callable[[Union[ndarray, Tensor], float], Tuple[Union[ndarray, Tensor], Any, Any]]</code> <p>Function that adds noise. Should accept (matrix, noise_level) and      return noisy_matrix (or tuple with noisy_matrix first)</p> required <code>denoise_fn</code> <code>Callable[[Union[ndarray, Tensor]], Union[ndarray, Tensor, Tuple]]</code> <p>Function that denoises. Should accept noisy matrix and return        either predictions alone or (predictions, logits) tuple</p> required <code>noise_level</code> <code>float</code> <p>Noise level parameter (e.g., epsilon)</p> required <code>noise_type</code> <code>str</code> <p>Name of the noise type for labeling</p> <code>'Unknown'</code> <code>title_prefix</code> <code>str</code> <p>Optional prefix for the plot title</p> <code>''</code> <code>cmap</code> <code>str</code> <p>Colormap to use for visualization</p> <code>'viridis'</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size as (width, height)</p> <code>(30, 5)</code> <code>save_path</code> <code>Optional[str]</code> <p>Optional path to save the plot</p> <code>None</code> <code>vmin</code> <code>Optional[float]</code> <p>Minimum value for color scale (if None, auto-determined)</p> <code>None</code> <code>vmax</code> <code>Optional[float]</code> <p>Maximum value for color scale (if None, auto-determined)</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Matplotlib figure object</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.plot_noise_level_comparison","level":2,"title":"<code>plot_noise_level_comparison(noise_levels, metrics_dict, metric_name='MSE', title=None, save_path=None)</code>","text":"<p>Plot how model performance varies with noise levels.</p> <p>Parameters:</p> Name Type Description Default <code>noise_levels</code> <code>List[float]</code> <p>List of noise levels</p> required <code>metrics_dict</code> <code>Dict[str, List[float]]</code> <p>Dictionary mapping dataset types to metric values</p> required <code>metric_name</code> <code>str</code> <p>Name of the metric being plotted</p> <code>'MSE'</code> <code>title</code> <code>Optional[str]</code> <p>Optional plot title</p> <code>None</code> <code>save_path</code> <code>Optional[str]</code> <p>Optional path to save the plot</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Matplotlib figure</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.plot_training_curves","level":2,"title":"<code>plot_training_curves(train_losses, val_losses, save_path=None, title='Training Curves')</code>","text":"<p>Plot training and validation loss curves.</p> <p>Parameters:</p> Name Type Description Default <code>train_losses</code> <code>List[float]</code> <p>List of training losses</p> required <code>val_losses</code> <code>List[float]</code> <p>List of validation losses</p> required <code>save_path</code> <code>Optional[str]</code> <p>Optional path to save the plot</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title</p> <code>'Training Curves'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Matplotlib figure</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.random_skew_symmetric_matrix","level":2,"title":"<code>random_skew_symmetric_matrix(n)</code>","text":"<p>Create a random n×n skew-symmetric matrix.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Matrix dimension</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Skew-symmetric matrix</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/#tmgg.experiment_utils.run_experiment_sanity_check","level":2,"title":"<code>run_experiment_sanity_check(model, noise_generator, data_loader, criterion, device=torch.device('cpu'), save_plots=True, output_dir=None)</code>","text":"<p>Run comprehensive sanity checks for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to test</p> required <code>noise_generator</code> <code>NoiseGenerator</code> <p>Noise generator to test</p> required <code>data_loader</code> <code>DataLoader</code> <p>DataLoader to test</p> required <code>criterion</code> <code>Module</code> <p>Loss function</p> required <code>device</code> <code>device</code> <p>Device to run on</p> <code>device('cpu')</code> <code>save_plots</code> <code>bool</code> <p>Whether to save diagnostic plots</p> <code>True</code> <code>output_dir</code> <code>Optional[Path]</code> <p>Directory to save plots</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with all check results</p>","path":["Reference","Experiment utils"],"tags":[]},{"location":"reference/experiment_utils/cloud/","level":1,"title":"Cloud","text":"<p>Cloud execution abstractions for distributed experiment runs.</p> <p>This module provides backend-agnostic interfaces for running experiments on cloud platforms (Modal, etc.) with unified storage and coordination.</p> <p>Use LocalRunner for local execution or ModalRunner for cloud execution:</p> <pre><code>&gt;&gt;&gt; from tmgg.experiment_utils.cloud import LocalRunner\n&gt;&gt;&gt; runner = LocalRunner()\n&gt;&gt;&gt; result = runner.run_experiment(config)\n</code></pre>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudRunner","level":2,"title":"<code>CloudRunner</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for cloud execution backends.</p> <p>Implementations handle the specifics of launching experiments on different platforms (Modal, Ray, local subprocess) while providing a unified interface for the experiment coordinator.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudRunner.cancel","level":3,"title":"<code>cancel(run_id)</code>  <code>abstractmethod</code>","text":"<p>Cancel a running experiment.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudRunner.cancel--parameters","level":5,"title":"Parameters","text":"<p>run_id     The run identifier to cancel.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudRunner.cancel--returns","level":5,"title":"Returns","text":"<p>bool     True if cancellation was successful.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudRunner.get_status","level":3,"title":"<code>get_status(run_id)</code>  <code>abstractmethod</code>","text":"<p>Get the current status of a running experiment.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudRunner.get_status--parameters","level":5,"title":"Parameters","text":"<p>run_id     The run identifier returned from run_experiment.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudRunner.get_status--returns","level":5,"title":"Returns","text":"<p>str     One of 'pending', 'running', 'completed', 'failed', 'timeout'.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudRunner.run_experiment","level":3,"title":"<code>run_experiment(config, gpu_type='standard', timeout_seconds=3600)</code>  <code>abstractmethod</code>","text":"<p>Run a single experiment with the given configuration.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudRunner.run_experiment--parameters","level":5,"title":"Parameters","text":"<p>config     Hydra configuration for the experiment. gpu_type     GPU tier to request ('debug', 'standard', 'fast', 'multi'). timeout_seconds     Maximum runtime before the job is killed.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudRunner.run_experiment--returns","level":5,"title":"Returns","text":"<p>ExperimentResult     The result of the experiment run.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudRunner.run_sweep","level":3,"title":"<code>run_sweep(configs, gpu_type='standard', parallelism=4, timeout_seconds=3600)</code>  <code>abstractmethod</code>","text":"<p>Run multiple experiments in parallel.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudRunner.run_sweep--parameters","level":5,"title":"Parameters","text":"<p>configs     List of Hydra configurations for each experiment. gpu_type     GPU tier to request for all experiments. parallelism     Maximum number of concurrent experiments. timeout_seconds     Maximum runtime per experiment.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudRunner.run_sweep--returns","level":5,"title":"Returns","text":"<p>list[ExperimentResult]     Results from all experiments, in the same order as configs.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage","level":2,"title":"<code>CloudStorage</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for cloud storage backends.</p> <p>Handles checkpoint upload/download and metrics persistence for distributed experiment runs.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.download_file","level":3,"title":"<code>download_file(remote_key, local_path)</code>  <code>abstractmethod</code>","text":"<p>Download a file from cloud storage.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.download_file--parameters","level":5,"title":"Parameters","text":"<p>remote_key     Key (path) in the storage bucket. local_path     Local path to save the file.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.download_file--returns","level":5,"title":"Returns","text":"<p>Path     The local path where the file was saved.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.download_metrics","level":3,"title":"<code>download_metrics(run_id)</code>  <code>abstractmethod</code>","text":"<p>Download metrics for an experiment run.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.download_metrics--parameters","level":5,"title":"Parameters","text":"<p>run_id     Unique identifier for the run.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.download_metrics--returns","level":5,"title":"Returns","text":"<p>dict     The stored metrics dictionary.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.exists","level":3,"title":"<code>exists(remote_key)</code>  <code>abstractmethod</code>","text":"<p>Check if a key exists in storage.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.exists--parameters","level":5,"title":"Parameters","text":"<p>remote_key     Key to check.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.exists--returns","level":5,"title":"Returns","text":"<p>bool     True if the key exists.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.list_checkpoints","level":3,"title":"<code>list_checkpoints(prefix='')</code>  <code>abstractmethod</code>","text":"<p>List checkpoint files in storage.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.list_checkpoints--parameters","level":5,"title":"Parameters","text":"<p>prefix     Optional prefix to filter results.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.list_checkpoints--returns","level":5,"title":"Returns","text":"<p>list[str]     List of checkpoint keys matching the prefix.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.upload_file","level":3,"title":"<code>upload_file(local_path, remote_key)</code>  <code>abstractmethod</code>","text":"<p>Upload a local file to cloud storage.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.upload_file--parameters","level":5,"title":"Parameters","text":"<p>local_path     Path to the local file. remote_key     Key (path) in the storage bucket.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.upload_file--returns","level":5,"title":"Returns","text":"<p>str     The full remote URI of the uploaded file.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.upload_metrics","level":3,"title":"<code>upload_metrics(metrics, run_id)</code>  <code>abstractmethod</code>","text":"<p>Upload metrics JSON for an experiment run.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.upload_metrics--parameters","level":5,"title":"Parameters","text":"<p>metrics     Dictionary of metrics to store. run_id     Unique identifier for the run.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.CloudStorage.upload_metrics--returns","level":5,"title":"Returns","text":"<p>str     The remote URI of the metrics file.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.ExperimentCoordinator","level":2,"title":"<code>ExperimentCoordinator</code>","text":"<p>Coordinates multi-experiment runs with checkpointing.</p> <p>Manages the execution of experimental stages, handling configuration generation, result aggregation, and persistence to cloud storage.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.ExperimentCoordinator.__init__","level":3,"title":"<code>__init__(runner=None, storage=None, base_config_path=None, cache_dir=None)</code>","text":"<p>Initialize the coordinator.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.ExperimentCoordinator.__init__--parameters","level":5,"title":"Parameters","text":"<p>runner     Cloud runner for experiment execution. Defaults to LocalRunner. storage     Cloud storage backend for checkpoints and metrics.     Defaults to LocalStorage. base_config_path     Path to base Hydra configuration directory. cache_dir     Local cache directory for temporary files.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.ExperimentCoordinator.export_results","level":3,"title":"<code>export_results(stage_name, output_path)</code>","text":"<p>Export stage results to a local JSON file.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.ExperimentCoordinator.generate_configs","level":3,"title":"<code>generate_configs(stage, base_config)</code>","text":"<p>Generate experiment configurations for a stage.</p> <p>Creates the cartesian product of architectures, datasets, hyperparameters, and seeds.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.ExperimentCoordinator.generate_configs--parameters","level":5,"title":"Parameters","text":"<p>stage     Stage configuration defining the sweep space. base_config     Base Hydra configuration to extend.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.ExperimentCoordinator.generate_configs--yields","level":5,"title":"Yields","text":"<p>DictConfig     Individual experiment configuration.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.ExperimentCoordinator.get_best_config","level":3,"title":"<code>get_best_config(stage_name)</code>","text":"<p>Retrieve the best configuration from a completed stage.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.ExperimentCoordinator.run_stage","level":3,"title":"<code>run_stage(stage, base_config, parallelism=4, resume=True)</code>","text":"<p>Execute all experiments in a stage.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.ExperimentCoordinator.run_stage--parameters","level":5,"title":"Parameters","text":"<p>stage     Stage configuration. base_config     Base Hydra configuration. parallelism     Maximum concurrent experiments. resume     If True, skip experiments with existing results.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.ExperimentCoordinator.run_stage--returns","level":5,"title":"Returns","text":"<p>StageResult     Aggregated results from the stage.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.ExperimentResult","level":2,"title":"<code>ExperimentResult</code>  <code>dataclass</code>","text":"<p>Result from a single experiment run.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.ExperimentResult--attributes","level":4,"title":"Attributes","text":"<p>run_id     Unique identifier for this run. config     The configuration used for this run. metrics     Final metrics from training (val_loss, test_loss, etc.). checkpoint_path     Path to the best model checkpoint (local or remote). status     Completion status: 'completed', 'failed', or 'timeout'. error_message     Error details if status is 'failed'. duration_seconds     Wall-clock time for the experiment.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.LocalRunner","level":2,"title":"<code>LocalRunner</code>","text":"<p>               Bases: <code>CloudRunner</code></p> <p>Local execution backend for development and testing.</p> <p>Runs experiments in the current process without any cloud infrastructure. Useful for debugging configurations before deploying to cloud.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.LocalRunner.__init__","level":3,"title":"<code>__init__(output_dir=None)</code>","text":"<p>Initialize local runner.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.LocalRunner.__init__--parameters","level":5,"title":"Parameters","text":"<p>output_dir     Directory for experiment outputs. Defaults to ./outputs.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.LocalRunner.cancel","level":3,"title":"<code>cancel(run_id)</code>","text":"<p>Cancel not supported for local runs.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.LocalRunner.get_status","level":3,"title":"<code>get_status(run_id)</code>","text":"<p>Get status of a local run.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.LocalRunner.run_experiment","level":3,"title":"<code>run_experiment(config, gpu_type='standard', timeout_seconds=3600)</code>","text":"<p>Run experiment locally in current process.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.LocalRunner.run_sweep","level":3,"title":"<code>run_sweep(configs, gpu_type='standard', parallelism=4, timeout_seconds=3600)</code>","text":"<p>Run experiments sequentially (no parallelism in local mode).</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.LocalStorage","level":2,"title":"<code>LocalStorage</code>","text":"<p>               Bases: <code>CloudStorage</code></p> <p>Local filesystem storage for development.</p> <p>Mimics the CloudStorage interface but stores files locally. Useful for testing without cloud credentials.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.LocalStorage.__init__","level":3,"title":"<code>__init__(base_dir=None)</code>","text":"<p>Initialize local storage.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.LocalStorage.__init__--parameters","level":5,"title":"Parameters","text":"<p>base_dir     Base directory for storage. Defaults to ./storage.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.LocalStorage.download_file","level":3,"title":"<code>download_file(remote_key, local_path)</code>","text":"<p>Copy file from local storage directory.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.LocalStorage.download_metrics","level":3,"title":"<code>download_metrics(run_id)</code>","text":"<p>Load metrics JSON from local storage.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.LocalStorage.exists","level":3,"title":"<code>exists(remote_key)</code>","text":"<p>Check if a file exists in local storage.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.LocalStorage.list_checkpoints","level":3,"title":"<code>list_checkpoints(prefix='')</code>","text":"<p>List checkpoint files in local storage.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.LocalStorage.upload_file","level":3,"title":"<code>upload_file(local_path, remote_key)</code>","text":"<p>Copy file to local storage directory.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.LocalStorage.upload_metrics","level":3,"title":"<code>upload_metrics(metrics, run_id)</code>","text":"<p>Save metrics as JSON to local storage.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.S3Storage","level":2,"title":"<code>S3Storage</code>","text":"<p>               Bases: <code>CloudStorage</code></p> <p>S3-compatible storage backend.</p> <p>Works with AWS S3, Tigris, MinIO, and other S3-compatible services. Configure via environment variables or constructor parameters.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.S3Storage--environment-variables","level":4,"title":"Environment Variables","text":"<p>TMGG_S3_BUCKET : str     Bucket name for storage. TMGG_S3_ENDPOINT : str, optional     Custom endpoint URL (required for Tigris/MinIO). TMGG_S3_ACCESS_KEY : str     Access key ID. TMGG_S3_SECRET_KEY : str     Secret access key. TMGG_S3_REGION : str, optional     AWS region (default: us-east-1).</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.S3Storage.client","level":3,"title":"<code>client</code>  <code>property</code>","text":"<p>Lazy-initialize boto3 client.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.S3Storage.__init__","level":3,"title":"<code>__init__(bucket=None, endpoint_url=None, access_key=None, secret_key=None, region=None, prefix='tmgg')</code>","text":"<p>Initialize S3 storage client.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.S3Storage.__init__--parameters","level":5,"title":"Parameters","text":"<p>bucket     S3 bucket name. Falls back to TMGG_S3_BUCKET env var. endpoint_url     Custom S3 endpoint for Tigris/MinIO. Falls back to TMGG_S3_ENDPOINT. access_key     Access key ID. Falls back to TMGG_S3_ACCESS_KEY. secret_key     Secret access key. Falls back to TMGG_S3_SECRET_KEY. region     AWS region. Falls back to TMGG_S3_REGION or 'us-east-1'. prefix     Key prefix for all objects (default: 'tmgg').</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.S3Storage.download_file","level":3,"title":"<code>download_file(remote_key, local_path)</code>","text":"<p>Download a file from S3.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.S3Storage.download_metrics","level":3,"title":"<code>download_metrics(run_id)</code>","text":"<p>Download metrics JSON.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.S3Storage.exists","level":3,"title":"<code>exists(remote_key)</code>","text":"<p>Check if a key exists.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.S3Storage.list_checkpoints","level":3,"title":"<code>list_checkpoints(prefix='')</code>","text":"<p>List checkpoint files in the bucket.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.S3Storage.upload_file","level":3,"title":"<code>upload_file(local_path, remote_key)</code>","text":"<p>Upload a local file to S3.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/cloud/#tmgg.experiment_utils.cloud.S3Storage.upload_metrics","level":3,"title":"<code>upload_metrics(metrics, run_id)</code>","text":"<p>Upload metrics as JSON.</p>","path":["Reference","Experiment utils","Cloud"],"tags":[]},{"location":"reference/experiment_utils/data/","level":1,"title":"Data","text":"<p>Public API for data utilities.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.ANUDatasetWrapper","level":2,"title":"<code>ANUDatasetWrapper</code>","text":"<p>               Bases: <code>GraphCollection</code></p> <p>Wrapper for ANUDataset to make it compatible with experiment data modules.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.ANUDatasetWrapper.get_adjacency_matrices","level":3,"title":"<code>get_adjacency_matrices()</code>","text":"<p>Extracts all adjacency matrices from the wrapped dataset and pads them to a uniform size.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.ClassicalGraphsWrapper","level":2,"title":"<code>ClassicalGraphsWrapper</code>","text":"<p>               Bases: <code>GraphCollection</code></p> <p>Wrapper for CassicalGraphs to make it compatible with experiment data modules.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.ClassicalGraphsWrapper.get_adjacency_matrices","level":3,"title":"<code>get_adjacency_matrices()</code>","text":"<p>Extracts all adjacency matrices from the wrapped dataset and pads them to a uniform size.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.DigressNoiseGenerator","level":2,"title":"<code>DigressNoiseGenerator</code>","text":"<p>               Bases: <code>NoiseGenerator</code></p> <p>Digress (edge flipping) noise generator (stateless).</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.DigressNoiseGenerator.add_noise","level":3,"title":"<code>add_noise(A, eps)</code>","text":"<p>Add digress noise by flipping edges with probability eps.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.GaussianNoiseGenerator","level":2,"title":"<code>GaussianNoiseGenerator</code>","text":"<p>               Bases: <code>NoiseGenerator</code></p> <p>Gaussian noise generator (stateless).</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.GaussianNoiseGenerator.add_noise","level":3,"title":"<code>add_noise(A, eps)</code>","text":"<p>Add Gaussian noise to adjacency matrix.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.GraphDataModule","level":2,"title":"<code>GraphDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>PyTorch Lightning data module for various graph datasets.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.GraphDataModule.__init__","level":3,"title":"<code>__init__(dataset_name, dataset_config, num_samples_per_graph=1000, batch_size=100, num_workers=4, pin_memory=True, val_split=0.2, test_split=0.2, noise_levels=None, **kwargs)</code>","text":"<p>Initialize the GraphDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset to use (e.g., \"sbm\", \"classical\").</p> required <code>dataset_config</code> <code>Dict[str, Any]</code> <p>Dictionary of parameters for the chosen dataset.</p> required <code>num_samples_per_graph</code> <code>int</code> <p>Number of samples (permutations) per graph.</p> <code>1000</code> <code>batch_size</code> <code>int</code> <p>Batch size for data loaders.</p> <code>100</code> <code>num_workers</code> <code>int</code> <p>Number of worker processes for data loading.</p> <code>4</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory for faster GPU transfer.</p> <code>True</code> <code>val_split</code> <code>float</code> <p>Fraction of data to use for validation (for non-SBM datasets).</p> <code>0.2</code> <code>test_split</code> <code>float</code> <p>Fraction of data to use for testing (for non-SBM datasets).</p> <code>0.2</code> <code>noise_levels</code> <code>Optional[List[float]]</code> <p>List of noise levels for evaluation.</p> <code>None</code>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.GraphDataModule.get_sample_adjacency_matrix","level":3,"title":"<code>get_sample_adjacency_matrix(stage='train')</code>","text":"<p>Get a sample adjacency matrix for visualization.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.GraphDataModule.prepare_data","level":3,"title":"<code>prepare_data()</code>","text":"<p>Download or prepare data. Called only once per node.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.GraphDataModule.setup","level":3,"title":"<code>setup(stage=None)</code>","text":"<p>Set up datasets for each stage.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.GraphDataModule.test_dataloader","level":3,"title":"<code>test_dataloader()</code>","text":"<p>Create test data loader.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.GraphDataModule.train_dataloader","level":3,"title":"<code>train_dataloader()</code>","text":"<p>Create training data loader.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.GraphDataModule.val_dataloader","level":3,"title":"<code>val_dataloader()</code>","text":"<p>Create validation data loader.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.GraphDataset","level":2,"title":"<code>GraphDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Unified dataset for graph adjacency matrices with permutation support.</p> <p>This dataset can handle both single and multiple adjacency matrices, and optionally applies random permutations to increase data diversity.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.GraphDataset.__getitem__","level":3,"title":"<code>__getitem__(idx)</code>","text":"<p>Get a sample from the dataset.</p> <p>Returns:</p> Type Description <code>Union[Tensor, Tuple[Tensor, int]]</code> <p>If return_original_idx is False: permuted adjacency matrix</p> <code>Union[Tensor, Tuple[Tensor, int]]</code> <p>If return_original_idx is True: (permuted adjacency matrix, original matrix index)</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.GraphDataset.__init__","level":3,"title":"<code>__init__(adjacency_matrices, num_samples, apply_permutation=True, return_original_idx=False)</code>","text":"<p>Initialize graph dataset.</p> <p>Parameters:</p> Name Type Description Default <code>adjacency_matrices</code> <code>Union[ndarray, Tensor, List[Union[ndarray, Tensor]]]</code> <p>Single adjacency matrix or list of matrices</p> required <code>num_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>apply_permutation</code> <code>bool</code> <p>Whether to apply random permutations</p> <code>True</code> <code>return_original_idx</code> <code>bool</code> <p>Whether to return the index of the original matrix</p> <code>False</code>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.NXGraphWrapperWrapper","level":2,"title":"<code>NXGraphWrapperWrapper</code>","text":"<p>               Bases: <code>GraphCollection</code></p> <p>Wrapper for NXGraphWrapper to make it compatible with experiment data modules.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.NXGraphWrapperWrapper.get_adjacency_matrices","level":3,"title":"<code>get_adjacency_matrices()</code>","text":"<p>Extracts all adjacency matrices from the wrapped dataset and pads them to a uniform size.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.NoiseGenerator","level":2,"title":"<code>NoiseGenerator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for noise generators.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.NoiseGenerator.requires_state","level":3,"title":"<code>requires_state</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Whether this noise generator maintains internal state.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.NoiseGenerator.add_noise","level":3,"title":"<code>add_noise(A, eps)</code>  <code>abstractmethod</code>","text":"<p>Add noise to an adjacency matrix.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>Input adjacency matrix (can be batched)</p> required <code>eps</code> <code>float</code> <p>Noise level/intensity</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Noisy adjacency matrix</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.RotationNoiseGenerator","level":2,"title":"<code>RotationNoiseGenerator</code>","text":"<p>               Bases: <code>NoiseGenerator</code></p> <p>Rotation noise generator with skew matrix management.</p> <p>This generator maintains a skew-symmetric matrix that is used to rotate eigenvectors, providing a consistent rotation throughout the experiment.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.RotationNoiseGenerator.__init__","level":3,"title":"<code>__init__(k, seed=None)</code>","text":"<p>Initialize rotation noise generator.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Dimension of the skew matrix (number of eigenvectors)</p> required <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducible skew matrix generation</p> <code>None</code>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.RotationNoiseGenerator.add_noise","level":3,"title":"<code>add_noise(A, eps)</code>","text":"<p>Add rotation noise by rotating eigenvectors.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.RotationNoiseGenerator.get_config","level":3,"title":"<code>get_config()</code>","text":"<p>Get configuration for this generator.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.SingleGraphDataModule","level":2,"title":"<code>SingleGraphDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>Lightning DataModule for single-graph training protocol.</p> <p>Supports two modes controlled by <code>same_graph_all_splits</code>:</p> <p>When <code>same_graph_all_splits=True</code> (Stage 1 protocol): - All splits (train/val/test) use the identical graph G - Only noise varies across samples - Tests generalization to new noise realizations on a known structure</p> <p>When <code>same_graph_all_splits=False</code> (default, Stage 2+ protocol): - Training uses graph G, validation/test use different graphs G', G'' - Tests generalization to both new noise and unseen graph structures</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.SingleGraphDataModule--parameters","level":4,"title":"Parameters","text":"<p>graph_type : str     Type of graph to generate. Supported types:     - \"sbm\": Stochastic Block Model     - \"erdos_renyi\": Erdős-Rényi random graphs     - \"regular\": d-regular graphs     - \"tree\": Random trees     - \"ring_of_cliques\": NetworkX ring of cliques     - \"lfr\": LFR benchmark graphs (community structure)     - \"pyg_enzymes\", \"pyg_qm9\", \"pyg_proteins\": PyG datasets (sample one) n : int     Number of nodes. num_train_samples : int     Number of training samples per epoch (noise realizations). num_val_samples : int     Number of validation samples. num_test_samples : int     Number of test samples. batch_size : int     Batch size for dataloaders. num_workers : int     Number of dataloader workers. train_seed : int     Seed for generating the training graph. val_test_seed : int     Seed for generating validation/test graphs (used only when     same_graph_all_splits=False). same_graph_all_splits : bool     If True, val/test use the same graph as training (Stage 1 protocol).     If False, val/test use different graphs (Stage 2+ protocol). noise_levels : list of float     Noise levels used for evaluation (passed to LightningModule). noise_type : str     Type of noise to apply. **graph_kwargs     Additional parameters for graph generation (e.g., p for ER, d for     regular, SBM parameters).</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.SingleGraphDataModule--examples","level":4,"title":"Examples","text":"<p>dm = SingleGraphDataModule( ...     graph_type=\"sbm\", ...     n=50, ...     num_train_samples=1000, ...     num_val_samples=100, ...     num_test_samples=100, ...     batch_size=16, ...     p_intra=0.7, ...     p_inter=0.05, ...     num_blocks=3, ... ) dm.setup() train_loader = dm.train_dataloader()</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.SingleGraphDataModule.get_sample_adjacency_matrix","level":3,"title":"<code>get_sample_adjacency_matrix(stage='train')</code>","text":"<p>Get a sample adjacency matrix for visualization.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.SingleGraphDataModule.get_sample_adjacency_matrix--parameters","level":5,"title":"Parameters","text":"<p>stage : str     Which split to sample from: \"train\", \"val\", or \"test\".</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.SingleGraphDataModule.get_sample_adjacency_matrix--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Adjacency matrix as a tensor.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.SingleGraphDataModule.get_test_graph","level":3,"title":"<code>get_test_graph()</code>","text":"<p>Return the test graph.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.SingleGraphDataModule.get_test_graph--returns","level":5,"title":"Returns","text":"<p>np.ndarray     Test adjacency matrix.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.SingleGraphDataModule.get_train_graph","level":3,"title":"<code>get_train_graph()</code>","text":"<p>Return the training graph.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.SingleGraphDataModule.get_train_graph--returns","level":5,"title":"Returns","text":"<p>np.ndarray     Training adjacency matrix.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.SingleGraphDataModule.get_val_graph","level":3,"title":"<code>get_val_graph()</code>","text":"<p>Return the validation graph.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.SingleGraphDataModule.get_val_graph--returns","level":5,"title":"Returns","text":"<p>np.ndarray     Validation adjacency matrix.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.SingleGraphDataModule.setup","level":3,"title":"<code>setup(stage=None)</code>","text":"<p>Generate graphs for train/val/test.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.SingleGraphDataModule.setup--parameters","level":5,"title":"Parameters","text":"<p>stage : str, optional     Stage: \"fit\", \"validate\", \"test\", or None (all).</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.SingleGraphDataModule.test_dataloader","level":3,"title":"<code>test_dataloader()</code>","text":"<p>Return test dataloader.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.SingleGraphDataModule.train_dataloader","level":3,"title":"<code>train_dataloader()</code>","text":"<p>Return training dataloader.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.SingleGraphDataModule.val_dataloader","level":3,"title":"<code>val_dataloader()</code>","text":"<p>Return validation dataloader.</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.add_digress_noise","level":2,"title":"<code>add_digress_noise(A, p, rng=None)</code>","text":"<p>Add noise to an adjacency matrix by flipping edges with probability p.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Union[Tensor, ndarray]</code> <p>Input adjacency matrix (0s and 1s)</p> required <code>p</code> <code>float</code> <p>Probability of flipping each element</p> required <code>rng</code> <code>Optional[Generator]</code> <p>Random number generator (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Noisy adjacency matrix</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.add_gaussian_noise","level":2,"title":"<code>add_gaussian_noise(A, eps)</code>","text":"<p>Add Gaussian noise to adjacency matrix.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Union[Tensor, ndarray]</code> <p>Input adjacency matrix</p> required <code>eps</code> <code>float</code> <p>Noise level</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Noisy adjacency matrix</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.add_rotation_noise","level":2,"title":"<code>add_rotation_noise(A, eps, skew)</code>","text":"<p>Add rotation noise to adjacency matrix by rotating eigenvectors.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Union[Tensor, ndarray]</code> <p>Input adjacency matrix</p> required <code>eps</code> <code>float</code> <p>Noise level</p> required <code>skew</code> <code>ndarray</code> <p>Skew-symmetric matrix for rotation</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Noisy adjacency matrix</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.compute_eigendecomposition","level":2,"title":"<code>compute_eigendecomposition(A)</code>","text":"<p>Compute eigenvalues and eigenvectors of a symmetric matrix.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Union[Tensor, ndarray]</code> <p>Input matrix (symmetric)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple of (eigenvalues, eigenvectors) where eigenvalues are sorted</p> <code>Tensor</code> <p>in ascending order</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.compute_spectral_distance","level":2,"title":"<code>compute_spectral_distance(A1, A2, metric='frobenius')</code>","text":"<p>Compute distance between two matrices based on their eigendecompositions.</p> <p>Parameters:</p> Name Type Description Default <code>A1</code> <code>Union[Tensor, ndarray]</code> <p>First matrix</p> required <code>A2</code> <code>Union[Tensor, ndarray]</code> <p>Second matrix</p> required <code>metric</code> <code>str</code> <p>Distance metric to use (\"frobenius\", \"eigenvalue\", \"subspace\")</p> <code>'frobenius'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Distance value</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.compute_top_k_eigendecomposition","level":2,"title":"<code>compute_top_k_eigendecomposition(A, k)</code>","text":"<p>Compute top-k eigenvalues and eigenvectors of a symmetric matrix.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Union[Tensor, ndarray]</code> <p>Input matrix (symmetric)</p> required <code>k</code> <code>int</code> <p>Number of top eigenvalues/eigenvectors to compute</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple of (top_k_eigenvalues, top_k_eigenvectors) sorted by magnitude</p> <code>Tensor</code> <p>in descending order</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.create_dataset_wrapper","level":2,"title":"<code>create_dataset_wrapper(dataset_type, **kwargs)</code>","text":"<p>Factory function to create dataset wrappers.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_type</code> <code>str</code> <p>Type of dataset (\"anu\", \"classical\", \"nx\")</p> required <code>**kwargs</code> <p>Parameters passed to the dataset constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>GraphCollection</code> <p>GraphCollection wrapper around the specified dataset</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.create_noise_generator","level":2,"title":"<code>create_noise_generator(noise_type, rotation_k=None, seed=None, **kwargs)</code>","text":"<p>Factory function to create noise generators.</p> <p>Parameters:</p> Name Type Description Default <code>noise_type</code> <code>str</code> <p>Type of noise (\"gaussian\", \"digress\", or \"rotation\")</p> required <code>rotation_k</code> <code>Optional[int]</code> <p>Dimension for rotation noise skew matrix</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducible noise generation</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments (for future extensibility)</p> <code>{}</code> <p>Returns:</p> Type Description <code>NoiseGenerator</code> <p>NoiseGenerator instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If noise_type is unknown or required parameters are missing</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.generate_block_sizes","level":2,"title":"<code>generate_block_sizes(n, min_blocks=2, max_blocks=4, min_size=2, max_size=15)</code>","text":"<p>Generate all valid block size partitions for n nodes.</p> <p>This function finds all ways to partition n nodes into blocks where: - The number of blocks is between min_blocks and max_blocks (inclusive) - Each block has size between min_size and max_size (inclusive) - The sum of all block sizes equals n</p> <p>The algorithm uses recursive backtracking to explore all valid partitions. For each position, it calculates the valid range of sizes based on: - Minimum size: Must be at least min_size, but also large enough that    remaining blocks can fit within max_size constraint - Maximum size: Must be at most max_size, but also small enough that   remaining blocks can satisfy min_size constraint</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Total number of nodes to partition</p> required <code>min_blocks</code> <code>int</code> <p>Minimum number of blocks in a partition</p> <code>2</code> <code>max_blocks</code> <code>int</code> <p>Maximum number of blocks in a partition</p> <code>4</code> <code>min_size</code> <code>int</code> <p>Minimum size for any block</p> <code>2</code> <code>max_size</code> <code>int</code> <p>Maximum size for any block</p> <code>15</code> <p>Returns:</p> Type Description <code>List[List[int]]</code> <p>List of valid block size partitions, where each partition is a list</p> <code>List[List[int]]</code> <p>of integers summing to n</p> Example <p>generate_block_sizes(6, min_blocks=2, max_blocks=3, min_size=2, max_size=4) [[2, 4], [3, 3], [4, 2], [2, 2, 2]]</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.generate_sbm_adjacency","level":2,"title":"<code>generate_sbm_adjacency(block_sizes, p, q, rng=None)</code>","text":"<p>Generate an adjacency matrix for a stochastic block model with variable block sizes.</p> <p>Parameters:</p> Name Type Description Default <code>block_sizes</code> <code>List[int]</code> <p>List of sizes for each block</p> required <code>p</code> <code>float</code> <p>Probability of intra-block edges</p> required <code>q</code> <code>float</code> <p>Probability of inter-block edges</p> required <code>rng</code> <code>Optional[Generator]</code> <p>Random number generator (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Adjacency matrix as a numpy array</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/data/#tmgg.experiment_utils.data.random_skew_symmetric_matrix","level":2,"title":"<code>random_skew_symmetric_matrix(n)</code>","text":"<p>Create a random n×n skew-symmetric matrix.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Matrix dimension</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Skew-symmetric matrix</p>","path":["Reference","Experiment utils","Data"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/","level":1,"title":"TensorBoard export","text":"<p>TensorBoard export utilities.</p> <p>Export TensorBoard event files to pandas DataFrames for analysis.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export--examples","level":3,"title":"Examples","text":"<p>from tmgg.experiment_utils.tensorboard_export import TensorBoardExporter exporter = TensorBoardExporter() result = exporter.export_directory(\"outputs/sweeps/stage1_poc\") result.events_df.head() result.hparams_df.head() result.save(\"analysis_output/\")</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.DiscoveredRun","level":2,"title":"<code>DiscoveredRun</code>  <code>dataclass</code>","text":"<p>A discovered TensorBoard run with its associated files.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.ExportResult","level":2,"title":"<code>ExportResult</code>  <code>dataclass</code>","text":"<p>Result of a TensorBoard export operation.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.ExportResult--attributes","level":4,"title":"Attributes","text":"<p>events_df     DataFrame with all scalar events across runs.     Columns: project_id, run_id, tag, step, wall_time, value hparams_df     DataFrame with hyperparameters/config for each run.     Columns: project_id, run_id, plus flattened config fields runs_processed     Number of runs successfully processed. runs_failed     Number of runs that failed to process. failed_runs     List of run_ids that failed.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.ExportResult.save","level":3,"title":"<code>save(output_dir)</code>","text":"<p>Save DataFrames to parquet files.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.ExportResult.save--parameters","level":5,"title":"Parameters","text":"<p>output_dir     Directory to save output files.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.ExportResult.summary","level":3,"title":"<code>summary()</code>","text":"<p>Return a summary of the export result.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.TensorBoardExporter","level":2,"title":"<code>TensorBoardExporter</code>","text":"<p>Export TensorBoard event files to DataFrames.</p> <p>Discovers runs in a directory tree, loads events and configs, and produces unified DataFrames for analysis.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.TensorBoardExporter--examples","level":4,"title":"Examples","text":"<p>exporter = TensorBoardExporter() result = exporter.export_directory(\"outputs/sweeps/stage1_poc\") result.events_df.head() result.save(\"analysis_output/\")</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.TensorBoardExporter.export_directory","level":3,"title":"<code>export_directory(root_dir, project_id=None)</code>","text":"<p>Export all TensorBoard runs from a directory.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.TensorBoardExporter.export_directory--parameters","level":5,"title":"Parameters","text":"<p>root_dir     Root directory containing run subdirectories. project_id     Project identifier. If None, uses directory name.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.TensorBoardExporter.export_directory--returns","level":5,"title":"Returns","text":"<p>ExportResult     Contains events_df and hparams_df with all runs.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.TensorBoardExporter.export_runs","level":3,"title":"<code>export_runs(runs)</code>","text":"<p>Export specific discovered runs.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.TensorBoardExporter.export_runs--parameters","level":5,"title":"Parameters","text":"<p>runs     List of discovered runs to export.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.TensorBoardExporter.export_runs--returns","level":5,"title":"Returns","text":"<p>ExportResult     Contains events_df and hparams_df for the runs.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.discover_runs","level":2,"title":"<code>discover_runs(root_dir, project_id=None)</code>","text":"<p>Discover all TensorBoard runs in a directory tree.</p> <p>Searches for tfevents files and associated config.yaml files. The directory structure is expected to be:     root_dir/     ├── run_id_1/     │   ├── config.yaml     │   └── tensorboard/.../.tfevents.*     ├── run_id_2/     │   └── ...</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.discover_runs--parameters","level":4,"title":"Parameters","text":"<p>root_dir     Root directory to search for runs. project_id     Project identifier. If None, uses root_dir.name.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.discover_runs--returns","level":4,"title":"Returns","text":"<p>list[DiscoveredRun]     List of discovered runs with their event files and configs.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.load_config","level":2,"title":"<code>load_config(run)</code>","text":"<p>Load and flatten config from a run's config.yaml.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.load_config--parameters","level":4,"title":"Parameters","text":"<p>run     Discovered run with config file path.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.load_config--returns","level":4,"title":"Returns","text":"<p>dict[str, Any]     Flattened config dictionary with project_id and run_id.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.load_events","level":2,"title":"<code>load_events(run)</code>","text":"<p>Load all scalar events from a run's TensorBoard files.</p> <p>Reads event files directly using protobuf without TensorFlow dependency.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.load_events--parameters","level":4,"title":"Parameters","text":"<p>run     Discovered run with event file paths.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.load_events--returns","level":4,"title":"Returns","text":"<p>pd.DataFrame     DataFrame with columns: project_id, run_id, tag, step, wall_time, value</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.load_run","level":2,"title":"<code>load_run(run)</code>","text":"<p>Load both events and config for a run.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.load_run--parameters","level":4,"title":"Parameters","text":"<p>run     Discovered run to load.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/tensorboard_export/#tmgg.experiment_utils.tensorboard_export.load_run--returns","level":4,"title":"Returns","text":"<p>tuple[pd.DataFrame, dict[str, Any]]     Events DataFrame and config dictionary.</p>","path":["Reference","Experiment utils","TensorBoard export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/","level":1,"title":"W&amp;B export","text":"<p>W&amp;B data export utilities.</p> <p>Provides tools for exporting W&amp;B project data to local Parquet/JSON files with full history (no subsampling), state tracking for resumable exports, and proper rate limiting.</p> <p>Example usage:</p> <pre><code>from tmgg.experiment_utils.wandb_export import WandbExporter, ExportConfig\n\nconfig = ExportConfig(\n    project=\"my-project\",\n    entity=\"my-team\",\n    output_dir=Path(\"./exports\"),\n)\nexporter = WandbExporter(config)\nsummary = exporter.export_project()\n</code></pre> <p>CLI usage:</p> <pre><code>tmgg-wandb-export --project my-project --output-dir ./exports\n</code></pre>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.ExportConfig","level":2,"title":"<code>ExportConfig</code>  <code>dataclass</code>","text":"<p>Configuration for W&amp;B export operation.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.ExportConfig--parameters","level":4,"title":"Parameters","text":"<p>project     W&amp;B project name. entity     W&amp;B entity (team/user). None uses default from wandb config. output_dir     Base directory for exported data. skip_media     Skip downloading media files. skip_artifacts     Skip downloading artifacts. page_size     Page size for scan_history pagination. max_retries     Maximum retries for rate-limited API calls.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.ExportState","level":2,"title":"<code>ExportState</code>","text":"<p>Manages JSONL state file for tracking export progress.</p> <p>The state file contains one JSON object per line, with the most recent status for each run taking precedence. This append-only design ensures crash safety - we never modify existing lines.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.ExportState--parameters","level":4,"title":"Parameters","text":"<p>state_file     Path to the JSONL state file. Will be created if it doesn't exist.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.ExportState.get_completed_runs","level":3,"title":"<code>get_completed_runs()</code>","text":"<p>Get list of run IDs that have been fully exported.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.ExportState.get_incomplete_runs","level":3,"title":"<code>get_incomplete_runs()</code>","text":"<p>Get list of run IDs that started but didn't complete.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.ExportState.get_status","level":3,"title":"<code>get_status(run_id)</code>","text":"<p>Get the current status for a run.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.ExportState.is_completed","level":3,"title":"<code>is_completed(run_id)</code>","text":"<p>Check if run has been fully exported.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.ExportState.is_failed","level":3,"title":"<code>is_failed(run_id)</code>","text":"<p>Check if run export previously failed.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.ExportState.is_in_progress","level":3,"title":"<code>is_in_progress(run_id)</code>","text":"<p>Check if run export started but didn't complete (partial export).</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.ExportState.mark_completed","level":3,"title":"<code>mark_completed(run_id, components)</code>","text":"<p>Mark run export as completed with component status.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.ExportState.mark_failed","level":3,"title":"<code>mark_failed(run_id, error)</code>","text":"<p>Mark run export as failed with error message.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.ExportState.mark_started","level":3,"title":"<code>mark_started(run_id, run_path)</code>","text":"<p>Mark run export as started.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.RateLimiter","level":2,"title":"<code>RateLimiter</code>","text":"<p>Handles rate limiting with exponential backoff for W&amp;B API.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.RateLimiter--parameters","level":4,"title":"Parameters","text":"<p>max_retries     Maximum number of retry attempts for rate-limited requests. min_wait     Minimum wait time between retries in seconds. max_wait     Maximum wait time between retries in seconds. request_delay     Delay between consecutive requests to avoid hitting rate limits.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.RateLimiter.call_with_retry","level":3,"title":"<code>call_with_retry(func)</code>","text":"<p>Execute a function with rate limiting.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.RateLimiter.call_with_retry--parameters","level":5,"title":"Parameters","text":"<p>func     Zero-argument callable to execute.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.RateLimiter.call_with_retry--returns","level":5,"title":"Returns","text":"<p>T     Result of the function call.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.RateLimiter.wrap","level":3,"title":"<code>wrap(func)</code>","text":"<p>Wrap a function with retry logic and inter-request delay.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.RateLimiter.wrap--parameters","level":5,"title":"Parameters","text":"<p>func     Function to wrap with rate limiting.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.RateLimiter.wrap--returns","level":5,"title":"Returns","text":"<p>Callable     Wrapped function with retry and delay logic.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.RunExportStatus","level":2,"title":"<code>RunExportStatus</code>  <code>dataclass</code>","text":"<p>Status of a single run's export.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.RunExportStatus--parameters","level":4,"title":"Parameters","text":"<p>run_id     W&amp;B run ID (short form, e.g. \"abc123\"). run_path     Full W&amp;B path (e.g. \"entity/project/abc123\"). started_at     ISO timestamp when export started. completed_at     ISO timestamp when export completed, None if incomplete. status     One of \"in_progress\", \"completed\", \"failed\". error_message     Error details if status is \"failed\". export_version     Schema version for future migrations. components     Dict mapping component names to export success status.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.RunExportStatus.from_dict","level":3,"title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Deserialize from dict.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.RunExportStatus.to_json","level":3,"title":"<code>to_json()</code>","text":"<p>Serialize to JSON string.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.WandbExporter","level":2,"title":"<code>WandbExporter</code>","text":"<p>Handles W&amp;B data export with state tracking and rate limiting.</p> <p>Exports run data including: - Configuration (JSON) - Summary metrics (JSON) - Metadata: tags, state, timestamps (JSON) - Full metrics history (Parquet, no subsampling) - System metrics (Parquet) - Media files (optional) - Artifacts (optional)</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.WandbExporter--parameters","level":4,"title":"Parameters","text":"<p>config     Export configuration. console     Rich console for progress display. If None, creates a new one.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.WandbExporter.export_project","level":3,"title":"<code>export_project(run_ids=None, force=False)</code>","text":"<p>Export all runs in project, or specific runs if provided.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.WandbExporter.export_project--parameters","level":5,"title":"Parameters","text":"<p>run_ids     Specific run IDs to export. If None, exports all runs. force     Re-export runs even if already completed.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.WandbExporter.export_project--returns","level":5,"title":"Returns","text":"<p>dict     Summary with counts of exported, skipped, and failed runs.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.WandbExporter.export_run","level":3,"title":"<code>export_run(run)</code>","text":"<p>Export a single run (without progress bar).</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiment_utils/wandb_export/#tmgg.experiment_utils.wandb_export.WandbExporter.export_run--parameters","level":5,"title":"Parameters","text":"<p>run     W&amp;B Run object to export.</p>","path":["Reference","Experiment utils","W&amp;B export"],"tags":[]},{"location":"reference/experiments/","level":1,"title":"Experiments","text":"","path":["Reference","Experiments"],"tags":[]},{"location":"reference/experiments/attention_denoising/","level":1,"title":"Attention denoising","text":"<p>Attention-based denoising experiment.</p>","path":["Reference","Experiments","Attention denoising"],"tags":[]},{"location":"reference/experiments/baselines/","level":1,"title":"Baselines","text":"<p>Baseline experiments for sanity checking training pipelines.</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule","level":2,"title":"<code>BaselineLightningModule</code>","text":"<p>               Bases: <code>DenoisingLightningModule</code></p> <p>Lightning module for baseline denoising models.</p> <p>Supports two baseline architectures for sanity checking: - linear: Simple W @ A @ W.T + b transformation - mlp: Flatten -&gt; MLP -&gt; reshape</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule--parameters","level":4,"title":"Parameters","text":"<p>model_type : str     Architecture to use: \"linear\" or \"mlp\". max_nodes : int     Maximum number of nodes in graphs. hidden_dim : int, optional     Hidden layer size for MLP. Default 256. num_layers : int, optional     Number of hidden layers for MLP. Default 2. learning_rate : float, optional     Learning rate. Default 1e-3. weight_decay : float, optional     Weight decay for AdamW. Default 1e-2. optimizer_type : str, optional     Optimizer: \"adam\" or \"adamw\". Default \"adamw\". **kwargs     Additional arguments passed to base class.</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule--examples","level":4,"title":"Examples","text":"<p>module = BaselineLightningModule( ...     model_type=\"mlp\", ...     max_nodes=32, ...     hidden_dim=256, ...     learning_rate=1e-3, ... )</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.eval_noise_levels","level":3,"title":"<code>eval_noise_levels</code>  <code>property</code>","text":"<p>Get noise levels for evaluation.</p> <p>Returns eval_noise_levels if explicitly set, otherwise falls back to noise_levels (same levels for training and evaluation).</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.noise_levels","level":3,"title":"<code>noise_levels</code>  <code>property</code>","text":"<p>Get noise levels from datamodule or override.</p> <p>Returns the noise levels to use for training and evaluation. Prefers the datamodule's noise_levels when attached to a trainer, but falls back to the override value (or default) during standalone usage.</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.configure_optimizers","level":3,"title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizers and learning rate schedulers.</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.forward","level":3,"title":"<code>forward(x)</code>","text":"<p>Forward pass through the attention model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input adjacency matrix</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed adjacency matrix</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.get_model_config","level":3,"title":"<code>get_model_config()</code>","text":"<p>Get model configuration for logging.</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.get_model_config--returns","level":5,"title":"Returns","text":"<p>dict     Model configuration dictionary.</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.get_model_name","level":3,"title":"<code>get_model_name()</code>","text":"<p>Get model name for visualization.</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.get_model_name--returns","level":5,"title":"Returns","text":"<p>str     Human-readable model name.</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.log_parameter_count","level":3,"title":"<code>log_parameter_count()</code>","text":"<p>Log the parameter count of the model in a formatted way.</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.on_fit_start","level":3,"title":"<code>on_fit_start()</code>","text":"<p>Called at the beginning of training.</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.on_test_epoch_end","level":3,"title":"<code>on_test_epoch_end()</code>","text":"<p>Called at the end of test epoch for visualization.</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.on_validation_epoch_end","level":3,"title":"<code>on_validation_epoch_end()</code>","text":"<p>Called at the end of validation for visualization (step-based).</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.setup","level":3,"title":"<code>setup(stage)</code>","text":"<p>Validate configuration before training/testing begins.</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.setup--parameters","level":5,"title":"Parameters","text":"<p>stage     Either 'fit', 'validate', 'test', or 'predict'.</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.test_step","level":3,"title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Batch of adjacency matrices</p> required <code>batch_idx</code> <code>int</code> <p>Batch index</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary of test metrics</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.training_step","level":3,"title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Execute single training step with noise application.</p> <p>In normal mode: noise is sampled fresh for each batch. In sanity check mode (fixed_noise_seed set): uses cached noise patterns so the model sees identical noisy inputs across epochs.</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.training_step--parameters","level":5,"title":"Parameters","text":"<p>batch     Batch of clean adjacency matrices, shape (B, N, N). batch_idx     Index of current batch. Used as cache key in fixed noise mode.</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.training_step--returns","level":5,"title":"Returns","text":"<p>dict     Dictionary with 'loss' (required by Lightning) and 'logits' for debugging.</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/baselines/#tmgg.experiments.baselines.BaselineLightningModule.validation_step","level":3,"title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Batch of adjacency matrices</p> required <code>batch_idx</code> <code>int</code> <p>Batch index</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary of validation metrics</p>","path":["Reference","Experiments","Baselines"],"tags":[]},{"location":"reference/experiments/digress_denoising/","level":1,"title":"DiGress denoising","text":"<p>Digress-based transformer denoising experiment.</p>","path":["Reference","Experiments","DiGress denoising"],"tags":[]},{"location":"reference/experiments/gnn_denoising/","level":1,"title":"GNN denoising","text":"<p>GNN-based denoising experiment.</p>","path":["Reference","Experiments","GNN denoising"],"tags":[]},{"location":"reference/experiments/hybrid_denoising/","level":1,"title":"Hybrid denoising","text":"<p>Hybrid GNN+Transformer denoising experiment.</p>","path":["Reference","Experiments","Hybrid denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/","level":1,"title":"Spectral denoising","text":"<p>Spectral denoising experiment module.</p> <p>Provides a unified experiment interface for training spectral graph denoising architectures: Linear PE, Graph Filter Bank, and Self-Attention denoisers.</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising--usage","level":3,"title":"Usage","text":"<p>Run via Hydra:     python -m tmgg.experiments.spectral_denoising.runner</p> Or import directly <p>from tmgg.experiments.spectral_denoising import SpectralDenoisingLightningModule</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule","level":2,"title":"<code>SpectralDenoisingLightningModule</code>","text":"<p>               Bases: <code>DenoisingLightningModule</code></p> <p>Unified Lightning module for spectral denoising architectures.</p> <p>This module dispatches to different spectral denoising models based on the <code>model_type</code> configuration parameter.</p> <p>All models output raw logits. Use model.predict() for probabilities.</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule--parameters","level":4,"title":"Parameters","text":"<p>model_type : str     Architecture to use. One of:     - \"linear_pe\": Linear positional encoding     - \"filter_bank\": Graph filter bank with spectral polynomial     - \"self_attention\": Query-key attention on eigenvectors k : int     Number of eigenvectors to use. max_nodes : int, optional     Maximum nodes (for Linear PE bias). Default 200. use_bias : bool, optional     Use bias in Linear PE. Default True. polynomial_degree : int, optional     Polynomial degree for filter bank. Default 5. d_k : int, optional     Key dimension for self-attention. Default 64. learning_rate : float, optional     Learning rate. Default 1e-4. weight_decay : float, optional     Weight decay for AdamW. Default 1e-2. optimizer_type : str, optional     Optimizer: \"adam\" or \"adamw\". Default \"adamw\". **kwargs     Additional arguments passed to base class.</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule--examples","level":4,"title":"Examples","text":"<p>module = SpectralDenoisingLightningModule( ...     model_type=\"linear_pe\", ...     k=8, ...     learning_rate=1e-4, ...     weight_decay=1e-2, ...     optimizer_type=\"adamw\", ... )</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.eval_noise_levels","level":3,"title":"<code>eval_noise_levels</code>  <code>property</code>","text":"<p>Get noise levels for evaluation.</p> <p>Returns eval_noise_levels if explicitly set, otherwise falls back to noise_levels (same levels for training and evaluation).</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.noise_levels","level":3,"title":"<code>noise_levels</code>  <code>property</code>","text":"<p>Get noise levels from datamodule or override.</p> <p>Returns the noise levels to use for training and evaluation. Prefers the datamodule's noise_levels when attached to a trainer, but falls back to the override value (or default) during standalone usage.</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.configure_optimizers","level":3,"title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizers and learning rate schedulers.</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.forward","level":3,"title":"<code>forward(x)</code>","text":"<p>Forward pass through the attention model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input adjacency matrix</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed adjacency matrix</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.get_model_config","level":3,"title":"<code>get_model_config()</code>","text":"<p>Get model configuration for logging.</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.get_model_config--returns","level":5,"title":"Returns","text":"<p>dict     Model configuration dictionary.</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.get_model_name","level":3,"title":"<code>get_model_name()</code>","text":"<p>Get model name for visualization.</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.get_model_name--returns","level":5,"title":"Returns","text":"<p>str     Human-readable model name.</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.log_parameter_count","level":3,"title":"<code>log_parameter_count()</code>","text":"<p>Log the parameter count of the model in a formatted way.</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.on_fit_start","level":3,"title":"<code>on_fit_start()</code>","text":"<p>Called at the beginning of training.</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.on_test_epoch_end","level":3,"title":"<code>on_test_epoch_end()</code>","text":"<p>Called at the end of test epoch for visualization.</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.on_validation_epoch_end","level":3,"title":"<code>on_validation_epoch_end()</code>","text":"<p>Called at the end of validation for visualization (step-based).</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.setup","level":3,"title":"<code>setup(stage)</code>","text":"<p>Validate configuration before training/testing begins.</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.setup--parameters","level":5,"title":"Parameters","text":"<p>stage     Either 'fit', 'validate', 'test', or 'predict'.</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.test_step","level":3,"title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Batch of adjacency matrices</p> required <code>batch_idx</code> <code>int</code> <p>Batch index</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary of test metrics</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.training_step","level":3,"title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Execute single training step with noise application.</p> <p>In normal mode: noise is sampled fresh for each batch. In sanity check mode (fixed_noise_seed set): uses cached noise patterns so the model sees identical noisy inputs across epochs.</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.training_step--parameters","level":5,"title":"Parameters","text":"<p>batch     Batch of clean adjacency matrices, shape (B, N, N). batch_idx     Index of current batch. Used as cache key in fixed noise mode.</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.training_step--returns","level":5,"title":"Returns","text":"<p>dict     Dictionary with 'loss' (required by Lightning) and 'logits' for debugging.</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/spectral_denoising/#tmgg.experiments.spectral_denoising.SpectralDenoisingLightningModule.validation_step","level":3,"title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Batch of adjacency matrices</p> required <code>batch_idx</code> <code>int</code> <p>Batch index</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary of validation metrics</p>","path":["Reference","Experiments","Spectral denoising"],"tags":[]},{"location":"reference/experiments/stages/","level":1,"title":"Stages","text":"<p>Stage-specific experiment runners.</p> <p>Provides CLI entry points for each experimental stage: - Stage 1: Proof of Concept - Stage 2: Core Validation - Stage 3: Dataset Diversity (future) - Stage 4: Real Benchmarks (future) - Stage 5: Full Validation (future)</p>","path":["Reference","Experiments","Stages"],"tags":[]},{"location":"reference/experiments/stages/#tmgg.experiments.stages.stage1","level":2,"title":"<code>stage1(cfg)</code>","text":"<p>Stage 1: Proof of Concept.</p> <p>Validates that spectral PE architectures can denoise graphs. Tests Linear PE, Graph Filter + Sigmoid, Self-Attention on SBM n=50.</p> <p>Budget: 4.4 GPU-hours</p>","path":["Reference","Experiments","Stages"],"tags":[]},{"location":"reference/experiments/stages/#tmgg.experiments.stages.stage1--usage","level":4,"title":"Usage","text":"<p>Single experiment:     tmgg-stage1</p> Full sweep <p>tmgg-stage1 sweep=true</p> With overrides <p>tmgg-stage1 model.k=16 learning_rate=5e-4</p>","path":["Reference","Experiments","Stages"],"tags":[]},{"location":"reference/experiments/stages/#tmgg.experiments.stages.stage2","level":2,"title":"<code>stage2(cfg)</code>","text":"<p>Stage 2: Core Validation.</p> <p>Validates generalization across configurations and compares with DiGress. Tests best architectures on multiple SBM configs.</p> <p>Budget: 166.5 GPU-hours</p>","path":["Reference","Experiments","Stages"],"tags":[]},{"location":"reference/experiments/stages/#tmgg.experiments.stages.stage2--usage","level":4,"title":"Usage","text":"<p>Single experiment:     tmgg-stage2</p> Full sweep <p>tmgg-stage2 sweep=true</p> With specific architecture <p>tmgg-stage2 model=models/spectral/filter_bank_nonlinear</p>","path":["Reference","Experiments","Stages"],"tags":[]},{"location":"reference/experiments/stages/#tmgg.experiments.stages.stage3","level":2,"title":"<code>stage3(cfg)</code>","text":"<p>Stage 3: Dataset Diversity (future work).</p> <p>Validates across all graph families.</p> <p>Budget: 400 GPU-hours (beyond initial 200h budget)</p> <p>Note: This stage is deferred to future work per experimental design.</p>","path":["Reference","Experiments","Stages"],"tags":[]},{"location":"reference/experiments/stages/#tmgg.experiments.stages.stage4","level":2,"title":"<code>stage4(cfg)</code>","text":"<p>Stage 4: Real-World Benchmarks (future work).</p> <p>Validates on practical benchmark datasets (QM9, ENZYMES, PROTEINS).</p> <p>Budget: 300 GPU-hours (beyond initial 200h budget)</p> <p>Note: This stage is deferred to future work per experimental design.</p>","path":["Reference","Experiments","Stages"],"tags":[]},{"location":"reference/experiments/stages/#tmgg.experiments.stages.stage5","level":2,"title":"<code>stage5(cfg)</code>","text":"<p>Stage 5: Full Validation (future work).</p> <p>Comprehensive ablations and robustness analysis for publication.</p> <p>Budget: 1500 GPU-hours (beyond initial 200h budget)</p> <p>Note: This stage is deferred to future work per experimental design.</p>","path":["Reference","Experiments","Stages"],"tags":[]},{"location":"reference/modal/","level":1,"title":"Modal","text":"<p>Modal cloud execution for TMGG spectral denoising experiments.</p> <p>This package provides Modal-specific implementations for running TMGG experiments on cloud GPUs with Tigris S3 storage.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.ModalRunner","level":2,"title":"<code>ModalRunner</code>","text":"<p>               Bases: <code>CloudRunner</code></p> <p>Modal-specific experiment runner.</p> <p>Executes experiments on Modal GPUs with automatic scaling and parallel execution support.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.ModalRunner.__init__","level":3,"title":"<code>__init__(gpu_type='standard', storage=None)</code>","text":"<p>Initialize Modal runner.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.ModalRunner.__init__--parameters","level":5,"title":"Parameters","text":"<p>gpu_type     Default GPU tier for experiments. storage     Tigris storage for results. If None, creates from env.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.ModalRunner.cancel","level":3,"title":"<code>cancel(run_id)</code>","text":"<p>Cancel not directly supported in Modal.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.ModalRunner.get_status","level":3,"title":"<code>get_status(run_id)</code>","text":"<p>Get status of a Modal run.</p> <p>Note: Modal doesn't provide easy status checking for completed function calls. This returns 'completed' or 'unknown'.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.ModalRunner.run_experiment","level":3,"title":"<code>run_experiment(config, gpu_type=None, timeout_seconds=None)</code>","text":"<p>Run a single experiment on Modal.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.ModalRunner.run_experiment--parameters","level":5,"title":"Parameters","text":"<p>config     Hydra configuration. gpu_type     GPU tier override. timeout_seconds     Timeout override.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.ModalRunner.run_experiment--returns","level":5,"title":"Returns","text":"<p>ExperimentResult     Result of the experiment.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.ModalRunner.run_sweep","level":3,"title":"<code>run_sweep(configs, gpu_type=None, parallelism=4, timeout_seconds=None)</code>","text":"<p>Run multiple experiments in parallel on Modal.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.ModalRunner.run_sweep--parameters","level":5,"title":"Parameters","text":"<p>configs     List of configurations. gpu_type     GPU tier for all experiments. parallelism     Maximum concurrent experiments. timeout_seconds     Timeout per experiment.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.ModalRunner.run_sweep--returns","level":5,"title":"Returns","text":"<p>list[ExperimentResult]     Results from all experiments.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage","level":2,"title":"<code>TigrisStorage</code>","text":"<p>Tigris S3-compatible storage configured from environment variables.</p> <p>Reads configuration from Modal secrets via environment variables: - TMGG_TIGRIS_BUCKET: Bucket name - TMGG_TIGRIS_ENDPOINT: Tigris endpoint URL - TMGG_TIGRIS_ACCESS_KEY: Access key ID - TMGG_TIGRIS_SECRET_KEY: Secret access key</p> <p>These should be configured as Modal secrets and attached to functions.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.client","level":3,"title":"<code>client</code>  <code>property</code>","text":"<p>Lazy-initialize boto3 client.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.__init__","level":3,"title":"<code>__init__(prefix='tmgg-experiments')</code>","text":"<p>Initialize Tigris storage from environment variables.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.__init__--parameters","level":5,"title":"Parameters","text":"<p>prefix     Key prefix for all objects. Default \"tmgg-experiments\".</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.__init__--raises","level":5,"title":"Raises","text":"<p>ValueError     If required environment variables are not set.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.download_checkpoint","level":3,"title":"<code>download_checkpoint(run_id, local_dir)</code>","text":"<p>Download the latest checkpoint for a run.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.download_checkpoint--parameters","level":5,"title":"Parameters","text":"<p>run_id     Unique run identifier. local_dir     Directory to save the checkpoint.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.download_checkpoint--returns","level":5,"title":"Returns","text":"<p>Path     Local path to the downloaded checkpoint.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.download_file","level":3,"title":"<code>download_file(remote_key, local_path)</code>","text":"<p>Download a file from Tigris.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.download_file--parameters","level":5,"title":"Parameters","text":"<p>remote_key     Key (path) in the storage bucket. local_path     Local path to save the file.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.download_file--returns","level":5,"title":"Returns","text":"<p>Path     The local path where the file was saved.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.download_metrics","level":3,"title":"<code>download_metrics(run_id)</code>","text":"<p>Download metrics JSON.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.download_metrics--parameters","level":5,"title":"Parameters","text":"<p>run_id     Unique identifier for the run.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.download_metrics--returns","level":5,"title":"Returns","text":"<p>dict     The stored metrics dictionary.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.exists","level":3,"title":"<code>exists(remote_key)</code>","text":"<p>Check if a key exists in storage.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.exists--parameters","level":5,"title":"Parameters","text":"<p>remote_key     Key to check.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.exists--returns","level":5,"title":"Returns","text":"<p>bool     True if the key exists.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.list_checkpoints","level":3,"title":"<code>list_checkpoints(prefix='')</code>","text":"<p>List checkpoint files in the bucket.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.list_checkpoints--parameters","level":5,"title":"Parameters","text":"<p>prefix     Optional prefix to filter results.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.list_checkpoints--returns","level":5,"title":"Returns","text":"<p>list[str]     List of checkpoint keys matching the prefix.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.sync_results","level":3,"title":"<code>sync_results(local_dir, run_id)</code>","text":"<p>Sync all result files from a local directory to storage.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.sync_results--parameters","level":5,"title":"Parameters","text":"<p>local_dir     Local directory containing result files. run_id     Unique run identifier.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.sync_results--returns","level":5,"title":"Returns","text":"<p>list[str]     List of uploaded S3 URIs.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.upload_checkpoint","level":3,"title":"<code>upload_checkpoint(local_path, run_id)</code>","text":"<p>Upload a model checkpoint.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.upload_checkpoint--parameters","level":5,"title":"Parameters","text":"<p>local_path     Path to the checkpoint file. run_id     Unique run identifier.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.upload_checkpoint--returns","level":5,"title":"Returns","text":"<p>str     S3 URI of the uploaded checkpoint.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.upload_file","level":3,"title":"<code>upload_file(local_path, remote_key)</code>","text":"<p>Upload a local file to Tigris.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.upload_file--parameters","level":5,"title":"Parameters","text":"<p>local_path     Path to the local file. remote_key     Key (path) in the storage bucket.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.upload_file--returns","level":5,"title":"Returns","text":"<p>str     The full S3 URI of the uploaded file.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.upload_metrics","level":3,"title":"<code>upload_metrics(metrics, run_id)</code>","text":"<p>Upload metrics as JSON.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.upload_metrics--parameters","level":5,"title":"Parameters","text":"<p>metrics     Dictionary of metrics to store. run_id     Unique identifier for the run.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/#tmgg.modal.TigrisStorage.upload_metrics--returns","level":5,"title":"Returns","text":"<p>str     The S3 URI of the metrics file.</p>","path":["Reference","Modal"],"tags":[]},{"location":"reference/modal/stages/","level":1,"title":"Stages","text":"<p>Stage-specific Modal functions.</p> <p>Provides Modal entry points for each experimental stage.</p>","path":["Reference","Modal","Stages"],"tags":[]},{"location":"reference/modal/stages/#tmgg.modal.stages.run_stage1","level":2,"title":"<code>run_stage1(parallelism=4, gpu_type='standard', dry_run=False)</code>","text":"<p>Run Stage 1: Proof of Concept experiments on Modal.</p>","path":["Reference","Modal","Stages"],"tags":[]},{"location":"reference/modal/stages/#tmgg.modal.stages.run_stage1--parameters","level":4,"title":"Parameters","text":"<p>parallelism     Maximum concurrent experiments. gpu_type     GPU tier for experiments. dry_run     If True, print configs without running.</p>","path":["Reference","Modal","Stages"],"tags":[]},{"location":"reference/modal/stages/#tmgg.modal.stages.run_stage1--returns","level":4,"title":"Returns","text":"<p>dict     Stage results summary.</p>","path":["Reference","Modal","Stages"],"tags":[]},{"location":"reference/modal/stages/#tmgg.modal.stages.run_stage2","level":2,"title":"<code>run_stage2(parallelism=4, gpu_type='standard', use_stage1_best=True, dry_run=False)</code>","text":"<p>Run Stage 2: Core Validation experiments on Modal.</p>","path":["Reference","Modal","Stages"],"tags":[]},{"location":"reference/modal/stages/#tmgg.modal.stages.run_stage2--parameters","level":4,"title":"Parameters","text":"<p>parallelism     Maximum concurrent experiments. gpu_type     GPU tier for experiments. use_stage1_best     If True, use Stage 1 best config to narrow search. dry_run     If True, print configs without running.</p>","path":["Reference","Modal","Stages"],"tags":[]},{"location":"reference/modal/stages/#tmgg.modal.stages.run_stage2--returns","level":4,"title":"Returns","text":"<p>dict     Stage results summary.</p>","path":["Reference","Modal","Stages"],"tags":[]},{"location":"reference/models/","level":1,"title":"Models","text":"<p>Shared model architectures for graph denoising experiments.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.DenoisingModel","level":2,"title":"<code>DenoisingModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Abstract base class for graph denoising models.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.DenoisingModel.__init__","level":3,"title":"<code>__init__()</code>","text":"<p>Initialize denoising model.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.DenoisingModel.forward","level":3,"title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward pass for denoising.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.DenoisingModel.forward--parameters","level":5,"title":"Parameters","text":"<p>x     Input tensor (typically noisy adjacency matrix).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.DenoisingModel.forward--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Raw logits (unbounded). Use predict() for [0, 1] probabilities.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.DenoisingModel.get_config","level":3,"title":"<code>get_config()</code>  <code>abstractmethod</code>","text":"<p>Get model configuration for logging/saving.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing model hyperparameters</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.DenoisingModel.logits_to_graph","level":3,"title":"<code>logits_to_graph(logits)</code>","text":"<p>Convert logits to binary graph predictions.</p> <p>For BCE loss, sigmoid(x) &gt; 0.5 is equivalent to x &gt; 0, so we threshold logits directly. Subclasses may override for different thresholding.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.DenoisingModel.logits_to_graph--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward().</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.DenoisingModel.logits_to_graph--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions (0 or 1).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.DenoisingModel.parameter_count","level":3,"title":"<code>parameter_count()</code>","text":"<p>Count trainable parameters in this module and its children.</p> <p>Returns a hierarchical dictionary with: - \"total\": Total trainable parameters in this module and all children - \"self\": Parameters directly owned by this module (not in children) - Child module counts with their names as keys</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parameter counts</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.DenoisingModel.predict","level":3,"title":"<code>predict(logits, zero_diag=True)</code>","text":"<p>Convert model output (logits) to binary graph predictions.</p> <p>Uses logits_to_graph() for thresholding, then optionally zeros diagonal (adjacency matrices have no self-loops).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.DenoisingModel.predict--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward(). zero_diag     If True, zero out diagonal entries. Default True since adjacency     matrices typically have no self-loops.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.DenoisingModel.predict--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions with zero diagonal (if zero_diag=True).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.DenoisingModel.transform_for_loss","level":3,"title":"<code>transform_for_loss(output, target)</code>","text":"<p>Return output and target directly for loss computation.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.DenoisingModel.transform_for_loss--parameters","level":5,"title":"Parameters","text":"<p>output     Model output tensor (denoised adjacency). target     Target tensor (clean adjacency matrix).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.DenoisingModel.transform_for_loss--returns","level":5,"title":"Returns","text":"<p>tuple     (output, target) unchanged.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.EigenEmbedding","level":2,"title":"<code>EigenEmbedding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Embedding layer using eigenvectors of adjacency matrix.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.EigenEmbedding--parameters","level":4,"title":"Parameters","text":"<p>eigenvalue_reg : float, optional     Diagonal regularization added before eigendecomposition. This spreads     eigenvalues apart, improving gradient stability through the eigh     operation. Default is 0.0 (no regularization). Values around 1e-4 to     1e-2 help when training produces NaN gradients or unstable loss.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.EigenEmbedding.forward","level":3,"title":"<code>forward(A)</code>","text":"<p>Forward pass of eigen embedding.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>Adjacency matrix of shape (batch_size, num_nodes, num_nodes)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Eigenvector embeddings of shape (batch_size, num_nodes, num_nodes)</p> <p>Raises:</p> Type Description <code>EigenDecompositionError</code> <p>If eigendecomposition fails with debugging context</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.Etoy","level":2,"title":"<code>Etoy</code>","text":"<p>               Bases: <code>Module</code></p> <p>Aggregate edge features to graph-level features via statistics.</p> <p>Computes mean, min, max, and std across all edges, then projects to output dimension.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.Etoy.__init__","level":3,"title":"<code>__init__(de, dy)</code>","text":"","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.Etoy.__init__--parameters","level":5,"title":"Parameters","text":"<p>de : int     Edge feature dimension dy : int     Output global feature dimension</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.Etoy.forward","level":3,"title":"<code>forward(E)</code>","text":"","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.Etoy.forward--parameters","level":5,"title":"Parameters","text":"<p>E : torch.Tensor     Edge features of shape (batch_size, n_nodes, n_nodes, de)</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.Etoy.forward--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Global features of shape (batch_size, dy)</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNN","level":2,"title":"<code>GNN</code>","text":"<p>               Bases: <code>DenoisingModel</code></p> <p>Standard Graph Neural Network for adjacency matrix reconstruction.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNN.__init__","level":3,"title":"<code>__init__(num_layers, num_terms=3, feature_dim_in=10, feature_dim_out=10, eigenvalue_reg=0.0)</code>","text":"<p>Initialize GNN.</p> <p>Parameters:</p> Name Type Description Default <code>num_layers</code> <code>int</code> <p>Number of graph convolution layers</p> required <code>num_terms</code> <code>int</code> <p>Number of terms in polynomial filters</p> <code>3</code> <code>feature_dim_in</code> <code>int</code> <p>Input feature dimension</p> <code>10</code> <code>feature_dim_out</code> <code>int</code> <p>Output feature dimension</p> <code>10</code> <code>eigenvalue_reg</code> <code>float</code> <p>Diagonal regularization for eigendecomposition stability</p> <code>0.0</code>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNN.forward","level":3,"title":"<code>forward(A)</code>","text":"<p>Forward pass returning node embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>Adjacency matrix</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (X_embeddings, Y_embeddings)</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNN.get_config","level":3,"title":"<code>get_config()</code>","text":"<p>Get model configuration.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNN.logits_to_graph","level":3,"title":"<code>logits_to_graph(logits)</code>","text":"<p>Convert logits to binary graph predictions.</p> <p>For BCE loss, sigmoid(x) &gt; 0.5 is equivalent to x &gt; 0, so we threshold logits directly. Subclasses may override for different thresholding.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNN.logits_to_graph--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward().</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNN.logits_to_graph--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions (0 or 1).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNN.parameter_count","level":3,"title":"<code>parameter_count()</code>","text":"<p>Count trainable parameters in this module and its children.</p> <p>Returns a hierarchical dictionary with: - \"total\": Total trainable parameters in this module and all children - \"self\": Parameters directly owned by this module (not in children) - Child module counts with their names as keys</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parameter counts</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNN.predict","level":3,"title":"<code>predict(logits, zero_diag=True)</code>","text":"<p>Convert model output (logits) to binary graph predictions.</p> <p>Uses logits_to_graph() for thresholding, then optionally zeros diagonal (adjacency matrices have no self-loops).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNN.predict--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward(). zero_diag     If True, zero out diagonal entries. Default True since adjacency     matrices typically have no self-loops.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNN.predict--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions with zero diagonal (if zero_diag=True).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNN.transform_for_loss","level":3,"title":"<code>transform_for_loss(output, target)</code>","text":"<p>Return output and target directly for loss computation.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNN.transform_for_loss--parameters","level":5,"title":"Parameters","text":"<p>output     Model output tensor (denoised adjacency). target     Target tensor (clean adjacency matrix).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNN.transform_for_loss--returns","level":5,"title":"Returns","text":"<p>tuple     (output, target) unchanged.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNNSymmetric","level":2,"title":"<code>GNNSymmetric</code>","text":"<p>               Bases: <code>DenoisingModel</code></p> <p>Symmetric GNN using same embedding for both X and Y.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNNSymmetric.forward","level":3,"title":"<code>forward(A)</code>","text":"<p>Forward pass with symmetric embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>Adjacency matrix</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (reconstructed_adjacency, X_embeddings)</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNNSymmetric.get_config","level":3,"title":"<code>get_config()</code>","text":"<p>Get model configuration.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNNSymmetric.logits_to_graph","level":3,"title":"<code>logits_to_graph(logits)</code>","text":"<p>Convert logits to binary graph predictions.</p> <p>For BCE loss, sigmoid(x) &gt; 0.5 is equivalent to x &gt; 0, so we threshold logits directly. Subclasses may override for different thresholding.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNNSymmetric.logits_to_graph--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward().</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNNSymmetric.logits_to_graph--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions (0 or 1).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNNSymmetric.parameter_count","level":3,"title":"<code>parameter_count()</code>","text":"<p>Count trainable parameters in this module and its children.</p> <p>Returns a hierarchical dictionary with: - \"total\": Total trainable parameters in this module and all children - \"self\": Parameters directly owned by this module (not in children) - Child module counts with their names as keys</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parameter counts</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNNSymmetric.predict","level":3,"title":"<code>predict(logits, zero_diag=True)</code>","text":"<p>Convert model output (logits) to binary graph predictions.</p> <p>Uses logits_to_graph() for thresholding, then optionally zeros diagonal (adjacency matrices have no self-loops).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNNSymmetric.predict--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward(). zero_diag     If True, zero out diagonal entries. Default True since adjacency     matrices typically have no self-loops.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNNSymmetric.predict--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions with zero diagonal (if zero_diag=True).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNNSymmetric.transform_for_loss","level":3,"title":"<code>transform_for_loss(output, target)</code>","text":"<p>Return output and target directly for loss computation.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNNSymmetric.transform_for_loss--parameters","level":5,"title":"Parameters","text":"<p>output     Model output tensor (denoised adjacency). target     Target tensor (clean adjacency matrix).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GNNSymmetric.transform_for_loss--returns","level":5,"title":"Returns","text":"<p>tuple     (output, target) unchanged.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GaussianEmbedding","level":2,"title":"<code>GaussianEmbedding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Gaussian embedding layer using powers of adjacency matrix.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GaussianEmbedding.__init__","level":3,"title":"<code>__init__(num_terms, num_channels)</code>","text":"<p>Initialize Gaussian embedding.</p> <p>Parameters:</p> Name Type Description Default <code>num_terms</code> <code>int</code> <p>Number of terms in the polynomial expansion</p> required <code>num_channels</code> <code>int</code> <p>Number of output channels</p> required","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GaussianEmbedding.forward","level":3,"title":"<code>forward(A)</code>","text":"<p>Forward pass of Gaussian embedding.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>Adjacency matrix of shape (batch_size, num_nodes, num_nodes)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Node embeddings of shape (batch_size, num_nodes, num_channels)</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GraphConvolutionLayer","level":2,"title":"<code>GraphConvolutionLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Graph convolution layer using polynomial filters.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GraphConvolutionLayer.__init__","level":3,"title":"<code>__init__(num_terms, num_channels)</code>","text":"<p>Initialize graph convolution layer.</p> <p>Parameters:</p> Name Type Description Default <code>num_terms</code> <code>int</code> <p>Number of terms in polynomial filter</p> required <code>num_channels</code> <code>int</code> <p>Number of input/output channels</p> required","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.GraphConvolutionLayer.forward","level":3,"title":"<code>forward(A, X)</code>","text":"<p>Forward pass of graph convolution.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>Adjacency matrix</p> required <code>X</code> <code>Tensor</code> <p>Input features</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Convolved features</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.MultiHeadAttention","level":2,"title":"<code>MultiHeadAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Multi-Head Attention module as described in 'Attention Is All You Need' paper.</p> <p>This implementation supports masked attention and different input/output dimensions.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.MultiHeadAttention.__init__","level":3,"title":"<code>__init__(d_model, num_heads, d_k=None, d_v=None, dropout=0.0, bias=False)</code>","text":"<p>Initialize the Multi-Head Attention module.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Model dimension (input and output dimension)</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>d_k</code> <code>Optional[int]</code> <p>Dimension of keys (default: d_model // num_heads)</p> <code>None</code> <code>d_v</code> <code>Optional[int]</code> <p>Dimension of values (default: d_model // num_heads)</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.0</code> <code>bias</code> <code>bool</code> <p>Whether to use bias in linear layers</p> <code>False</code>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.MultiHeadAttention.forward","level":3,"title":"<code>forward(x, mask=None, residual=None)</code>","text":"<p>Forward pass of the Multi-Head Attention module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_len, d_model)</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>Optional mask tensor of shape (batch_size, seq_len_q, seq_len_k)</p> <code>None</code> <code>residual</code> <code>Optional[Tensor]</code> <p>Optional residual connection</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple of (output, combined_attention_scores)</p> <code>Tensor</code> <ul> <li>output: Output tensor of shape (batch_size, seq_len, d_model)</li> </ul> <code>Tuple[Tensor, Tensor]</code> <ul> <li>combined_attention_scores: Attention weights of shape (batch_size, seq_len, seq_len)</li> </ul>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.MultiLayerAttention","level":2,"title":"<code>MultiLayerAttention</code>","text":"<p>               Bases: <code>DenoisingModel</code></p> <p>Multi-layer attention model for graph denoising.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.MultiLayerAttention.__init__","level":3,"title":"<code>__init__(d_model, num_heads, num_layers, d_k=None, d_v=None, dropout=0.0, bias=False)</code>","text":"<p>Initialize the Multi-Layer Attention module.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Model dimension</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>num_layers</code> <code>int</code> <p>Number of attention layers</p> required <code>d_k</code> <code>Optional[int]</code> <p>Dimension of keys (default: d_model // num_heads)</p> <code>None</code> <code>d_v</code> <code>Optional[int]</code> <p>Dimension of values (default: d_model // num_heads)</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.0</code> <code>bias</code> <code>bool</code> <p>Whether to use bias in linear layers</p> <code>False</code>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.MultiLayerAttention.forward","level":3,"title":"<code>forward(A, mask=None)</code>","text":"<p>Forward pass through all attention layers.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>Input adjacency matrix</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>Optional attention mask</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed adjacency matrix</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.MultiLayerAttention.get_config","level":3,"title":"<code>get_config()</code>","text":"<p>Get model configuration.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.MultiLayerAttention.logits_to_graph","level":3,"title":"<code>logits_to_graph(logits)</code>","text":"<p>Convert logits to binary graph predictions.</p> <p>For BCE loss, sigmoid(x) &gt; 0.5 is equivalent to x &gt; 0, so we threshold logits directly. Subclasses may override for different thresholding.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.MultiLayerAttention.logits_to_graph--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward().</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.MultiLayerAttention.logits_to_graph--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions (0 or 1).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.MultiLayerAttention.parameter_count","level":3,"title":"<code>parameter_count()</code>","text":"<p>Count trainable parameters in this module and its children.</p> <p>Returns a hierarchical dictionary with: - \"total\": Total trainable parameters in this module and all children - \"self\": Parameters directly owned by this module (not in children) - Child module counts with their names as keys</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parameter counts</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.MultiLayerAttention.predict","level":3,"title":"<code>predict(logits, zero_diag=True)</code>","text":"<p>Convert model output (logits) to binary graph predictions.</p> <p>Uses logits_to_graph() for thresholding, then optionally zeros diagonal (adjacency matrices have no self-loops).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.MultiLayerAttention.predict--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward(). zero_diag     If True, zero out diagonal entries. Default True since adjacency     matrices typically have no self-loops.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.MultiLayerAttention.predict--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions with zero diagonal (if zero_diag=True).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.MultiLayerAttention.transform_for_loss","level":3,"title":"<code>transform_for_loss(output, target)</code>","text":"<p>Return output and target directly for loss computation.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.MultiLayerAttention.transform_for_loss--parameters","level":5,"title":"Parameters","text":"<p>output     Model output tensor (denoised adjacency). target     Target tensor (clean adjacency matrix).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.MultiLayerAttention.transform_for_loss--returns","level":5,"title":"Returns","text":"<p>tuple     (output, target) unchanged.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGNN","level":2,"title":"<code>NodeVarGNN</code>","text":"<p>               Bases: <code>DenoisingModel</code></p> <p>Node-variant Graph Neural Network.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGNN.__init__","level":3,"title":"<code>__init__(num_layers, num_terms=3, feature_dim=10, eigenvalue_reg=0.0)</code>","text":"<p>Initialize Node-variant GNN.</p> <p>Parameters:</p> Name Type Description Default <code>num_layers</code> <code>int</code> <p>Number of layers</p> required <code>num_terms</code> <code>int</code> <p>Number of polynomial terms</p> <code>3</code> <code>feature_dim</code> <code>int</code> <p>Feature dimension</p> <code>10</code> <code>eigenvalue_reg</code> <code>float</code> <p>Diagonal regularization for eigendecomposition stability</p> <code>0.0</code>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGNN.forward","level":3,"title":"<code>forward(A)</code>","text":"<p>Forward pass returning reconstructed adjacency matrix.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>Input adjacency matrix</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed adjacency matrix</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGNN.get_config","level":3,"title":"<code>get_config()</code>","text":"<p>Get model configuration.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGNN.logits_to_graph","level":3,"title":"<code>logits_to_graph(logits)</code>","text":"<p>Convert logits to binary graph predictions.</p> <p>For BCE loss, sigmoid(x) &gt; 0.5 is equivalent to x &gt; 0, so we threshold logits directly. Subclasses may override for different thresholding.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGNN.logits_to_graph--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward().</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGNN.logits_to_graph--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions (0 or 1).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGNN.parameter_count","level":3,"title":"<code>parameter_count()</code>","text":"<p>Count trainable parameters in this module and its children.</p> <p>Returns a hierarchical dictionary with: - \"total\": Total trainable parameters in this module and all children - \"self\": Parameters directly owned by this module (not in children) - Child module counts with their names as keys</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parameter counts</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGNN.predict","level":3,"title":"<code>predict(logits, zero_diag=True)</code>","text":"<p>Convert model output (logits) to binary graph predictions.</p> <p>Uses logits_to_graph() for thresholding, then optionally zeros diagonal (adjacency matrices have no self-loops).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGNN.predict--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward(). zero_diag     If True, zero out diagonal entries. Default True since adjacency     matrices typically have no self-loops.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGNN.predict--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions with zero diagonal (if zero_diag=True).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGNN.transform_for_loss","level":3,"title":"<code>transform_for_loss(output, target)</code>","text":"<p>Return output and target directly for loss computation.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGNN.transform_for_loss--parameters","level":5,"title":"Parameters","text":"<p>output     Model output tensor (denoised adjacency). target     Target tensor (clean adjacency matrix).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGNN.transform_for_loss--returns","level":5,"title":"Returns","text":"<p>tuple     (output, target) unchanged.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGraphConvolutionLayer","level":2,"title":"<code>NodeVarGraphConvolutionLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Graph convolution layer with polynomial filters.</p> This layer computes a polynomial graph convolution <p>Y = sum_{i=0}^{num_terms} A^i @ X @ H[i]</p> <p>where A is the (normalized) adjacency matrix and H[i] are learnable weight matrices. Unlike the original \"node-variant\" design, parameters are shared across all nodes, enabling support for variable graph sizes.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGraphConvolutionLayer--parameters","level":4,"title":"Parameters","text":"<p>num_terms : int     Number of polynomial terms (excluding the identity term). num_channels_in : int     Input feature dimension. num_channels_out : int     Output feature dimension.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGraphConvolutionLayer--notes","level":4,"title":"Notes","text":"<p>The <code>num_nodes</code> parameter from the original API is kept for backwards compatibility but is now ignored - the layer works with any graph size.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGraphConvolutionLayer.num_nodes","level":3,"title":"<code>num_nodes</code>  <code>property</code>","text":"<p>Backwards compatibility: return a placeholder value.</p> <p>The new design doesn't store num_nodes since it's node-agnostic. Returns -1 to indicate this, which will always differ from actual node counts, preventing the old dynamic recreation logic from triggering.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGraphConvolutionLayer.__init__","level":3,"title":"<code>__init__(num_terms, num_channels_in, num_channels_out=None)</code>","text":"<p>Initialize the graph convolution layer.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGraphConvolutionLayer.__init__--parameters","level":5,"title":"Parameters","text":"<p>num_terms : int     Number of polynomial terms. num_channels_in : int     Input feature dimension. num_channels_out : int, optional     Output feature dimension. If None, defaults to num_channels_in.     (For backwards compatibility with the old 3-arg signature where     the third arg was num_nodes, this is ignored if it seems too large.)</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGraphConvolutionLayer.forward","level":3,"title":"<code>forward(A, X)</code>","text":"<p>Forward pass of graph convolution.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGraphConvolutionLayer.forward--parameters","level":5,"title":"Parameters","text":"<p>A : torch.Tensor     Adjacency matrix of shape (batch, num_nodes, num_nodes). X : torch.Tensor     Input features of shape (batch, num_nodes, num_channels_in).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.NodeVarGraphConvolutionLayer.forward--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Output features of shape (batch, num_nodes, num_channels_out).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.SequentialDenoisingModel","level":2,"title":"<code>SequentialDenoisingModel</code>","text":"<p>               Bases: <code>DenoisingModel</code></p> <p>Sequential model combining GNN embeddings with attention-based denoising.</p> <p>This model first generates embeddings using a GNN, then applies a transformer to denoise these embeddings, and finally reconstructs the adjacency matrix.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.SequentialDenoisingModel.__init__","level":3,"title":"<code>__init__(embedding_model, denoising_model=None)</code>","text":"<p>Initialize the sequential denoising model.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_model</code> <code>EmbeddingModel</code> <p>GNN model for generating embeddings</p> required <code>denoising_model</code> <code>Optional[DenoisingModel]</code> <p>Optional transformer model for denoising embeddings</p> <code>None</code>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.SequentialDenoisingModel.forward","level":3,"title":"<code>forward(x)</code>","text":"<p>Forward pass combining GNN embedding and transformer denoising.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input adjacency matrix</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed adjacency matrix</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.SequentialDenoisingModel.get_config","level":3,"title":"<code>get_config()</code>","text":"<p>Get configuration for both embedding and denoising components.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.SequentialDenoisingModel.logits_to_graph","level":3,"title":"<code>logits_to_graph(logits)</code>","text":"<p>Convert logits to binary graph predictions.</p> <p>For BCE loss, sigmoid(x) &gt; 0.5 is equivalent to x &gt; 0, so we threshold logits directly. Subclasses may override for different thresholding.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.SequentialDenoisingModel.logits_to_graph--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward().</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.SequentialDenoisingModel.logits_to_graph--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions (0 or 1).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.SequentialDenoisingModel.parameter_count","level":3,"title":"<code>parameter_count()</code>","text":"<p>Count trainable parameters in this module and its children.</p> <p>Returns a hierarchical dictionary with: - \"total\": Total trainable parameters in this module and all children - \"self\": Parameters directly owned by this module (not in children) - Child module counts with their names as keys</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parameter counts</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.SequentialDenoisingModel.predict","level":3,"title":"<code>predict(logits, zero_diag=True)</code>","text":"<p>Convert model output (logits) to binary graph predictions.</p> <p>Uses logits_to_graph() for thresholding, then optionally zeros diagonal (adjacency matrices have no self-loops).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.SequentialDenoisingModel.predict--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward(). zero_diag     If True, zero out diagonal entries. Default True since adjacency     matrices typically have no self-loops.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.SequentialDenoisingModel.predict--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions with zero diagonal (if zero_diag=True).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.SequentialDenoisingModel.transform_for_loss","level":3,"title":"<code>transform_for_loss(output, target)</code>","text":"<p>Return output and target directly for loss computation.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.SequentialDenoisingModel.transform_for_loss--parameters","level":5,"title":"Parameters","text":"<p>output     Model output tensor (denoised adjacency). target     Target tensor (clean adjacency matrix).</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.SequentialDenoisingModel.transform_for_loss--returns","level":5,"title":"Returns","text":"<p>tuple     (output, target) unchanged.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.Xtoy","level":2,"title":"<code>Xtoy</code>","text":"<p>               Bases: <code>Module</code></p> <p>Aggregate node features to graph-level features via statistics.</p> <p>Computes mean, min, max, and std across nodes, then projects to output dimension.</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.Xtoy.__init__","level":3,"title":"<code>__init__(dx, dy)</code>","text":"","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.Xtoy.__init__--parameters","level":5,"title":"Parameters","text":"<p>dx : int     Node feature dimension dy : int     Output global feature dimension</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.Xtoy.forward","level":3,"title":"<code>forward(X)</code>","text":"","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.Xtoy.forward--parameters","level":5,"title":"Parameters","text":"<p>X : torch.Tensor     Node features of shape (batch_size, n_nodes, dx)</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.Xtoy.forward--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Global features of shape (batch_size, dy)</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/#tmgg.models.create_sequential_model","level":2,"title":"<code>create_sequential_model(gnn_config, transformer_config=None)</code>","text":"<p>Factory function to create a sequential denoising model.</p> <p>Parameters:</p> Name Type Description Default <code>gnn_config</code> <code>Dict[str, Any]</code> <p>Configuration for the GNN embedding model</p> required <code>transformer_config</code> <code>Optional[Dict[str, Any]]</code> <p>Optional configuration for the transformer denoising model</p> <code>None</code> <p>Returns:</p> Type Description <code>SequentialDenoisingModel</code> <p>Configured SequentialDenoisingModel</p>","path":["Reference","Models"],"tags":[]},{"location":"reference/models/attention/","level":1,"title":"Attention","text":"","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/attention/#tmgg.models.attention.MultiHeadAttention","level":2,"title":"<code>MultiHeadAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Multi-Head Attention module as described in 'Attention Is All You Need' paper.</p> <p>This implementation supports masked attention and different input/output dimensions.</p>","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/attention/#tmgg.models.attention.MultiHeadAttention.__init__","level":3,"title":"<code>__init__(d_model, num_heads, d_k=None, d_v=None, dropout=0.0, bias=False)</code>","text":"<p>Initialize the Multi-Head Attention module.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Model dimension (input and output dimension)</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>d_k</code> <code>Optional[int]</code> <p>Dimension of keys (default: d_model // num_heads)</p> <code>None</code> <code>d_v</code> <code>Optional[int]</code> <p>Dimension of values (default: d_model // num_heads)</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.0</code> <code>bias</code> <code>bool</code> <p>Whether to use bias in linear layers</p> <code>False</code>","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/attention/#tmgg.models.attention.MultiHeadAttention.forward","level":3,"title":"<code>forward(x, mask=None, residual=None)</code>","text":"<p>Forward pass of the Multi-Head Attention module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_len, d_model)</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>Optional mask tensor of shape (batch_size, seq_len_q, seq_len_k)</p> <code>None</code> <code>residual</code> <code>Optional[Tensor]</code> <p>Optional residual connection</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple of (output, combined_attention_scores)</p> <code>Tensor</code> <ul> <li>output: Output tensor of shape (batch_size, seq_len, d_model)</li> </ul> <code>Tuple[Tensor, Tensor]</code> <ul> <li>combined_attention_scores: Attention weights of shape (batch_size, seq_len, seq_len)</li> </ul>","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/attention/#tmgg.models.attention.MultiLayerAttention","level":2,"title":"<code>MultiLayerAttention</code>","text":"<p>               Bases: <code>DenoisingModel</code></p> <p>Multi-layer attention model for graph denoising.</p>","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/attention/#tmgg.models.attention.MultiLayerAttention.__init__","level":3,"title":"<code>__init__(d_model, num_heads, num_layers, d_k=None, d_v=None, dropout=0.0, bias=False)</code>","text":"<p>Initialize the Multi-Layer Attention module.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Model dimension</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>num_layers</code> <code>int</code> <p>Number of attention layers</p> required <code>d_k</code> <code>Optional[int]</code> <p>Dimension of keys (default: d_model // num_heads)</p> <code>None</code> <code>d_v</code> <code>Optional[int]</code> <p>Dimension of values (default: d_model // num_heads)</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.0</code> <code>bias</code> <code>bool</code> <p>Whether to use bias in linear layers</p> <code>False</code>","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/attention/#tmgg.models.attention.MultiLayerAttention.forward","level":3,"title":"<code>forward(A, mask=None)</code>","text":"<p>Forward pass through all attention layers.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>Input adjacency matrix</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>Optional attention mask</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed adjacency matrix</p>","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/attention/#tmgg.models.attention.MultiLayerAttention.get_config","level":3,"title":"<code>get_config()</code>","text":"<p>Get model configuration.</p>","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/attention/#tmgg.models.attention.MultiLayerAttention.logits_to_graph","level":3,"title":"<code>logits_to_graph(logits)</code>","text":"<p>Convert logits to binary graph predictions.</p> <p>For BCE loss, sigmoid(x) &gt; 0.5 is equivalent to x &gt; 0, so we threshold logits directly. Subclasses may override for different thresholding.</p>","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/attention/#tmgg.models.attention.MultiLayerAttention.logits_to_graph--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward().</p>","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/attention/#tmgg.models.attention.MultiLayerAttention.logits_to_graph--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions (0 or 1).</p>","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/attention/#tmgg.models.attention.MultiLayerAttention.parameter_count","level":3,"title":"<code>parameter_count()</code>","text":"<p>Count trainable parameters in this module and its children.</p> <p>Returns a hierarchical dictionary with: - \"total\": Total trainable parameters in this module and all children - \"self\": Parameters directly owned by this module (not in children) - Child module counts with their names as keys</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parameter counts</p>","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/attention/#tmgg.models.attention.MultiLayerAttention.predict","level":3,"title":"<code>predict(logits, zero_diag=True)</code>","text":"<p>Convert model output (logits) to binary graph predictions.</p> <p>Uses logits_to_graph() for thresholding, then optionally zeros diagonal (adjacency matrices have no self-loops).</p>","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/attention/#tmgg.models.attention.MultiLayerAttention.predict--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward(). zero_diag     If True, zero out diagonal entries. Default True since adjacency     matrices typically have no self-loops.</p>","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/attention/#tmgg.models.attention.MultiLayerAttention.predict--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions with zero diagonal (if zero_diag=True).</p>","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/attention/#tmgg.models.attention.MultiLayerAttention.transform_for_loss","level":3,"title":"<code>transform_for_loss(output, target)</code>","text":"<p>Return output and target directly for loss computation.</p>","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/attention/#tmgg.models.attention.MultiLayerAttention.transform_for_loss--parameters","level":5,"title":"Parameters","text":"<p>output     Model output tensor (denoised adjacency). target     Target tensor (clean adjacency matrix).</p>","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/attention/#tmgg.models.attention.MultiLayerAttention.transform_for_loss--returns","level":5,"title":"Returns","text":"<p>tuple     (output, target) unchanged.</p>","path":["Reference","Models","Attention"],"tags":[]},{"location":"reference/models/baselines/","level":1,"title":"Baselines","text":"<p>Baseline denoising models for sanity checking training pipelines.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline","level":2,"title":"<code>LinearBaseline</code>","text":"<p>               Bases: <code>DenoisingModel</code></p> <p>Linear transformation baseline: A_pred = W @ A @ W.T + b.</p> <p>This model applies a learnable linear transformation to the input adjacency matrix. Initialized with W=I (identity) so the initial output equals the input, providing a stable starting point for gradient descent.</p> <p>Use this model to verify the training pipeline works correctly. If this simple model cannot learn, the issue lies in the training loop or data, not in model architecture complexity.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline--parameters","level":4,"title":"Parameters","text":"<p>max_nodes     Maximum number of nodes in the graph. Determines parameter dimensions.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline--attributes","level":4,"title":"Attributes","text":"<p>W : nn.Parameter     Learnable weight matrix of shape (max_nodes, max_nodes). b : nn.Parameter     Learnable bias matrix of shape (max_nodes, max_nodes).</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline--examples","level":4,"title":"Examples","text":"<p>model = LinearBaseline(max_nodes=32) A = torch.eye(32).unsqueeze(0)  # batch of 1 logits = model(A)  # Returns raw logits probs = model.predict(logits)  # Apply sigmoid for [0, 1]</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline.forward","level":3,"title":"<code>forward(A)</code>","text":"<p>Apply linear transformation to input adjacency matrix.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline.forward--parameters","level":5,"title":"Parameters","text":"<p>A     Input adjacency matrix of shape (batch, N, N) where N &lt;= max_nodes.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline.forward--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Raw logits of shape (batch, N, N). Use predict() for probabilities.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline.get_config","level":3,"title":"<code>get_config()</code>","text":"<p>Return model configuration.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline.logits_to_graph","level":3,"title":"<code>logits_to_graph(logits)</code>","text":"<p>Convert logits to binary graph predictions.</p> <p>For BCE loss, sigmoid(x) &gt; 0.5 is equivalent to x &gt; 0, so we threshold logits directly. Subclasses may override for different thresholding.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline.logits_to_graph--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward().</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline.logits_to_graph--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions (0 or 1).</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline.parameter_count","level":3,"title":"<code>parameter_count()</code>","text":"<p>Count trainable parameters in this module and its children.</p> <p>Returns a hierarchical dictionary with: - \"total\": Total trainable parameters in this module and all children - \"self\": Parameters directly owned by this module (not in children) - Child module counts with their names as keys</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parameter counts</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline.predict","level":3,"title":"<code>predict(logits, zero_diag=True)</code>","text":"<p>Convert model output (logits) to binary graph predictions.</p> <p>Uses logits_to_graph() for thresholding, then optionally zeros diagonal (adjacency matrices have no self-loops).</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline.predict--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward(). zero_diag     If True, zero out diagonal entries. Default True since adjacency     matrices typically have no self-loops.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline.predict--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions with zero diagonal (if zero_diag=True).</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline.transform_for_loss","level":3,"title":"<code>transform_for_loss(output, target)</code>","text":"<p>Return output and target directly for loss computation.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline.transform_for_loss--parameters","level":5,"title":"Parameters","text":"<p>output     Model output tensor (denoised adjacency). target     Target tensor (clean adjacency matrix).</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.LinearBaseline.transform_for_loss--returns","level":5,"title":"Returns","text":"<p>tuple     (output, target) unchanged.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline","level":2,"title":"<code>MLPBaseline</code>","text":"<p>               Bases: <code>DenoisingModel</code></p> <p>MLP baseline: flatten -&gt; MLP -&gt; reshape.</p> <p>This model treats the adjacency matrix as a flat vector, processes it through a standard MLP, and reshapes back to a matrix. It ignores graph structure entirely but should be able to learn if the training loop works.</p> <p>Use this model as a sanity check: if an MLP cannot learn to denoise, the training pipeline itself has issues.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline--parameters","level":4,"title":"Parameters","text":"<p>max_nodes     Maximum number of nodes in the graph. hidden_dim     Size of hidden layers in the MLP. Default 256. num_layers     Number of hidden layers. Default 2.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline--attributes","level":4,"title":"Attributes","text":"<p>mlp : nn.Sequential     The MLP architecture with ReLU activations.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline--examples","level":4,"title":"Examples","text":"<p>model = MLPBaseline(max_nodes=32, hidden_dim=256) A = torch.rand(4, 32, 32)  # batch of 4 logits = model(A)  # Returns raw logits probs = model.predict(logits)  # Apply sigmoid for [0, 1]</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline.forward","level":3,"title":"<code>forward(A)</code>","text":"<p>Apply MLP to flattened adjacency matrix.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline.forward--parameters","level":5,"title":"Parameters","text":"<p>A     Input adjacency matrix of shape (batch, N, N) where N &lt;= max_nodes.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline.forward--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Raw logits of shape (batch, N, N). Use predict() for probabilities.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline.get_config","level":3,"title":"<code>get_config()</code>","text":"<p>Return model configuration.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline.logits_to_graph","level":3,"title":"<code>logits_to_graph(logits)</code>","text":"<p>Convert logits to binary graph predictions.</p> <p>For BCE loss, sigmoid(x) &gt; 0.5 is equivalent to x &gt; 0, so we threshold logits directly. Subclasses may override for different thresholding.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline.logits_to_graph--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward().</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline.logits_to_graph--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions (0 or 1).</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline.parameter_count","level":3,"title":"<code>parameter_count()</code>","text":"<p>Count trainable parameters in this module and its children.</p> <p>Returns a hierarchical dictionary with: - \"total\": Total trainable parameters in this module and all children - \"self\": Parameters directly owned by this module (not in children) - Child module counts with their names as keys</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parameter counts</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline.predict","level":3,"title":"<code>predict(logits, zero_diag=True)</code>","text":"<p>Convert model output (logits) to binary graph predictions.</p> <p>Uses logits_to_graph() for thresholding, then optionally zeros diagonal (adjacency matrices have no self-loops).</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline.predict--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward(). zero_diag     If True, zero out diagonal entries. Default True since adjacency     matrices typically have no self-loops.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline.predict--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions with zero diagonal (if zero_diag=True).</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline.transform_for_loss","level":3,"title":"<code>transform_for_loss(output, target)</code>","text":"<p>Return output and target directly for loss computation.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline.transform_for_loss--parameters","level":5,"title":"Parameters","text":"<p>output     Model output tensor (denoised adjacency). target     Target tensor (clean adjacency matrix).</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/baselines/#tmgg.models.baselines.MLPBaseline.transform_for_loss--returns","level":5,"title":"Returns","text":"<p>tuple     (output, target) unchanged.</p>","path":["Reference","Models","Baselines"],"tags":[]},{"location":"reference/models/digress/","level":1,"title":"DiGress","text":"","path":["Reference","Models","DiGress"],"tags":[]},{"location":"reference/models/gnn/","level":1,"title":"GNN","text":"","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNN","level":2,"title":"<code>GNN</code>","text":"<p>               Bases: <code>DenoisingModel</code></p> <p>Standard Graph Neural Network for adjacency matrix reconstruction.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNN.__init__","level":3,"title":"<code>__init__(num_layers, num_terms=3, feature_dim_in=10, feature_dim_out=10, eigenvalue_reg=0.0)</code>","text":"<p>Initialize GNN.</p> <p>Parameters:</p> Name Type Description Default <code>num_layers</code> <code>int</code> <p>Number of graph convolution layers</p> required <code>num_terms</code> <code>int</code> <p>Number of terms in polynomial filters</p> <code>3</code> <code>feature_dim_in</code> <code>int</code> <p>Input feature dimension</p> <code>10</code> <code>feature_dim_out</code> <code>int</code> <p>Output feature dimension</p> <code>10</code> <code>eigenvalue_reg</code> <code>float</code> <p>Diagonal regularization for eigendecomposition stability</p> <code>0.0</code>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNN.forward","level":3,"title":"<code>forward(A)</code>","text":"<p>Forward pass returning node embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>Adjacency matrix</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (X_embeddings, Y_embeddings)</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNN.get_config","level":3,"title":"<code>get_config()</code>","text":"<p>Get model configuration.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNN.logits_to_graph","level":3,"title":"<code>logits_to_graph(logits)</code>","text":"<p>Convert logits to binary graph predictions.</p> <p>For BCE loss, sigmoid(x) &gt; 0.5 is equivalent to x &gt; 0, so we threshold logits directly. Subclasses may override for different thresholding.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNN.logits_to_graph--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward().</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNN.logits_to_graph--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions (0 or 1).</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNN.parameter_count","level":3,"title":"<code>parameter_count()</code>","text":"<p>Count trainable parameters in this module and its children.</p> <p>Returns a hierarchical dictionary with: - \"total\": Total trainable parameters in this module and all children - \"self\": Parameters directly owned by this module (not in children) - Child module counts with their names as keys</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parameter counts</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNN.predict","level":3,"title":"<code>predict(logits, zero_diag=True)</code>","text":"<p>Convert model output (logits) to binary graph predictions.</p> <p>Uses logits_to_graph() for thresholding, then optionally zeros diagonal (adjacency matrices have no self-loops).</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNN.predict--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward(). zero_diag     If True, zero out diagonal entries. Default True since adjacency     matrices typically have no self-loops.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNN.predict--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions with zero diagonal (if zero_diag=True).</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNN.transform_for_loss","level":3,"title":"<code>transform_for_loss(output, target)</code>","text":"<p>Return output and target directly for loss computation.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNN.transform_for_loss--parameters","level":5,"title":"Parameters","text":"<p>output     Model output tensor (denoised adjacency). target     Target tensor (clean adjacency matrix).</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNN.transform_for_loss--returns","level":5,"title":"Returns","text":"<p>tuple     (output, target) unchanged.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNNSymmetric","level":2,"title":"<code>GNNSymmetric</code>","text":"<p>               Bases: <code>DenoisingModel</code></p> <p>Symmetric GNN using same embedding for both X and Y.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNNSymmetric.forward","level":3,"title":"<code>forward(A)</code>","text":"<p>Forward pass with symmetric embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>Adjacency matrix</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (reconstructed_adjacency, X_embeddings)</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNNSymmetric.get_config","level":3,"title":"<code>get_config()</code>","text":"<p>Get model configuration.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNNSymmetric.logits_to_graph","level":3,"title":"<code>logits_to_graph(logits)</code>","text":"<p>Convert logits to binary graph predictions.</p> <p>For BCE loss, sigmoid(x) &gt; 0.5 is equivalent to x &gt; 0, so we threshold logits directly. Subclasses may override for different thresholding.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNNSymmetric.logits_to_graph--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward().</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNNSymmetric.logits_to_graph--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions (0 or 1).</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNNSymmetric.parameter_count","level":3,"title":"<code>parameter_count()</code>","text":"<p>Count trainable parameters in this module and its children.</p> <p>Returns a hierarchical dictionary with: - \"total\": Total trainable parameters in this module and all children - \"self\": Parameters directly owned by this module (not in children) - Child module counts with their names as keys</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parameter counts</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNNSymmetric.predict","level":3,"title":"<code>predict(logits, zero_diag=True)</code>","text":"<p>Convert model output (logits) to binary graph predictions.</p> <p>Uses logits_to_graph() for thresholding, then optionally zeros diagonal (adjacency matrices have no self-loops).</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNNSymmetric.predict--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward(). zero_diag     If True, zero out diagonal entries. Default True since adjacency     matrices typically have no self-loops.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNNSymmetric.predict--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions with zero diagonal (if zero_diag=True).</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNNSymmetric.transform_for_loss","level":3,"title":"<code>transform_for_loss(output, target)</code>","text":"<p>Return output and target directly for loss computation.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNNSymmetric.transform_for_loss--parameters","level":5,"title":"Parameters","text":"<p>output     Model output tensor (denoised adjacency). target     Target tensor (clean adjacency matrix).</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.GNNSymmetric.transform_for_loss--returns","level":5,"title":"Returns","text":"<p>tuple     (output, target) unchanged.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.NodeVarGNN","level":2,"title":"<code>NodeVarGNN</code>","text":"<p>               Bases: <code>DenoisingModel</code></p> <p>Node-variant Graph Neural Network.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.NodeVarGNN.__init__","level":3,"title":"<code>__init__(num_layers, num_terms=3, feature_dim=10, eigenvalue_reg=0.0)</code>","text":"<p>Initialize Node-variant GNN.</p> <p>Parameters:</p> Name Type Description Default <code>num_layers</code> <code>int</code> <p>Number of layers</p> required <code>num_terms</code> <code>int</code> <p>Number of polynomial terms</p> <code>3</code> <code>feature_dim</code> <code>int</code> <p>Feature dimension</p> <code>10</code> <code>eigenvalue_reg</code> <code>float</code> <p>Diagonal regularization for eigendecomposition stability</p> <code>0.0</code>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.NodeVarGNN.forward","level":3,"title":"<code>forward(A)</code>","text":"<p>Forward pass returning reconstructed adjacency matrix.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>Input adjacency matrix</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed adjacency matrix</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.NodeVarGNN.get_config","level":3,"title":"<code>get_config()</code>","text":"<p>Get model configuration.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.NodeVarGNN.logits_to_graph","level":3,"title":"<code>logits_to_graph(logits)</code>","text":"<p>Convert logits to binary graph predictions.</p> <p>For BCE loss, sigmoid(x) &gt; 0.5 is equivalent to x &gt; 0, so we threshold logits directly. Subclasses may override for different thresholding.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.NodeVarGNN.logits_to_graph--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward().</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.NodeVarGNN.logits_to_graph--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions (0 or 1).</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.NodeVarGNN.parameter_count","level":3,"title":"<code>parameter_count()</code>","text":"<p>Count trainable parameters in this module and its children.</p> <p>Returns a hierarchical dictionary with: - \"total\": Total trainable parameters in this module and all children - \"self\": Parameters directly owned by this module (not in children) - Child module counts with their names as keys</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parameter counts</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.NodeVarGNN.predict","level":3,"title":"<code>predict(logits, zero_diag=True)</code>","text":"<p>Convert model output (logits) to binary graph predictions.</p> <p>Uses logits_to_graph() for thresholding, then optionally zeros diagonal (adjacency matrices have no self-loops).</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.NodeVarGNN.predict--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward(). zero_diag     If True, zero out diagonal entries. Default True since adjacency     matrices typically have no self-loops.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.NodeVarGNN.predict--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions with zero diagonal (if zero_diag=True).</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.NodeVarGNN.transform_for_loss","level":3,"title":"<code>transform_for_loss(output, target)</code>","text":"<p>Return output and target directly for loss computation.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.NodeVarGNN.transform_for_loss--parameters","level":5,"title":"Parameters","text":"<p>output     Model output tensor (denoised adjacency). target     Target tensor (clean adjacency matrix).</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/gnn/#tmgg.models.gnn.NodeVarGNN.transform_for_loss--returns","level":5,"title":"Returns","text":"<p>tuple     (output, target) unchanged.</p>","path":["Reference","Models","GNN"],"tags":[]},{"location":"reference/models/hybrid/","level":1,"title":"Hybrid","text":"","path":["Reference","Models","Hybrid"],"tags":[]},{"location":"reference/models/hybrid/#tmgg.models.hybrid.SequentialDenoisingModel","level":2,"title":"<code>SequentialDenoisingModel</code>","text":"<p>               Bases: <code>DenoisingModel</code></p> <p>Sequential model combining GNN embeddings with attention-based denoising.</p> <p>This model first generates embeddings using a GNN, then applies a transformer to denoise these embeddings, and finally reconstructs the adjacency matrix.</p>","path":["Reference","Models","Hybrid"],"tags":[]},{"location":"reference/models/hybrid/#tmgg.models.hybrid.SequentialDenoisingModel.__init__","level":3,"title":"<code>__init__(embedding_model, denoising_model=None)</code>","text":"<p>Initialize the sequential denoising model.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_model</code> <code>EmbeddingModel</code> <p>GNN model for generating embeddings</p> required <code>denoising_model</code> <code>Optional[DenoisingModel]</code> <p>Optional transformer model for denoising embeddings</p> <code>None</code>","path":["Reference","Models","Hybrid"],"tags":[]},{"location":"reference/models/hybrid/#tmgg.models.hybrid.SequentialDenoisingModel.forward","level":3,"title":"<code>forward(x)</code>","text":"<p>Forward pass combining GNN embedding and transformer denoising.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input adjacency matrix</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed adjacency matrix</p>","path":["Reference","Models","Hybrid"],"tags":[]},{"location":"reference/models/hybrid/#tmgg.models.hybrid.SequentialDenoisingModel.get_config","level":3,"title":"<code>get_config()</code>","text":"<p>Get configuration for both embedding and denoising components.</p>","path":["Reference","Models","Hybrid"],"tags":[]},{"location":"reference/models/hybrid/#tmgg.models.hybrid.SequentialDenoisingModel.logits_to_graph","level":3,"title":"<code>logits_to_graph(logits)</code>","text":"<p>Convert logits to binary graph predictions.</p> <p>For BCE loss, sigmoid(x) &gt; 0.5 is equivalent to x &gt; 0, so we threshold logits directly. Subclasses may override for different thresholding.</p>","path":["Reference","Models","Hybrid"],"tags":[]},{"location":"reference/models/hybrid/#tmgg.models.hybrid.SequentialDenoisingModel.logits_to_graph--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward().</p>","path":["Reference","Models","Hybrid"],"tags":[]},{"location":"reference/models/hybrid/#tmgg.models.hybrid.SequentialDenoisingModel.logits_to_graph--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions (0 or 1).</p>","path":["Reference","Models","Hybrid"],"tags":[]},{"location":"reference/models/hybrid/#tmgg.models.hybrid.SequentialDenoisingModel.parameter_count","level":3,"title":"<code>parameter_count()</code>","text":"<p>Count trainable parameters in this module and its children.</p> <p>Returns a hierarchical dictionary with: - \"total\": Total trainable parameters in this module and all children - \"self\": Parameters directly owned by this module (not in children) - Child module counts with their names as keys</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parameter counts</p>","path":["Reference","Models","Hybrid"],"tags":[]},{"location":"reference/models/hybrid/#tmgg.models.hybrid.SequentialDenoisingModel.predict","level":3,"title":"<code>predict(logits, zero_diag=True)</code>","text":"<p>Convert model output (logits) to binary graph predictions.</p> <p>Uses logits_to_graph() for thresholding, then optionally zeros diagonal (adjacency matrices have no self-loops).</p>","path":["Reference","Models","Hybrid"],"tags":[]},{"location":"reference/models/hybrid/#tmgg.models.hybrid.SequentialDenoisingModel.predict--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward(). zero_diag     If True, zero out diagonal entries. Default True since adjacency     matrices typically have no self-loops.</p>","path":["Reference","Models","Hybrid"],"tags":[]},{"location":"reference/models/hybrid/#tmgg.models.hybrid.SequentialDenoisingModel.predict--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions with zero diagonal (if zero_diag=True).</p>","path":["Reference","Models","Hybrid"],"tags":[]},{"location":"reference/models/hybrid/#tmgg.models.hybrid.SequentialDenoisingModel.transform_for_loss","level":3,"title":"<code>transform_for_loss(output, target)</code>","text":"<p>Return output and target directly for loss computation.</p>","path":["Reference","Models","Hybrid"],"tags":[]},{"location":"reference/models/hybrid/#tmgg.models.hybrid.SequentialDenoisingModel.transform_for_loss--parameters","level":5,"title":"Parameters","text":"<p>output     Model output tensor (denoised adjacency). target     Target tensor (clean adjacency matrix).</p>","path":["Reference","Models","Hybrid"],"tags":[]},{"location":"reference/models/hybrid/#tmgg.models.hybrid.SequentialDenoisingModel.transform_for_loss--returns","level":5,"title":"Returns","text":"<p>tuple     (output, target) unchanged.</p>","path":["Reference","Models","Hybrid"],"tags":[]},{"location":"reference/models/hybrid/#tmgg.models.hybrid.create_sequential_model","level":2,"title":"<code>create_sequential_model(gnn_config, transformer_config=None)</code>","text":"<p>Factory function to create a sequential denoising model.</p> <p>Parameters:</p> Name Type Description Default <code>gnn_config</code> <code>Dict[str, Any]</code> <p>Configuration for the GNN embedding model</p> required <code>transformer_config</code> <code>Optional[Dict[str, Any]]</code> <p>Optional configuration for the transformer denoising model</p> <code>None</code> <p>Returns:</p> Type Description <code>SequentialDenoisingModel</code> <p>Configured SequentialDenoisingModel</p>","path":["Reference","Models","Hybrid"],"tags":[]},{"location":"reference/models/layers/","level":1,"title":"Layers","text":"","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.EigenDecompositionError","level":2,"title":"<code>EigenDecompositionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for eigendecomposition failures with debugging context.</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.EigenEmbedding","level":2,"title":"<code>EigenEmbedding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Embedding layer using eigenvectors of adjacency matrix.</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.EigenEmbedding--parameters","level":4,"title":"Parameters","text":"<p>eigenvalue_reg : float, optional     Diagonal regularization added before eigendecomposition. This spreads     eigenvalues apart, improving gradient stability through the eigh     operation. Default is 0.0 (no regularization). Values around 1e-4 to     1e-2 help when training produces NaN gradients or unstable loss.</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.EigenEmbedding.forward","level":3,"title":"<code>forward(A)</code>","text":"<p>Forward pass of eigen embedding.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>Adjacency matrix of shape (batch_size, num_nodes, num_nodes)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Eigenvector embeddings of shape (batch_size, num_nodes, num_nodes)</p> <p>Raises:</p> Type Description <code>EigenDecompositionError</code> <p>If eigendecomposition fails with debugging context</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.Etoy","level":2,"title":"<code>Etoy</code>","text":"<p>               Bases: <code>Module</code></p> <p>Aggregate edge features to graph-level features via statistics.</p> <p>Computes mean, min, max, and std across all edges, then projects to output dimension.</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.Etoy.__init__","level":3,"title":"<code>__init__(de, dy)</code>","text":"","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.Etoy.__init__--parameters","level":5,"title":"Parameters","text":"<p>de : int     Edge feature dimension dy : int     Output global feature dimension</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.Etoy.forward","level":3,"title":"<code>forward(E)</code>","text":"","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.Etoy.forward--parameters","level":5,"title":"Parameters","text":"<p>E : torch.Tensor     Edge features of shape (batch_size, n_nodes, n_nodes, de)</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.Etoy.forward--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Global features of shape (batch_size, dy)</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.GaussianEmbedding","level":2,"title":"<code>GaussianEmbedding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Gaussian embedding layer using powers of adjacency matrix.</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.GaussianEmbedding.__init__","level":3,"title":"<code>__init__(num_terms, num_channels)</code>","text":"<p>Initialize Gaussian embedding.</p> <p>Parameters:</p> Name Type Description Default <code>num_terms</code> <code>int</code> <p>Number of terms in the polynomial expansion</p> required <code>num_channels</code> <code>int</code> <p>Number of output channels</p> required","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.GaussianEmbedding.forward","level":3,"title":"<code>forward(A)</code>","text":"<p>Forward pass of Gaussian embedding.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>Adjacency matrix of shape (batch_size, num_nodes, num_nodes)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Node embeddings of shape (batch_size, num_nodes, num_channels)</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.GraphConvolutionLayer","level":2,"title":"<code>GraphConvolutionLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Graph convolution layer using polynomial filters.</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.GraphConvolutionLayer.__init__","level":3,"title":"<code>__init__(num_terms, num_channels)</code>","text":"<p>Initialize graph convolution layer.</p> <p>Parameters:</p> Name Type Description Default <code>num_terms</code> <code>int</code> <p>Number of terms in polynomial filter</p> required <code>num_channels</code> <code>int</code> <p>Number of input/output channels</p> required","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.GraphConvolutionLayer.forward","level":3,"title":"<code>forward(A, X)</code>","text":"<p>Forward pass of graph convolution.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>Adjacency matrix</p> required <code>X</code> <code>Tensor</code> <p>Input features</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Convolved features</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.MultiHeadAttention","level":2,"title":"<code>MultiHeadAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Multi-Head Attention module as described in 'Attention Is All You Need' paper.</p> <p>This implementation supports masked attention and different input/output dimensions.</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.MultiHeadAttention.__init__","level":3,"title":"<code>__init__(d_model, num_heads, d_k=None, d_v=None, dropout=0.0, bias=False)</code>","text":"<p>Initialize the Multi-Head Attention module.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Model dimension (input and output dimension)</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>d_k</code> <code>Optional[int]</code> <p>Dimension of keys (default: d_model // num_heads)</p> <code>None</code> <code>d_v</code> <code>Optional[int]</code> <p>Dimension of values (default: d_model // num_heads)</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.0</code> <code>bias</code> <code>bool</code> <p>Whether to use bias in linear layers</p> <code>False</code>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.MultiHeadAttention.forward","level":3,"title":"<code>forward(x, mask=None, residual=None)</code>","text":"<p>Forward pass of the Multi-Head Attention module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_len, d_model)</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>Optional mask tensor of shape (batch_size, seq_len_q, seq_len_k)</p> <code>None</code> <code>residual</code> <code>Optional[Tensor]</code> <p>Optional residual connection</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple of (output, combined_attention_scores)</p> <code>Tensor</code> <ul> <li>output: Output tensor of shape (batch_size, seq_len, d_model)</li> </ul> <code>Tuple[Tensor, Tensor]</code> <ul> <li>combined_attention_scores: Attention weights of shape (batch_size, seq_len, seq_len)</li> </ul>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.NodeVarGraphConvolutionLayer","level":2,"title":"<code>NodeVarGraphConvolutionLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Graph convolution layer with polynomial filters.</p> This layer computes a polynomial graph convolution <p>Y = sum_{i=0}^{num_terms} A^i @ X @ H[i]</p> <p>where A is the (normalized) adjacency matrix and H[i] are learnable weight matrices. Unlike the original \"node-variant\" design, parameters are shared across all nodes, enabling support for variable graph sizes.</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.NodeVarGraphConvolutionLayer--parameters","level":4,"title":"Parameters","text":"<p>num_terms : int     Number of polynomial terms (excluding the identity term). num_channels_in : int     Input feature dimension. num_channels_out : int     Output feature dimension.</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.NodeVarGraphConvolutionLayer--notes","level":4,"title":"Notes","text":"<p>The <code>num_nodes</code> parameter from the original API is kept for backwards compatibility but is now ignored - the layer works with any graph size.</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.NodeVarGraphConvolutionLayer.num_nodes","level":3,"title":"<code>num_nodes</code>  <code>property</code>","text":"<p>Backwards compatibility: return a placeholder value.</p> <p>The new design doesn't store num_nodes since it's node-agnostic. Returns -1 to indicate this, which will always differ from actual node counts, preventing the old dynamic recreation logic from triggering.</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.NodeVarGraphConvolutionLayer.__init__","level":3,"title":"<code>__init__(num_terms, num_channels_in, num_channels_out=None)</code>","text":"<p>Initialize the graph convolution layer.</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.NodeVarGraphConvolutionLayer.__init__--parameters","level":5,"title":"Parameters","text":"<p>num_terms : int     Number of polynomial terms. num_channels_in : int     Input feature dimension. num_channels_out : int, optional     Output feature dimension. If None, defaults to num_channels_in.     (For backwards compatibility with the old 3-arg signature where     the third arg was num_nodes, this is ignored if it seems too large.)</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.NodeVarGraphConvolutionLayer.forward","level":3,"title":"<code>forward(A, X)</code>","text":"<p>Forward pass of graph convolution.</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.NodeVarGraphConvolutionLayer.forward--parameters","level":5,"title":"Parameters","text":"<p>A : torch.Tensor     Adjacency matrix of shape (batch, num_nodes, num_nodes). X : torch.Tensor     Input features of shape (batch, num_nodes, num_channels_in).</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.NodeVarGraphConvolutionLayer.forward--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Output features of shape (batch, num_nodes, num_channels_out).</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.Xtoy","level":2,"title":"<code>Xtoy</code>","text":"<p>               Bases: <code>Module</code></p> <p>Aggregate node features to graph-level features via statistics.</p> <p>Computes mean, min, max, and std across nodes, then projects to output dimension.</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.Xtoy.__init__","level":3,"title":"<code>__init__(dx, dy)</code>","text":"","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.Xtoy.__init__--parameters","level":5,"title":"Parameters","text":"<p>dx : int     Node feature dimension dy : int     Output global feature dimension</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.Xtoy.forward","level":3,"title":"<code>forward(X)</code>","text":"","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.Xtoy.forward--parameters","level":5,"title":"Parameters","text":"<p>X : torch.Tensor     Node features of shape (batch_size, n_nodes, dx)</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/layers/#tmgg.models.layers.Xtoy.forward--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Global features of shape (batch_size, dy)</p>","path":["Reference","Models","Layers"],"tags":[]},{"location":"reference/models/spectral_denoisers/","level":1,"title":"Spectral denoisers","text":"<p>Spectral denoising models for graph diffusion tasks.</p> <p>This package provides denoising architectures that operate on the eigenspace of noisy adjacency matrices. All models follow a common pattern: extract top-k eigenvectors, apply a learnable transformation, reconstruct adjacency.</p> <p>All models output raw logits. Use model.predict(logits) to get [0,1] probabilities.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers--models","level":3,"title":"Models","text":"<p>LinearPE     Simple linear transformation: Â = V W V^T + bias GraphFilterBank     Spectral polynomial filter: W = Σ Λ^ℓ H^{(ℓ)} SelfAttentionDenoiser     Scaled dot-product attention: Â = Q K^T / √d_k</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers--layers","level":3,"title":"Layers","text":"<p>TopKEigenLayer     Extract top-k eigenvectors with sign normalization</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers--base-classes","level":3,"title":"Base Classes","text":"<p>SpectralDenoiser     Abstract base for all spectral denoising models</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.GraphFilterBank","level":2,"title":"<code>GraphFilterBank</code>","text":"<p>               Bases: <code>SpectralDenoiser</code></p> <p>Graph Filter Bank denoiser with spectral polynomial.</p> Reconstructs the adjacency matrix using a spectral polynomial filter <p>W = Σ_{ℓ=0}^{K-1} Λ^ℓ ⊙ H^{(ℓ)} Â = V W V^T</p> <p>where V ∈ R^{n×k} are top-k eigenvectors, Λ = diag(λ_1,...,λ_k) contains the corresponding eigenvalues, and H^{(ℓ)} ∈ R^{k×k} are learnable coefficient matrices.</p> <p>The polynomial filter allows learning frequency-dependent transformations: different eigenvalue magnitudes (frequencies) can be amplified or attenuated differently.</p> <p>Outputs raw logits. Use model.predict(logits) for [0,1] probabilities.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.GraphFilterBank--parameters","level":4,"title":"Parameters","text":"<p>k : int     Number of eigenvectors to use. polynomial_degree : int, optional     Degree of the spectral polynomial (K in the formula). Higher degrees     allow more complex frequency responses. Default is 5.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.GraphFilterBank--notes","level":4,"title":"Notes","text":"<p>The spectral polynomial is computed as:     W_ij = Σ_ℓ (λ_i^ℓ) * H^{(ℓ)}_ij</p> <p>This can be seen as a bank of filters, each H^{(ℓ)} scaled by the ℓ-th power of eigenvalues, capturing different spectral characteristics.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.GraphFilterBank--examples","level":4,"title":"Examples","text":"<p>model = GraphFilterBank(k=8, polynomial_degree=5) A_noisy = torch.randn(4, 50, 50) A_noisy = (A_noisy + A_noisy.transpose(-1, -2)) / 2 logits = model(A_noisy) predictions = model.predict(logits)  # apply sigmoid predictions.shape torch.Size([4, 50, 50])</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.GraphFilterBank.forward","level":3,"title":"<code>forward(A)</code>","text":"<p>Denoise adjacency matrix via spectral transformation.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.GraphFilterBank.forward--parameters","level":5,"title":"Parameters","text":"<p>A : torch.Tensor     Noisy adjacency matrix of shape (batch, n, n) or (n, n).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.GraphFilterBank.forward--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Denoised adjacency matrix of same shape as input.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.GraphFilterBank.logits_to_graph","level":3,"title":"<code>logits_to_graph(logits)</code>","text":"<p>Convert logits to binary graph predictions.</p> <p>For BCE loss, sigmoid(x) &gt; 0.5 is equivalent to x &gt; 0, so we threshold logits directly. Subclasses may override for different thresholding.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.GraphFilterBank.logits_to_graph--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward().</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.GraphFilterBank.logits_to_graph--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions (0 or 1).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.GraphFilterBank.parameter_count","level":3,"title":"<code>parameter_count()</code>","text":"<p>Count trainable parameters in this module and its children.</p> <p>Returns a hierarchical dictionary with: - \"total\": Total trainable parameters in this module and all children - \"self\": Parameters directly owned by this module (not in children) - Child module counts with their names as keys</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parameter counts</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.GraphFilterBank.predict","level":3,"title":"<code>predict(logits, zero_diag=True)</code>","text":"<p>Convert model output (logits) to binary graph predictions.</p> <p>Uses logits_to_graph() for thresholding, then optionally zeros diagonal (adjacency matrices have no self-loops).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.GraphFilterBank.predict--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward(). zero_diag     If True, zero out diagonal entries. Default True since adjacency     matrices typically have no self-loops.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.GraphFilterBank.predict--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions with zero diagonal (if zero_diag=True).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.GraphFilterBank.transform_for_loss","level":3,"title":"<code>transform_for_loss(output, target)</code>","text":"<p>Return output and target directly for loss computation.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.GraphFilterBank.transform_for_loss--parameters","level":5,"title":"Parameters","text":"<p>output     Model output tensor (denoised adjacency). target     Target tensor (clean adjacency matrix).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.GraphFilterBank.transform_for_loss--returns","level":5,"title":"Returns","text":"<p>tuple     (output, target) unchanged.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.LinearPE","level":2,"title":"<code>LinearPE</code>","text":"<p>               Bases: <code>SpectralDenoiser</code></p> <p>Linear Positional Encoding denoiser.</p> Reconstructs the adjacency matrix as <p>Â = V W V^T + 1 b^T + b 1^T</p> <p>where V ∈ R^{n×k} are the top-k eigenvectors of the noisy adjacency, W ∈ R^{k×k} is a learnable weight matrix, and b ∈ R^{max_n} is a learnable bias vector capturing node-specific degree corrections.</p> <p>Outputs raw logits. Use model.predict(logits) for [0,1] probabilities.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.LinearPE--parameters","level":4,"title":"Parameters","text":"<p>k : int     Number of eigenvectors to use. max_nodes : int, optional     Maximum number of nodes in any graph. Required if use_bias is True.     Default is 200. use_bias : bool, optional     Whether to use the node-specific bias term. For multi-graph settings     with varying sizes, consider setting this to False. Default is True.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.LinearPE--notes","level":4,"title":"Notes","text":"<p>The bias term adds a rank-2 correction to the low-rank reconstruction. Entry (i, j) receives bias b_i + b_j, which captures node degree effects not captured by the top-k eigenspace.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.LinearPE--examples","level":4,"title":"Examples","text":"<p>model = LinearPE(k=8, max_nodes=50) A_noisy = torch.randn(4, 50, 50) A_noisy = (A_noisy + A_noisy.transpose(-1, -2)) / 2 logits = model(A_noisy) predictions = model.predict(logits)  # apply sigmoid predictions.shape torch.Size([4, 50, 50])</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.LinearPE.forward","level":3,"title":"<code>forward(A)</code>","text":"<p>Denoise adjacency matrix via spectral transformation.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.LinearPE.forward--parameters","level":5,"title":"Parameters","text":"<p>A : torch.Tensor     Noisy adjacency matrix of shape (batch, n, n) or (n, n).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.LinearPE.forward--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Denoised adjacency matrix of same shape as input.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.LinearPE.logits_to_graph","level":3,"title":"<code>logits_to_graph(logits)</code>","text":"<p>Convert logits to binary graph predictions.</p> <p>For BCE loss, sigmoid(x) &gt; 0.5 is equivalent to x &gt; 0, so we threshold logits directly. Subclasses may override for different thresholding.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.LinearPE.logits_to_graph--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward().</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.LinearPE.logits_to_graph--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions (0 or 1).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.LinearPE.parameter_count","level":3,"title":"<code>parameter_count()</code>","text":"<p>Count trainable parameters in this module and its children.</p> <p>Returns a hierarchical dictionary with: - \"total\": Total trainable parameters in this module and all children - \"self\": Parameters directly owned by this module (not in children) - Child module counts with their names as keys</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parameter counts</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.LinearPE.predict","level":3,"title":"<code>predict(logits, zero_diag=True)</code>","text":"<p>Convert model output (logits) to binary graph predictions.</p> <p>Uses logits_to_graph() for thresholding, then optionally zeros diagonal (adjacency matrices have no self-loops).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.LinearPE.predict--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward(). zero_diag     If True, zero out diagonal entries. Default True since adjacency     matrices typically have no self-loops.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.LinearPE.predict--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions with zero diagonal (if zero_diag=True).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.LinearPE.transform_for_loss","level":3,"title":"<code>transform_for_loss(output, target)</code>","text":"<p>Return output and target directly for loss computation.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.LinearPE.transform_for_loss--parameters","level":5,"title":"Parameters","text":"<p>output     Model output tensor (denoised adjacency). target     Target tensor (clean adjacency matrix).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.LinearPE.transform_for_loss--returns","level":5,"title":"Returns","text":"<p>tuple     (output, target) unchanged.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SelfAttentionDenoiser","level":2,"title":"<code>SelfAttentionDenoiser</code>","text":"<p>               Bases: <code>SpectralDenoiser</code></p> <p>Self-Attention denoiser with query/key projections.</p> <p>Reconstructs the adjacency matrix using scaled dot-product attention on the eigenvector embeddings:     Q = V W_Q     K = V W_K     Â = Q K^T / √d_k</p> <p>where V ∈ R^{n×k} are the top-k eigenvectors, W_Q, W_K ∈ R^{k×d_k} are learnable projection matrices, and d_k is the key dimension.</p> <p>The 1/√d_k scaling stabilizes gradients following standard practice in transformer architectures.</p> <p>Outputs raw logits. Use model.predict(logits) for [0,1] probabilities.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SelfAttentionDenoiser--parameters","level":4,"title":"Parameters","text":"<p>k : int     Number of eigenvectors to use as input dimension. d_k : int, optional     Key/query dimension for attention. Default is 64.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SelfAttentionDenoiser--notes","level":4,"title":"Notes","text":"<p>Unlike transformer attention which uses softmax over the sequence dimension, this produces a symmetric attention matrix representing edge logits. The model.predict() method applies sigmoid to get probabilities.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SelfAttentionDenoiser--examples","level":4,"title":"Examples","text":"<p>model = SelfAttentionDenoiser(k=8, d_k=32) A_noisy = torch.randn(4, 50, 50) A_noisy = (A_noisy + A_noisy.transpose(-1, -2)) / 2 logits = model(A_noisy) predictions = model.predict(logits)  # apply sigmoid predictions.shape torch.Size([4, 50, 50])</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SelfAttentionDenoiser.forward","level":3,"title":"<code>forward(A)</code>","text":"<p>Denoise adjacency matrix via spectral transformation.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SelfAttentionDenoiser.forward--parameters","level":5,"title":"Parameters","text":"<p>A : torch.Tensor     Noisy adjacency matrix of shape (batch, n, n) or (n, n).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SelfAttentionDenoiser.forward--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Denoised adjacency matrix of same shape as input.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SelfAttentionDenoiser.logits_to_graph","level":3,"title":"<code>logits_to_graph(logits)</code>","text":"<p>Convert logits to binary graph predictions.</p> <p>For BCE loss, sigmoid(x) &gt; 0.5 is equivalent to x &gt; 0, so we threshold logits directly. Subclasses may override for different thresholding.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SelfAttentionDenoiser.logits_to_graph--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward().</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SelfAttentionDenoiser.logits_to_graph--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions (0 or 1).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SelfAttentionDenoiser.parameter_count","level":3,"title":"<code>parameter_count()</code>","text":"<p>Count trainable parameters in this module and its children.</p> <p>Returns a hierarchical dictionary with: - \"total\": Total trainable parameters in this module and all children - \"self\": Parameters directly owned by this module (not in children) - Child module counts with their names as keys</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parameter counts</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SelfAttentionDenoiser.predict","level":3,"title":"<code>predict(logits, zero_diag=True)</code>","text":"<p>Convert model output (logits) to binary graph predictions.</p> <p>Uses logits_to_graph() for thresholding, then optionally zeros diagonal (adjacency matrices have no self-loops).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SelfAttentionDenoiser.predict--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward(). zero_diag     If True, zero out diagonal entries. Default True since adjacency     matrices typically have no self-loops.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SelfAttentionDenoiser.predict--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions with zero diagonal (if zero_diag=True).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SelfAttentionDenoiser.transform_for_loss","level":3,"title":"<code>transform_for_loss(output, target)</code>","text":"<p>Return output and target directly for loss computation.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SelfAttentionDenoiser.transform_for_loss--parameters","level":5,"title":"Parameters","text":"<p>output     Model output tensor (denoised adjacency). target     Target tensor (clean adjacency matrix).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SelfAttentionDenoiser.transform_for_loss--returns","level":5,"title":"Returns","text":"<p>tuple     (output, target) unchanged.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser","level":2,"title":"<code>SpectralDenoiser</code>","text":"<p>               Bases: <code>DenoisingModel</code></p> <p>Abstract base class for spectral graph denoising models.</p> <p>Spectral denoisers operate on the eigenspace of the noisy adjacency matrix. They extract the top-k eigenvectors, apply a learnable transformation, and reconstruct the denoised adjacency.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser--parameters","level":4,"title":"Parameters","text":"<p>k : int     Number of eigenvectors to use. Capped by graph size at runtime.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser--attributes","level":4,"title":"Attributes","text":"<p>eigen_layer : TopKEigenLayer     Layer for extracting top-k eigenvectors with sign normalization.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser.forward","level":3,"title":"<code>forward(A)</code>","text":"<p>Denoise adjacency matrix via spectral transformation.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser.forward--parameters","level":5,"title":"Parameters","text":"<p>A : torch.Tensor     Noisy adjacency matrix of shape (batch, n, n) or (n, n).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser.forward--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Denoised adjacency matrix of same shape as input.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser.get_config","level":3,"title":"<code>get_config()</code>","text":"<p>Get model configuration for logging/saving.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser.get_config--returns","level":5,"title":"Returns","text":"<p>dict     Dictionary containing model hyperparameters.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser.logits_to_graph","level":3,"title":"<code>logits_to_graph(logits)</code>","text":"<p>Convert logits to binary graph predictions.</p> <p>For BCE loss, sigmoid(x) &gt; 0.5 is equivalent to x &gt; 0, so we threshold logits directly. Subclasses may override for different thresholding.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser.logits_to_graph--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward().</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser.logits_to_graph--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions (0 or 1).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser.parameter_count","level":3,"title":"<code>parameter_count()</code>","text":"<p>Count trainable parameters in this module and its children.</p> <p>Returns a hierarchical dictionary with: - \"total\": Total trainable parameters in this module and all children - \"self\": Parameters directly owned by this module (not in children) - Child module counts with their names as keys</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parameter counts</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser.predict","level":3,"title":"<code>predict(logits, zero_diag=True)</code>","text":"<p>Convert model output (logits) to binary graph predictions.</p> <p>Uses logits_to_graph() for thresholding, then optionally zeros diagonal (adjacency matrices have no self-loops).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser.predict--parameters","level":5,"title":"Parameters","text":"<p>logits     Raw model output from forward(). zero_diag     If True, zero out diagonal entries. Default True since adjacency     matrices typically have no self-loops.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser.predict--returns","level":5,"title":"Returns","text":"<p>torch.Tensor     Binary predictions with zero diagonal (if zero_diag=True).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser.transform_for_loss","level":3,"title":"<code>transform_for_loss(output, target)</code>","text":"<p>Return output and target directly for loss computation.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser.transform_for_loss--parameters","level":5,"title":"Parameters","text":"<p>output     Model output tensor (denoised adjacency). target     Target tensor (clean adjacency matrix).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.SpectralDenoiser.transform_for_loss--returns","level":5,"title":"Returns","text":"<p>tuple     (output, target) unchanged.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.TopKEigenLayer","level":2,"title":"<code>TopKEigenLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Extract top-k eigenvectors from symmetric adjacency matrices.</p> <p>Given a batch of symmetric adjacency matrices, computes eigendecomposition and returns the top-k eigenvectors ordered by eigenvalue magnitude. Sign ambiguity is resolved by enforcing the first nonzero entry of each eigenvector to be positive.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.TopKEigenLayer--parameters","level":4,"title":"Parameters","text":"<p>k : int     Number of eigenvectors to extract. If k exceeds the matrix dimension n,     all n eigenvectors are returned.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.TopKEigenLayer--examples","level":4,"title":"Examples","text":"<p>layer = TopKEigenLayer(k=4) A = torch.randn(8, 20, 20) A = (A + A.transpose(-1, -2)) / 2  # symmetrize V, Lambda = layer(A) V.shape torch.Size([8, 20, 4]) Lambda.shape torch.Size([8, 4])</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.TopKEigenLayer.forward","level":3,"title":"<code>forward(A)</code>","text":"<p>Extract top-k eigenvectors from adjacency matrices.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.TopKEigenLayer.forward--parameters","level":5,"title":"Parameters","text":"<p>A : torch.Tensor     Batch of symmetric adjacency matrices of shape (batch, n, n).     Can also accept unbatched input of shape (n, n).</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]},{"location":"reference/models/spectral_denoisers/#tmgg.models.spectral_denoisers.TopKEigenLayer.forward--returns","level":5,"title":"Returns","text":"<p>V : torch.Tensor     Top-k eigenvectors of shape (batch, n, k) or (n, k) if unbatched.     Each column is an eigenvector, ordered by decreasing eigenvalue     magnitude. Sign-normalized so first nonzero entry is positive. Lambda : torch.Tensor     Corresponding eigenvalues of shape (batch, k) or (k,) if unbatched.     Ordered by decreasing magnitude.</p>","path":["Reference","Models","Spectral denoisers"],"tags":[]}]}